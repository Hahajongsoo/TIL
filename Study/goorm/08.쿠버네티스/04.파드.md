쿠버네티스에서 컨테이너를 실행하는 것은 워크로드라고 하고 워크로드에는 파드와 워크로드 리소스가 있다. (워크로드 리소스의 원래 이름은 컨트롤러였다. 그래서 책에 워크로드 리소스라고 나와있는 것) 
대강 간단하게 파드를 컨테이너라고 볼 수 있다. 그리고 워크로드 리소스(컨트롤러)는 파드를 컨트롤하는 것이다. 워크로드 리소스는 파드를 만드는 것에 목적이 있다. 앞으로 파드를 직접 만들일은 없고 워크로드 리소스를 통해서 파드를 만들게 될 것이다. 파드가 기본이 되기 때문에 파드에 대한 이해가 필요하다.

- ==파드는 쿠버네티스에서 생성하고 관리할 수 있는 배포 가능한 가장 작은 컴퓨팅 단위==이다. (배포 가능하다는 것에 워크로드, application)
	- 쿠버네티스는 파드를 관리하는 것이지 컨테이너를 관리하는 것이 아니다. 
- 파드는 하나 이상의 컨테이너 그룹이다. ==이 그룹은 스토리지 및 네트워크를 공유==하고, 해당 컨테이너를 구동하는 방식에 대한 명세를 갖는다. 
	- 보통은 파드 하나에 컨테이너 하나를 갖는 형태(singleton)를 가지지만 둘 이상의 컨테이너를 가지는 경우도 있다. 
	- [여러 컨테이너를 두는 유형](https://kubernetes.io/blog/2015/06/the-distributed-system-toolkit-patterns/)
		- sidecar containers: 주 역할을 하는 컨테이너와 이를 ==**보조**하는== 컨테이너로 이루어진 파드이다. 예를 들어 웹서버 컨테이너와 로그를 저장하는 컨테이너로 파드를 생성할 수 있다.
		- ambassador containers: 주 역할을 하는 컨테이너의 proxy 역할을 하는 컨테이너를 만드는 것이다. 말 그대로 ambassador 의 역할을 하는 컨테이너를 두는 것
		- adapter containers: 위와 반대로 네트워크가 안으로 들어올 때 어댑터를 거쳐서 주 역할을 하는 컨테이너로 들어오게 한다. 주로 모니터링을 하는 컨테이너를 둔다.
- 파드의 콘텐츠는 항상 함께 배치되고, 함께 스케줄되며, 공유 콘텍스트에서 실행된다. 파드는 애플리케이션 별 "논리 호스트"를 모델링한다. 클라우드가 아닌 콘텍스트에서, 동일한 물리 또는 가상 머신에서 실행되는 애플리케이션은 동일한 논리 호스트에서 실행되는 클라우드 애플리케이션과 비슷하다. 
- 파드의 공유 콘텍스트는 리눅스 네임스페이스, cgroup 및 도커 컨테이너를 격리하는 것과 같이 잠재적으로 다른 격리 요소들이다. 파드의 콘텍스트 내에서 개별 애플리케이션은 추가적으로 하위 격리가 적용된다. 도커 개념 측면에서, 파드는 공유 네임스페이스와 공유 파일시스템 볼륨이 있는 도커 컨테이너 그룹과 비슷하다.

# 파드가 여러 컨테이너를 관리하는 방법
파드는 응집력있는 서비스 단위를 형성하는 여러 협력 프로세스(컨테이너)를 지원하도록 설계되었다. 파드의 컨테이너는 클러스터의 동일한 물리 또는 가상 머신에서 자동으로 같은 위치에 배치되고 함께 스케줄된다. 컨테이너는 리소스와 의존성을 공유하고 서로 통신하고, 종료 시기와 방법을 조정할 수 있다. 

## 하나의 파드에 여러 컨테이너를 두는 경우에 대한 생각
- Web 3-Tier를 만드는 경우 web server, web app, DB를 하나의 파드로 묶어서 설계해도 되는가를 생각해보면 절대 그렇게 해서는 안된다. 이 기능들은 모두 주요한 기능들이다. SPoF를 기억하자. 
	- 실제로 애플리케이션들을 격리하는 이유는 SPoF를 막기 위해서이다. 자본이 많다면 애플리케이션 하나마다 하드웨어들로 격리하여 구성할 수 없다. 이것이 힘든경우 가상화를 이용한다. 하나의 하드웨어 위에 하이퍼바이저를 올리고 여러 대의 VM을 만들어 애플리케이션들을 격리한다. 
	- 하지만 게스트 운영체제를 사용한다는 것을 생각해봤을 때 용량이 적은 애플리케이션을 실행하기위해 그것보다 훨씬 더 무거운 운영체제를 사용해야한다는 단점이 있다. 또한 운영체제의 경우 시간이 오래 걸리는 부팅을 해야하기 때문에 스케일 아웃에 매우 불리하다.
	- 위에서와 마찬가지로 컨테이너를 사용하는 것도 결국은 애플리케이션을 격리하기 위해서 사용하는 것이다. 이렇게 해서 애플리케이션들이 서로 영향을 미치는 것을 막을 수 있다. 
- 가상화, 컨테이너를 사용하는 이유 중 하나가 격리를 하는 것인데 하나로 묶는 것 자체가 말이 안되는 짓이다. 

![](images/Pasted%20image%2020230214144138.png)
- 도커에서는 기본적으로 브릿지를 기반으로 컨테이너들이 하나의 IP 주소를 부여받게 된다. 그러나 쿠버네티스의 컨테이너들은 그렇지 않다. 하나의 파드안에 있는 컨테이너는 하나의 네트워크 스택을 공유하게 된다. 즉, 하나의 파드안에 있는 컨테이너들은 동일한 IP를 사용한다. 하나의 파드안에 있는 컨테이너들이 동일한 포트를 사용할 수 없다. 또한 파드안의 여러 개의 컨테이너들은 볼륨을 공유한다. 

# 파드 라이프사이클
파드는 정의된 라이프사이클을 따른다. `Pending` 단계에서 시작해서, 기본 컨테이너 중 적어도 하나 이상이 OK로 시작되면 `Running` 단계를 통과하고, 그런 다음 파드의 컨테이너가 실패로 종료되었는지 여부에 따라 `Succeded` 또는 `Failed` 단계로 이동한다.
파드가 실행되는 동안, kubelet은 일종의 오류를 처리하기 위해 컨테이너를 다시 시작할 수 있다. 파드 내에서, 쿠버네티스는 다양한 컨테이너 상태를 추적하고 파드를 다시 정상 상태로 만들기 위해 취할 조치를 결정한다.


## 파드의 수명
개별 애플리케이션 컨테이너와 마찬가지로 파드는 비교적 임시(계속 이어지는 것이 아닌) 엔티티로 간주된다. 정성껏 관리하는 물리적 서버와 달리 파드는 그저 임시적으로 사용하는 것이며 따라서 컨테이너에는 되도록 상태를 넣지 않아야한다. 
파드가 생성되고, 고유 ID가 할당되고, 종료 또는 삭제될 때까지 남아있는 노드에 스케줄된다. UID로 정의된 특정 파드는 다른 노드로 절대 다시 스케줄되지 않는다. 대신, 해당 파드는 사용자가 원한다면 이름은 같지만, UID가 다른, 거의 동일한 새 파드로 대체될 수 있다.

## 파드의 단계
파드의 `status` 필드는 `phase` 필드를 포함하는 PodStatus 오브젝트로 정의된다.
파드의 phase는 파드가 라이프사이클 중 어느 단계에 해당하는지 표현하는 간단한 고수준의 요약이다. phase는 컨테이너나 파드의 관측 정보에 대한 포괄적인 롤업이나, 포괄적인 상태 머신을 표현하도록 의도되지는 않았다. 

| 값        | 의미                                                                                                                                                                                                                            |
| --------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Pending   | 파드가 쿠버네티스 클러스터에서 승인되었지만, 하나 이상의 컨테이너가 설정되지 않았고 실행할 준비가 되지 않았다. 여기에는 파드가 스케줄되기 이전까지의 시간 뿐만 아니라 네트워크를 통한 컨테이너 이미지 다운로드 시간도 포함된다. |
| Running   | 파드가 노드에 바인딩되었고, 모든 컨테이너가 생성되었다. 적어도 하나의 컨테이너가 아직 실행중이거나, 시작 또는 재시작 중에 있다.                                                                                                 |
| Succeeded | 파드에 있는 모든 컨테이너들이 성공적으로 종료되었고, 재시작되지 않을 것이다.                                                                                                                                                    |
| Failed    | 파드에 있는 모든 컨테이너가 종료되었고, 적어도 하나 이상의 컨테이너가 실패로 종료되었다. 즉, 해당 컨테이너는 non-zero 상태로 빠져나왔거나 시스템에 의해서 종료되었다.                                                           |
| Unknown   | 어떤 이유에 의해서 파드의 상태를 얻을 수 없다. 이 단계는 일반적으로 파드가 실행되어야 하는 노드와의 통신 오류로 인해 발생한다.                                                                                                                                                                                                                                |

파드가 삭제될 때, 일부 kubectl 커맨드에서 Terminating이 표시된다. 이 Terminating 상태는 파드의 단계에 해당하지 않는다. 파드에는 그레이스풀하게 종료되도록 기간이 부여되며, 그 기본값은 30초이다.

## 컨테이너 상태
쿠버네티스는 파드 내부의 각 컨테이너 상태를 추적한다. 컨테이너 라이프사이클 훅을 사용하여 컨테이너 라이프사이클의 특정 지점에서 실행할 이벤트를 트리거할 수 있다.
스케줄러가 노드에 파드를 할당하면, kubelet은 컨테이너 런타임을 사용하여 해당 파드에 대한 컨테이너 생성을 시작한다.
 - wating

## 컨테이너 재시작 정책
파드의 `spec` 에는 `restartPowlicy` 필드가 있다. 사용 가능한 값은 Always, OnFailure 그리고 Never이다. 기본값은 Always이다.
`restartPolicy` 는 파드의 모든 컨테이너에 적용된다. `restartPolicy` 는 동일한 노드에서 kubelet에 의한 컨테이너 재시작만을 의미한다. 파드의 컨테이너가 종료된 후, kubelet은 5분으로 제한되는 지수 loopbackoff 지연으로(10초, 20초, 40초, ...) 컨테이너를 재시작한다. 컨테이너가 10분 동안 아무런 문제없이 실행되면, kubelet은 해당 컨테이너의 재시작 백오프 타이머를 재설정한다. 
재시작시에 리소스 소모가 크기 때문에 재시작 간격을 늘려주는 것이다. 

## 컨테이너 프로브(probe)
프로브는 컨테이너에서 kubelet에 의해 주기적으로 수행되는 진단이다. 진단을 수행하기 위해서, kubelet은 컨테이너 안에서 코드를 실행하거나, 네트워크 요청을 전송한다. 
컨테이너가 실행되는 것과 컨테이너의 애플리케이션이 실행되고 있는 것은 다른 이야기이다. READY나 STATUS가 정상적이라고 하더라도 애플리케이션은 제대로 동작하고 있지 않을 수 있다. 프로브는 애플리케이션이 제대로 실행되는지를 확인한다.

### 체크 메커니즘
각 프로브는 다음의 4가지 메커니즘 중 단 하나만을 정의해야 한다.
- exec: 컨테이너 내에서 지정된 명령어를 실행한다. 명령어가 상태코드 0으로 종료되면 진단이 성공한 것으로 간주.
- grpc: grpc를 사용하여 원격 프로시저 호출을 수행한다. 체크 대상이 gRPC 헬스 체크를 구현해야 한다. 응답의 status가 SERVING 이면 진단이 성공했다고 간주한다.
- httpGet: 지정한 포트 및 경로에서 컨테이너의 IP주소에 대한 HTTP GET 요청을 수행한다. 응답의 상태코드가 200이상 400미만이면 진단이 성공한 것으로 간주한다.
- tcpSocket: 지정된 포트에서 컨테이너 IP주소에 대한 TCP 검사를 수행한다. 포트가 활성화되어 있다면 진단이 성공한 것으로 간주한다. 

### 프로브 결과
- Success: 컨테이너가 진단을 통과함
- Failure: 컨테이너가 진단에 실패함
- Unknwon: 진단 자체가 실패함

### 프로브 종류
- livenessProbe: 컨테이너(애플리케이션)가 동작 중인 여부를 나타낸다. 만약 livenessProbe에 실패한다면 kubelet은 컨테이너를 죽이고 해당 컨테이너는 재시작 정책의 대상이 된다. 만약 컨테이너가 활성 프로브를 제공하지 않을 경우, 기본 상태는 Success 이다.
- readinessProbe: 컨테이너가 요청을 처리할 준비가 되어있는지 여부를 나타낸다. 실패한다면 엔드포인트 컨트롤러는 파드에 연관된 모든 서비스들의 엔드포인트에서 파드의 IP주소를 제거한다.  
- startupProbe: 컨테이너 내의 애플리케이션이 시작되었는지를 나타낸다. 성공할 때까지 다른 나머지 프로브는 활성화되지 않는다. 실패하면, kubelet이 컨테이너를 죽이고, 컨테이너는 재시작 정책에 따라 처리된다. 컨테이너에 스타트업 프로브가 없는 경우, 기본 상태는 `Success` 이다.

#### livenessProbe
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod-liveness
spec:
  containers:
  - name: myapp
    image: ghcr.io/c1t1d0s7/go-myweb:alpine
    ports:
    - containerPort: 8080
      protocol: TCP
    livenessProbe:
      httpGet:
        path: /health
        port: 8080
```

```
vagrant@kube-control1:~/goorm-8th-k8s/manifests/03_workload_pod/05_lifecycle$ cat myapp-pod-liveness.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod-liveness
...
Containers:
  myapp:
...
    Restart Count:  0
    Liveness:       http-get http://:8080/health delay=0s timeout=1s period=10s #success=1 #failure=3
...
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  61s   default-scheduler  Successfully assigned default/myapp-pod-liveness to kube-node2
  Normal  Pulled     60s   kubelet            Container image "ghcr.io/c1t1d0s7/go-myweb:alpine" already present on machine
  Normal  Created    60s   kubelet            Created container myapp
  Normal  Started    60s   kubelet            Started container myapp

```
- Liveness 부분에서 어느 경로에 어느 주기로 요청을 보내는지 나와있다. 성공과 실패는 threshold 값으로 성공이나 실패가 연속적으로 발생해서 해당 값을 넘기면 실패하거나 성공하게 된다.

- 일부러 livenessprove가 실패하도록 한 오브젝트이다. 
```yaml

apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod-liveness-404
spec:
  containers:
  - name: myapp
    image: ghcr.io/c1t1d0s7/go-myweb:alpine
    ports:
    - containerPort: 8080
      protocol: TCP
    livenessProbe:
      httpGet:
        path: /health?code=404
        port: 8080
```
- describe로 확인시 프로브에 실패하여 컨테이너가 재시작되고 있는 것을 볼 수 있다.
```
$ kubectl describe pod myapp-pod-liveness-404 
...
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason     Age                    From               Message
  ----     ------     ----                   ----               -------
  Normal   Scheduled  3m40s                  default-scheduler  Successfully assigned default/myapp-pod-liveness-404 to kube-node2
  Normal   Pulled     2m9s (x4 over 3m39s)   kubelet            Container image "ghcr.io/c1t1d0s7/go-myweb:alpine" already present on machine
  Normal   Created    2m9s (x4 over 3m39s)   kubelet            Created container myapp
  Normal   Started    2m9s (x4 over 3m39s)   kubelet            Started container myapp
  Normal   Killing    2m9s (x3 over 3m9s)    kubelet            Container myapp failed liveness probe, will be restarted
  Warning  Unhealthy  119s (x10 over 3m29s)  kubelet            Liveness probe failed: HTTP probe failed with statuscode: 404

```
그리고 Backoff에 걸리면서 점점 재시작 주기가 늘어나는 것도 볼 수 있다.

#### startupProbe
애플리케이션이 실행하는데 오랜시간이 걸리는 경우 livenessProbe를 사용하면 애플리케이션 실행 성공여부와 관계없이 실패처리가 되고 컨테이너가 재시작될 수 있다. livenessProbe의 시간 간격을 늘려서 애플리케이션 실행 시간을 넘기게 둔다 하더라도 실제 애플리케이션에 문제가 생긴 경우 문제를 발견하는데 오랜시간이 걸릴 수 있어 좋지 않을 수 있다.
이 문제 때문에 startupProbe를 사용한다. startupProbe가 성공할 때 까지 다른 프로브들은 동작하지 않는다. 

- startupProbe 단계에서 실패하는 경우 livenessProbe가 동작하지 않기 때문에 READY가 1이 될 수 없다.
```
vagrant@kube-control1:~$ kubectl describe pod myapp-pod-startup-404 
Name:         myapp-pod-startup-404
...
Containers:
  myapp:
...
    Ready:          False
    Restart Count:  0
    Liveness:       http-get http://:8080/health delay=0s timeout=1s period=10s #success=1 #failure=3
    Startup:        http-get http://:8080/health%3Fcode=404 delay=0s timeout=1s period=10s #success=1 #failure=3
...
Events:
  Type     Reason     Age   From               Message
  ----     ------     ----  ----               -------
  Normal   Scheduled  19s   default-scheduler  Successfully assigned default/myapp-pod-startup-404 to kube-node2
  Normal   Pulled     19s   kubelet            Container image "ghcr.io/c1t1d0s7/go-myweb:alpine" already present on machine
  Normal   Created    19s   kubelet            Created container myapp
  Normal   Started    19s   kubelet            Started container myapp
  Warning  Unhealthy  9s    kubelet            Startup probe failed: HTTP probe failed with statuscode: 404

```