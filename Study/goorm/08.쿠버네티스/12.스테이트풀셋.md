# 스테이트풀 셋
파드 집합의 디플로이먼트와 스케일링을 관리하며, 파드들의 순서 및 고유성을 보장한다 .

디플로이먼트와 유사하게, 스테이트풀셋은 동일한 컨테이너 스펙을 기반으로 둔 파드들을 관리한다. 디플로이먼트와는 다르게, 스테이트풀셋은 각 파드의 독자성을 유지한다. 이 파드들은 동일한 스팩으로 생성되었지만, 서로 교체는 불가능하다. 다시 말해, 각각은 재스케줄링 간에도 지속적으로 유지되는 식별자를 가진다.

스토리지 볼륨을 사용해서 워크로드에 지속성을 제공하려는 경우, 솔루션의 일부로 스테이트풀셋을 사용할 수 있다. 스테이트풀셋의 개별 파드는 장애에 취약하지만, 퍼시스턴트 파드 식별자는 기존 볼륨을 실패한 볼륨을 대체하는 새 파드에 더 쉽게 일치시킬 수 있다.

```
statefulsets         sts          apps/v1         true        StatefulSet
```

kubectl explain sts.spec 을 확인해보면 디플로이먼트와 비슷한 것을 확인할 수 있다. 다만 serviceName이 필수 값으로 추가적으로 필요하다. 이름 그대로 statefulset 이므로 상태가 있는 애플리케이션을 배포하는 경우에 특화되어 있다. 상태는 당연히 별도의 볼륨에 저장한다. 

스테이트풀 셋은 다음 중 하나 또는 이상이 필요한 애플리케이션에 유용하다.
- 안정된, 고유한 네트워크 식별자
- 안정된, 지속성을 갖는 스토리지
- 순차적인, 정상 배포(graceful deployment)와 스케일링
- 순차적인 자동 롤링 업데이트

rs의 pod가 PVC를 사용하는 것을 생각해보면 rs의 replicas가 늘어나는 경우를 생각해보면 ps의 모든 pod, 컨테이너는 동일한 하나의 PVC에 연결된다. 즉 파드들은 별도의 고유의 상태를 가지는 것이 아니라 동일한 PV를 사용하는 것이다. Deploy나 RS가 PVC, PV를 사용하는 이유는 데이터를 저장하기 위함이 아니라 그곳에 있는 웹 컨텐츠를 제공하기 위함이 더 크다. 그리고 그곳에 있는 상태(예를 들어 HTML, Python 코드 등)은 업데이트를 하기 전 까지는 변하는 것이 아니다. stateless한 웹,앱은 상태를 저장하는 것이 아니라 전달 받은 데이터로 그에 맞는 결과를 줄 뿐이다. 

statefulset의 경우는 deploy나 rs와 달리 각각의 파드들이 고유의 상태를 가지는 것을 위해서 사용하는 리소스이다. 

이때 `sts.spec.serviceName`에 지정해줘야 하는 서비스는 headless 서비스로 ClusterIP가 없어야 한다. 
```
serviceName  <string> -required-
     serviceName is the name of the service that governs this StatefulSet. This service must exist before the StatefulSet, and is responsible for the network identity of the set. Pods get DNS/hostnames that follow the pattern: pod-specific-string.serviceName.default.svc.cluster.local where"pod-specific-string" is managed by the StatefulSet controller.
```

파드 생성시 동일한 이름의 파드를 생성해야하기 때문에 파드 삭제 후 재 생성시에 리소스가 삭제될 때 까지 기다려야하기 때문에 시간이 오래걸릴 수 밖에 없다. 따라서 복제본이 1개인 스테이트풀셋은 굳이 사용할 필요가 없다. 다운 타임이 무조건 발생하기 때문

## 제한사항
- 파드에 지정된 스토리지는 관리자에 의해 PV provisoner를 기반으로하는 스토리지 클래스를 요청해서 프로비저닝하거나 사전에 프로비저닝 되어야한다.
- 스테이트풀셋을 삭제 또는 스케일 다운해도 스테이트풀 셋과 연관된 볼륨이 삭제되지 않는다.
- 스테이트풀셋은 현재 파드의 네트워크 신원을 책임지고 있는 헤드리스 서비스가 필요하다. 사용자가 이 서비스를 사전에 생성할 책임이 있다.
- 스테이트풀셋은 스테이트풀셋의 삭제 시 파드의 종료에 대해 어떠한 보증을 제공하지 않는다. 스테이트풀셋에서는 파드가 순차적이고 정상적으로 종료되도록 하려면, 삭제 전 스테이트풀셋의 스케일을 0으로 축소하면 된다. 

## 예시 
deploy나 rs의 경우 생성되는 파드들은 랜덤한 이름이 붙게 된다. 각 파드들의 고유성이 없기 때문이다. 그러나 sts로 생성되는 파드들은 숫자가 순서대로 붙게 된다. 각 파드들이 고유성을 가지기 때문이다.

```yaml
apiVersion: v1
kind: Service
metadata:
  name: myapp-svc-headless
  labels:
    app: myapp-svc-headless
spec:
  ports:
  - name: http
    port: 80
  clusterIP: None
  selector:
    app: myapp-sts
```

```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: myapp-sts
spec:
  selector:
    matchLabels:
      app: myapp-sts
  serviceName: myapp-svc-headless
  replicas: 2
  template:
    metadata:
      labels:
        app: myapp-sts
    spec:
      containers:
      - name: myapp
        image: ghcr.io/c1t1d0s7/go-myweb:alpine
        ports:
        - containerPort: 8080
```

```
vagrant@kube-control1:~$ kubectl get svc,po
NAME                         TYPE           CLUSTER-IP     EXTERNAL-IP      PORT(S)        AGE
service/myapp-svc-headless   ClusterIP      None           <none>           80/TCP         3m49s

NAME                                          READY   STATUS    RESTARTS         AGE
pod/myapp-sts-0                               1/1     Running   0                2m21s
pod/myapp-sts-1                               1/1     Running   0                2m19s
```

## 파드 신원
### 순서색인
N개의 레플리카가 있는 스테이트풀셋은 스테이트풀셋에 있는 각 파드에 0에서 N-1 까지의 정수를 할당한다. 그리고 이 파드들은 스테이트풀셋에서 고유하다.

스테이트풀셋의 파드를 삭제하더라도 새로운 이름의 랜덤한 파드가 생성되는 것이 아니라 이전 파드와 동일한 파드가 생성된다.
```
vagrant@kube-control1:~$ kubectl get po
NAME                       READY   STATUS    RESTARTS         AGE
myapp-sts-0                1/1     Running   0                11m
myapp-sts-1                1/1     Running   0                11m

vagrant@kube-control1:~$ kubectl delete pod myapp-sts-0
pod "myapp-sts-0" deleted
vagrant@kube-control1:~$ kubectl get pods
NAME                       READY   STATUS    RESTARTS         AGE
myapp-sts-0                1/1     Running   0                6s
myapp-sts-1                1/1     Running   0                11m
```

### 안정적인 네트워크 신원
서비스를 생성하면 당연히 클러스터 내부에서 서비스 이름의 호스트명으로 해당 서비스에 접근할 수 있게 된다. 도메인은 `svc-name.namespace.svc.cluster.local`이 된다. cluster.local 은 클러스터의 도메인이다. 
```
testnet:~# host myapp-svc-headless
myapp-svc-headless.default.svc.cluster.local has address 10.233.74.11
myapp-svc-headless.default.svc.cluster.local has address 10.233.73.115
```
스테이트풀셋은 여기서 추가적으로 각각의 파드에도 도메인으로 접근할 수 있게 된다. 풀네임은 `pod-name.svc-name.namespace.svc.cluster.local` 이다.
```
testnet:~# host myapp-sts-0.myapp-svc-headless
myapp-sts-0.myapp-svc-headless.default.svc.cluster.local has address 10.233.74.11
testnet:~# host myapp-sts-1.myapp-svc-headless
myapp-sts-1.myapp-svc-headless.default.svc.cluster.local has address 10.233.73.115
```

#### 헤드리스 서비스를 사용하는 이유
헤드리스 서비스를 사용하게 되면 dns 서버가 해당 서비스에 매핑된 파드들의 IP를 모두 알려주게 된다.(ClusterIP가 없기 때문에 대신 모든 파드들의 A 레코드를 반환하는 것이다.) 따라서 이를 기반으로 각 파드들의 고유한 도메인을 만들 수 있기 때문에 안정적인 네트워크 신원을 제공할 수 있게 된다. 

### 안정적인 스토리지 신원
스테이트풀셋에 정의된 VolumeClaimTemplate 항목마다, 각 파드는 하나의 PVC를 받게 된다. 즉 각 파드마다 서로 다른 스토리지를 가지게 되고 고유한 상태를 가지게 될 수 있는 것이다. 스토리지 클래스가 명시되지 않은 경우, 기본 스토리지 클래스가 사용된다. 파드가 노드에서 스케줄 혹은 재스케줄이 되면 파드의 volumeMounts는 PVC와 관련된 PV가 마운트된다. 

```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: myapp-sts-vol
spec:
  selector:
    matchLabels:
      app: myapp-sts-vol
  serviceName: myapp-svc-headless
  replicas: 2
  template:
    metadata:
      labels:
        app: myapp-sts-vol
    spec:
      containers:
      - name: myapp
        image: ghcr.io/c1t1d0s7/go-myweb:alpine
        ports:
        - containerPort: 8080
        volumeMounts:
        - name: myapp-data
          mountPath: /data
  volumeClaimTemplates:
  - metadata:
      name: myapp-data
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 1Gi
      storageClassName: nfs-client
```

```
vagrant@kube-control1:~$ kubectl get pvc
NAME                         STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
myapp-data-myapp-sts-vol-0   Bound    pvc-0d55c522-47d7-46c4-8c54-d6a8e1ac7882   1Gi        RWO            nfs-client     15s
myapp-data-myapp-sts-vol-1   Bound    pvc-e43cb1d4-c32d-49e9-bcfb-2db961b5a823   1Gi        RWO            nfs-client     11s
```

#### 주의
스케일링을 하거나 파드를 삭제하더라도 PVC는 삭제되지 않음을 생각해야한다. 파드의 장애 부분을 고치고나서 다시 배포하더라도 연결하는 PVC의 초기화 자체가 잘못되었기 때문에 계속 장애가 될 수도 있다. 따라서 이러한 경우 이를 인지하고 PVC도 삭제해줘야한다.

## 스케일링 보증
-   N개의 레플리카가 있는 스테이트풀셋이 파드를 배포할 때 연속해서 {0..N-1}의 순서로 생성한다.
-   파드가 삭제될 때는 {N-1..0}의 순서인 역순으로 종료된다.
-   파드에 스케일링 작업을 적용하기 전에 모든 선행 파드가 Running 및 Ready 상태여야 한다.
-   파드가 종료되기 전에 모든 후속 파드가 완전히 종료 되어야 한다.

# DB 이중화 예시
[링크](https://kubernetes.io/docs/tasks/run-application/run-replicated-stateful-application/)
web app은 primary DB와 secondary DB를 알고있어야 하기 때문에 헤드리스 서비스를 이용할 수 밖에 없다. 스테이트풀셋은 각각 고유성을 유지하기 위해서 사용하는 것이긴 하지만 DB이중화의 경우에는 볼륨을 sync 하는 것이 필요하다. 
DB의 경우 저장되는 위치가 `/var/lib/<sql>` 로 되게 되는데 똑같은 데이터를 같이 접근하는 경우(nfs 같은 공유스토리지를 사용해서 볼륨을 같이 사용한다거나 멀티마스터, 멀티프라이머리를 사용하는 경우)는 이미 에러가 나기 때문에 불가능하거나 구현하기 힘들다. 따라서 별도의 볼륨을 가지고 한 쪽에서만 r/w를 하게 하고 다른 곳은 read only만 하도록하고 r/w을 하는 쪽(primary)에서는 read only 쪽(secontdary)과 항상 sync 하도록 설정해놓는다. 
**샤딩에서의 사용** 
데이터베이스 샤딩을 하는 경우에 각각 고유한 DB를 가지는 것이므로 이렇게 statefulset을 사용할 수 밖에 없다. 샤딩을 하는 경우 특정 DB에 부하가 많이 가게 될 수도 있다. 