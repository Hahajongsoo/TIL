
워크로드 리소스의 원래 이름은 컨트롤러였고 역할은 파드를 컨트롤하는 것이다.  디플로이먼트와 스테이트풀셋은 네트워크와 볼륨을 한 후에 이야기하는 것이 좋다.
# 레플리케이션 컨트롤러
```
vagrant@kube-control1:~$ kubectl api-resources | grep replication
replicationcontrollers        rc         v1           true         ReplicationController
```
- rc 라는 shortname, api version 은 v1, 네임스페이스를 사용하는 것을 확인할 수 있다.

레플리케이션 컨트롤러는 쿠버네티스가 처음 나올 때 부터 있었던 컨드롤러이며 이를 대체하기 위해 나온 것이 레플리카셋이다. 이후 교체될 것이기 때문에 `쿠버네티스 쿠버네티스 공식문서에 ReplicaSet을 구성하는 Deployment가 현재 권장하는 레플리케이션 설정 방법이다.` 라고 나와있다.

레플리케이션 컨트롤러는 언제든지 지정된 수의 파드 레플리카(복제본)가 실행 중임을 보장한다. 레플리케이션 컨트롤러는 파드 또는 동일 종류의 파드의 셋이 항상 기동되고 사용 가능한지를 확인한다.

## 레플리케이션 컨트롤러의 동작 방식
파드가 너무 많으면 레플리케이션 컨트롤러가 추가적인 파드를 제거한다. 너무 적으면 레플리케이션 컨트롤러는 더 많은 파드를 시작한다. 즉 원하는 파드의 수에 맞게 파드가 실행되게 한다. 따라서 수동으로 생성된 파드와 달리 레플리케이션 컨트롤러가 유지 관리하는 파드는 실패하거나 삭제되거나 종료되는 경우 자동으로 교체된다. 레플리케이션 컨트롤러는 프로세스 감시자(supervisor)와 유사하지만 단일 노드에서 개별 프로세스를 감시하는 대신 여러 노드에서 여러 파드를 감시한다.

레플리케이션 컨트롤러는 결국 파드를 컨트롤하는 것이기 때문에 어떤 파드를 생성할 것인지에 대한 정보가 필요하다. 이것이 `rc.spec.template`에 담기게 된다.  따라서 `rc.spec.template.metadata` 는 `pod.metadata` 와 같고 `rc.spec.template.spec`은 `pod.spec`과 동일하다. 
```
KIND:     ReplicationController
VERSION:  v1

RESOURCE: template <Object>

DESCRIPTION:
     Template is the object that describes the pod that will be created if
     insufficient replicas are detected. This takes precedence over a
     TemplateRef. More info:
     https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller#pod-template

     PodTemplateSpec describes the data a pod should have when created from a
     template

FIELDS:
   metadata     <Object>
     Standard object's metadata. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata

   spec <Object>
     Specification of the desired behavior of the pod. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status
```

## 레플리케이션 컨트롤러 예제
```yaml
apiVersion: v1
kind: ReplicationController
metadata:
  name: nginx
spec:
  replicas: 3
  selector:
    app: nginx
  template:
    metadata:
      name: nginx
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80
```

수 많은 파드 중에서 컨트롤러가 제어해야하는 파드가 어떤 것인지 알려줘야한다. 컨트롤러는 어떤 파드를 관리해야하는지 셀렉터를 이용하여 확인한다. 그래서 파드의 `metadata`에 `labels`를 지정해줘야하며 컨트롤러는 그 `labels` 값을 이용한다.
```
NAME                          DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES   SELECTOR    LABELS
replicationcontroller/nginx   3         3         3       72s   nginx        nginx    app=nginx   app=nginx

NAME          READY   STATUS    RESTARTS      AGE     LABELS
nginx-lln26   1/1     Running   0             2m12s   app=nginx
nginx-pjh9j   1/1     Running   0             2m12s   app=nginx
nginx-xjkjg   1/1     Running   0             2m12s   app=nginx
```

레플리케이션 컨트롤러에서 레이블의 기본값은 `.spec.template.metadata.labels`와 동일하다.  값을 지정하는 경우 해당 레이블이 지정된다.

`kubectl logs` 를 사용하는 경우 기본적으로 파드의 이름을 넣어줘야한다. 파드 부분에 `rc/<name>` 을 사용해도 볼 수 있다. 하지만 레플리케이션 중 하나를 선택해서 보여주기 때문에 좋지않을 수 있다.

# 레플리카셋
레플리카셋의 목적은 레플리카 파드 집합의 실행을 항상 안정적으로 유지하는 것이다. 보통 명시된 동일 파드 개수에 대한 가용성을 보장하는데 사용한다.

레이블 셀렉터에는 일치성 기준과 집합성 기준이 있는데 집합성 기준이 비교적 나중에 나오게 되었다. 이에 따라 레플리케이션 컨트롤러에서 집합성 기준을 사용할 수 있도록 레플리카셋이 나중에 나오게 된 것이다. 이를 `.rc.spec.selector` 와 `.rs.spec.selector`  에서 확인할 수 있다.  `rc`의 경우 일치성 기준에만 사용가능하고 `rs`의 경우 일치성 기준에는 `.rs.spec.selector.matchLabels` 를 집합성 기준에는 `.rs.spec.selector.matchExpressions`를 사용하면된다. 

## 레플리카셋의 작동방식
레플리카셋을 정의하는 필드는 획득 가능한 파드를 식별하는 방법이 명시된 셀렉터, 유지해야하는 파드 개수를 명시하는 레플리카의 개수, 그리고 레플리카 수 유지를 위해 생성하는 신규 파드에 대한 데이터를 명시하는 파드 템플릿을 포함한다. 그러면 레플리카셋은 필드에 지정된 설정을 충족하기 위해 필요한 만큼 파드를 만들고 삭제한다.

## 레플리카셋을 사용하는시기 
레플리카셋은 지정된 수의 파드 레플리카가 항상 실행되도록 보장한다. 디플로이먼트는 레플리카셋을 관리하고 다른 유용한 기능과 함께 파드에 대한 선언적 업데이트를 제공하는 상위 개념이다. 따라서 우리는 사용자 지정 오케스트레이션이 필요하거나 업데이트가 전혀 필요하지 않은 경우가 아니라면 디플로이먼트를 사용하는 것을 추천한다.  

## 파드 셀렉터
- matchExpression 을 사용하는 예시
```yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: myapp-rs-exp
spec:
  replicas: 3
  selector:
    matchExpressions:
      - key: app
        operator: In
        values:
          - myapp-rs-exp
      - key: env
        operator: Exists
  template:
    metadata:
      labels:
        app: myapp-rs-exp
        env: dev
    spec:
      containers:
      - name: myapp
        image: ghcr.io/c1t1d0s7/go-myweb:alpine
        ports:
        - containerPort: 8080
```

# 데몬셋
explain으로 확인해보면 레플리카셋과 다른 부분이 있다. selector와 template이 있는 것은 유사하나 replicas가 없다. 
데몬셋은 모든(또는 일부) 노드가 파드의 사본을 실행하도록 한다. 노드가 클러스터에 추가되면 파드도 추가된다. 노드가 클러스터에서 제거되면 해당 파드는 가비지로 수집된다. 데몬셋을 삭제하면 데몬셋이 생성한 파드들이 정리된다.
데몬셋의 일 부 대표적인 용도는 다음과 같다.
- 모든 노드에서 클러스터 스토리지 데몬 실행
- 모든 노드에서 로그 수집 데몬 실행
- 모든 노드에서 노드 모니터링 데몬 실행
단순한 케이스에서는 각 데몬 유형의 처리를 위해서 모든 노드를 커버하는 한의 데몬 셋이 사용된다. 더 복잡한 구성에서는 단일 유형의 데몬에 여러 데몬셋을 사용할 수 있지만, 각기 다른 하드웨어 유형에 따라 서로 다른 플래그, 메모리, CPU 요구가 달라진다. 

노드들의 리소스 한계로 스케일 아웃을 하는 경우 데몬셋이 있다면 해당 노드에 파드가 자동으로 생성된다. 이러한 경우에 사용할 수 있다. 

보통 데몬셋은 인프라를 위한 인프라를 구성해야하는 경우에 사용한다. 배포해야하는 파드에 사용하는 것이 아니다. 

레플리카셋으로 사용하는 경우 노드의 리소스에 따라서 파드들이 어떻게 배치될지 모른다. 따라서 각 노드에 하나 씩 생성되는 것이 아니라 특정 노드에 파드가 두 개가 생성되는 경우도 발생할 수 있다. 그러나 데몬셋의 경우 리소스가 부족한 노드에는 파드를 생성하지 않는다. 무조건 노드당 파드를 하나씩만 생성하는 것이다. 

## 일부 노드에서만 파드 실행
`.spec.template.spec.nodeSelector`를 명시하면 데몬셋 컨트롤러는 노드 셀렉터와 일치하는 노드에 파드를 생성한다. 이는 template의 spec임을 기억하자. 즉 템플릿을 사용하는 모든 리소스는 파드가 생성될 때 특정 노드에만 생성되도록 정할 수 있다.

## 인프라를 위한 인프라
데몬셋이 인프라를 위한 인프라, 즉 로그, 스토리지, 네트워크 구성 등을 위해 사용된다는 것을 kube-proxy를 통해 확인할 수 있다. 
```
vagrant@kube-control1:~$ kubectl get -n kube-system ds,pod -l k8s-app=kube-proxy
NAME                        DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
daemonset.apps/kube-proxy   4         4         4       4            4           kubernetes.io/os=linux   2d23h

NAME                   READY   STATUS    RESTARTS   AGE
pod/kube-proxy-hmrxw   1/1     Running   0          2d23h
pod/kube-proxy-ksdgf   1/1     Running   0          2d23h
pod/kube-proxy-sx9pq   1/1     Running   0          2d23h
pod/kube-proxy-wvc62   1/1     Running   0          2d23h
```

kube-proxy는 데몬셋에 의해 생성되며 해당 레이블은 k8s-app=kube-proxy 임을 확인할 수 있다. 

## 업데이트 방식
ds.spec.updateStrategy 의 기본값은 RollingUpdate이다. 노드당 파드가 하나씩 밖에 없기 때문에 다운타임은 감수해야한다.

# 잡
잡에서 하나 이상의 파드를 생성하고 지정된 수의 파드가 성공적으로 ==종료==될 때까지 계속해서 파드의 실행을 재시도한다. 컨트롤러를 사용하는 것은 보통 서비스의 지속적인 제공을 위해 사용하는 것이다. 하지만 잡의 경우는 애플리케이션의 성공적인 종료를 보장한다.  파드가 성공적으로 완료되면, 성공적으로 완료된 잡을 추적한다. 지정된 수의 성공 완료에 도달하면, 잡이 완료된다. 잡을 삭제하면 잡이 생성한 파드가 정리된다. 작업을 일시 중지하면 작업이 다시 재개될 때까지 활성 파드가 삭제된다. 

## 예시
```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: pi
spec:
  template:
    spec:
      containers:
      - name: pi
        image: perl:5.34.0
        command: ["perl",  "-Mbignum=bpi", "-wle", "print bpi(2000)"]
      restartPolicy: Never
  backoffLimit: 4
```
잡의 경우 종료되어야 하므로 기본 값이 Always인 restartPolicy를 OnFailure나 Never로 바꿔주어야 한다.  
잡의 경우 셀렉터를 구성할 수 있지만 굳이 하지 않는다. 실수로 컨트롤러에 의해 관리되고 있는 파드를 셀렉팅 할 경우 파드를 계속 실행시키는 컨트롤러와 파드를 종료시키는 잡과의 충돌이 일어나게 될 수 있다. 따라서 셀렉터를 작성하지 않는 것을 권장한다. 

`.spec.completion` 에 잡을 완료할 횟수를 지정할 수 있다. `.spec.parallelism` 으로 병렬로 실행될 규모를 지정할 수도 있다. `.spec.activeDeadlineSeconds`를 사용하면 해당 시간 내에 잡이 완료되지 않는 경우 잡이 종료된다. 

# 크론잡
반복 일정에 따라 잡을 만든다. 하나의 크론잡 오브젝트는 크론탭 파일의 한 줄과 같다. 크론잡은 잡을 크론 형식으로 쓰여진 주어진 일정에 따라 주기적으로 동작시킨다. 
모든 크론잡 `.spec.schedule`은 kube-controller-manager의 시간대를 기준으로 한다. 
크론잡은 잡을 만들고 잡은 파드를 생성한다. 그래서 크론잡의 `.spec.jopTemplat` 이 존재하는 것이다. 
`.spec.concurrencyPolicy` 로 job이 동시에 실행 가능한지 아닌지, 대체할지 지정할 수 있다. 

`kubectl edit` 을 이용하여 cronjob의 suspend 값을 바꿔서 일시정지 시킬 수 있다.

`startingDeadlineSeconds`가 큰 값으로 설정되거나 설정되지 않고, `concurrencyPolicy`가 `Allow`로 설정될 경우, 잡은 항상 적어도 한 번은 실행될 것이다. 모든 크론잡에 대해 크론잡 컨트롤러는 마지막 일정부터 지금까지 얼마나 많은 일정이 누락되었는지 확인한다. 만약 100회 이상의 일정이 누락되었다면, 잡을 실행하지 않고 아래와 같은 에러 로그를 남긴다. 
```
Cannot determine if job needs to be started. Too many missed start time (> 100). Set or decrease .spec.startingDeadlineSeconds or check clock skew.
```
만약 `startingDeadlineSeconds`가 설정되면 컨트롤러는 마지막 실행부터 지금까지를 기준으로 잡지 않고 `startingDeadlineSeconds` 값을 기준으로 몇 개의 잡이 누락되었는지 카운팅한다. 예를들어 `startingDeadlineSeconds`이 200이라면 컨트롤러는 최근 200초 동안 누락된 잡을 카운팅 한다. 

크론잡은 예약된 시간에 생성하는 것에 실패하면 누락된 것으로 카운팅한다. 예를 들어 `concurrencyPolicy`가 `Forbid`로 되어있고 이전 스케줄이 아직 실행중일 때 크론잡이 스케줄링되려고 한다면 이는 누락된 것으로 카운팅될 것이다. 

즉, 크론잡이 `08:30:00` 에 시작하여 매 분마다 새로운 잡을 실행하도록 설정이 되었고, `startingDeadlineSeconds` 값이 설정되어 있지 않는다고 가정해보자. 만약 크론잡 컨트롤러가 `08:29:00` 부터 `10:21:00` 까지 고장이 나면, 일정을 놓친 작업 수가 100개를 초과하여 잡이 실행되지 않을 것이다.

이 개념을 더 자세히 설명하자면, 크론잡이 `08:30:00` 부터 매 분 실행되는 일정으로 설정되고, `startingDeadlineSeconds` 이 200이라고 가정한다. 크론잡 컨트롤러가 전의 예시와 같이 고장났다고 하면 (`08:29:00` 부터 `10:21:00` 까지), 잡은 10:22:00 부터 시작될 것이다. 이 경우, 컨트롤러가 마지막 일정부터 지금까지가 아니라, 최근 200초 안에 얼마나 놓쳤는지 체크하기 때문이다. (여기서는 3번 놓쳤다고 체크함)