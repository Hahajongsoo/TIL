aws에서 쿠버네티스 자원을 사용하기 위한 리소스이다.
eks를 사용하기 위한 방법 중에서는 ekscli를 사용하는 것을 권장한다. 
ekscli를 사용하기 위해서는 kubectl, eksctl, aws-cli, aws-iam-authenticator 가 필요하다. 
- 먼저 aws 계정 IAM에 액세스 키를 생성한다. 이후 aws configure를 이용하여 해당 액세스 키를 등록한다.
```
hajong@hajong-H87M-D3H:~$ aws configure
AWS Access Key ID [None]: ****************
AWS Secret Access Key [None]: *****************
Default region name [None]: ap-northeast-2
Default output format [None]:
```
- aws sts get-caller-identity 를 사용하여 현재 인증된 사용자를 확인해본다.
```
hajong@hajong-H87M-D3H:~$ aws sts get-caller-identity
{
    "UserId": "AIDAZ72NSUCEXHTNSWCW7",
    "Account": "686820597897",
    "Arn": "arn:aws:iam::686820597897:user/nrf0028"
}
```
- 이후 eksctl로 클러스터를 생성한다. aws에서는 클러스터 배포에 시간이 좀 걸린다고 한다. (15~30분)
```
eksctl create cluster --name myeks --region ap-northeast-2
```

![](images/Pasted%20image%2020230227121129.png)

들어가서 확인해보면 서버 주소, 인증서, 리소스, 노드(ec2 인스턴스), VPC 등을 확인할 수 있다.

- ec2 에서는 생성된 ec2, 오토스케일링 그룹, 시작 템플릿, elastic IP, 볼륨 등을 확인할 수 있다.
- VPC에서 6개의 서브넷(pub 3, prv 3)과 NAT 게이트웨이 등을 확인할 수 있다. 

- eks 배포시의 로그는 다음과 같다.
```
2023-02-27 11:50:54 [ℹ]  eksctl version 0.131.0
2023-02-27 11:50:54 [ℹ]  using region ap-northeast-2
2023-02-27 11:50:54 [ℹ]  setting availability zones to [ap-northeast-2a ap-northeast-2d ap-northeast-2c]
2023-02-27 11:50:54 [ℹ]  subnets for ap-northeast-2a - public:192.168.0.0/19 private:192.168.96.0/19
2023-02-27 11:50:54 [ℹ]  subnets for ap-northeast-2d - public:192.168.32.0/19 private:192.168.128.0/19
2023-02-27 11:50:54 [ℹ]  subnets for ap-northeast-2c - public:192.168.64.0/19 private:192.168.160.0/19
2023-02-27 11:50:54 [ℹ]  nodegroup "ng-a590e66a" will use "" [AmazonLinux2/1.24]
2023-02-27 11:50:54 [ℹ]  using Kubernetes version 1.24
2023-02-27 11:50:54 [ℹ]  creating EKS cluster "myeks" in "ap-northeast-2" region with managed nodes
2023-02-27 11:50:54 [ℹ]  will create 2 separate CloudFormation stacks for cluster itself and the initial managed nodegroup
2023-02-27 11:50:54 [ℹ]  if you encounter any issues, check CloudFormation console or try 'eksctl utils describe-stacks --region=ap-northeast-2 --cluster=myeks'
2023-02-27 11:50:54 [ℹ]  Kubernetes API endpoint access will use default of {publicAccess=true, privateAccess=false} for cluster "myeks" in "ap-northeast-2"
2023-02-27 11:50:54 [ℹ]  CloudWatch logging will not be enabled for cluster "myeks" in "ap-northeast-2"
2023-02-27 11:50:54 [ℹ]  you can enable it with 'eksctl utils update-cluster-logging --enable-types={SPECIFY-YOUR-LOG-TYPES-HERE (e.g. all)} --region=ap-northeast-2 --cluster=myeks'
2023-02-27 11:50:54 [ℹ]  
2 sequential tasks: { create cluster control plane "myeks", 
    2 sequential sub-tasks: { 
        wait for control plane to become ready,
        create managed nodegroup "ng-a590e66a",
    } 
}
2023-02-27 11:50:54 [ℹ]  building cluster stack "eksctl-myeks-cluster"
2023-02-27 11:50:55 [ℹ]  deploying stack "eksctl-myeks-cluster"
2023-02-27 11:51:25 [ℹ]  waiting for CloudFormation stack "eksctl-myeks-cluster"
2023-02-27 11:51:55 [ℹ]  waiting for CloudFormation stack "eksctl-myeks-cluster"
2023-02-27 11:52:55 [ℹ]  waiting for CloudFormation stack "eksctl-myeks-cluster"
2023-02-27 11:53:55 [ℹ]  waiting for CloudFormation stack "eksctl-myeks-cluster"
2023-02-27 11:54:55 [ℹ]  waiting for CloudFormation stack "eksctl-myeks-cluster"
2023-02-27 11:55:55 [ℹ]  waiting for CloudFormation stack "eksctl-myeks-cluster"
2023-02-27 11:56:56 [ℹ]  waiting for CloudFormation stack "eksctl-myeks-cluster"
2023-02-27 11:57:56 [ℹ]  waiting for CloudFormation stack "eksctl-myeks-cluster"
2023-02-27 11:58:56 [ℹ]  waiting for CloudFormation stack "eksctl-myeks-cluster"
2023-02-27 11:59:56 [ℹ]  waiting for CloudFormation stack "eksctl-myeks-cluster"
2023-02-27 12:00:56 [ℹ]  waiting for CloudFormation stack "eksctl-myeks-cluster"
2023-02-27 12:01:56 [ℹ]  waiting for CloudFormation stack "eksctl-myeks-cluster"
2023-02-27 12:02:56 [ℹ]  waiting for CloudFormation stack "eksctl-myeks-cluster"
2023-02-27 12:04:57 [ℹ]  building managed nodegroup stack "eksctl-myeks-nodegroup-ng-a590e66a"
2023-02-27 12:04:58 [ℹ]  deploying stack "eksctl-myeks-nodegroup-ng-a590e66a"
2023-02-27 12:04:58 [ℹ]  waiting for CloudFormation stack "eksctl-myeks-nodegroup-ng-a590e66a"
2023-02-27 12:05:28 [ℹ]  waiting for CloudFormation stack "eksctl-myeks-nodegroup-ng-a590e66a"
2023-02-27 12:06:15 [ℹ]  waiting for CloudFormation stack "eksctl-myeks-nodegroup-ng-a590e66a"
2023-02-27 12:07:18 [ℹ]  waiting for CloudFormation stack "eksctl-myeks-nodegroup-ng-a590e66a"
2023-02-27 12:08:31 [ℹ]  waiting for CloudFormation stack "eksctl-myeks-nodegroup-ng-a590e66a"
2023-02-27 12:09:05 [ℹ]  waiting for CloudFormation stack "eksctl-myeks-nodegroup-ng-a590e66a"
2023-02-27 12:09:05 [ℹ]  waiting for the control plane to become ready
2023-02-27 12:09:05 [✔]  saved kubeconfig as "/home/hajong/.kube/config"
2023-02-27 12:09:05 [ℹ]  no tasks
2023-02-27 12:09:05 [✔]  all EKS cluster resources for "myeks" have been created
2023-02-27 12:09:05 [ℹ]  nodegroup "ng-a590e66a" has 2 node(s)
2023-02-27 12:09:05 [ℹ]  node "ip-192-168-43-255.ap-northeast-2.compute.internal" is ready
2023-02-27 12:09:05 [ℹ]  node "ip-192-168-71-13.ap-northeast-2.compute.internal" is ready
2023-02-27 12:09:05 [ℹ]  waiting for at least 2 node(s) to become ready in "ng-a590e66a"
2023-02-27 12:09:05 [ℹ]  nodegroup "ng-a590e66a" has 2 node(s)
2023-02-27 12:09:05 [ℹ]  node "ip-192-168-43-255.ap-northeast-2.compute.internal" is ready
2023-02-27 12:09:05 [ℹ]  node "ip-192-168-71-13.ap-northeast-2.compute.internal" is ready
2023-02-27 12:09:05 [ℹ]  kubectl command should work with "/home/hajong/.kube/config", try 'kubectl get nodes'
2023-02-27 12:09:05 [✔]  EKS cluster "myeks" in "ap-northeast-2" region is ready
```
- 리전선택 후 가용영역을 선택한다. 그리고 각 가용영역에 서브넷을 생성한다.
- 노드 그룹을 만든다.(오토 스케일링)
- cloud formation 스택을 만든다. 템플릿에 따라서 두 개의 스택을 만든다. 하나는 컨트롤 플레인이고 하나는 노드이다.
	- cloud formation은 aws의 IaC 툴이다. 
	- cloud formation에서 스택의 이벤트들을 확인하면 어떤 리소스들을 생성했는지 확인할 수 있고 템플릿도 확인할 수 있다.
- 이후에는 컨트롤 플레인을 만들고 컨트롤 플레인을 배포한다. 그리고 노드를 만들고 노드를 배포한다. 
- 완료되면 kubeconfig에 관련 내용을 저장한다. aws의 경우 aws-iam-authenticator 을 통해서 iam 인증을 하게 된다.
```
hajong@hajong-H87M-D3H:~$ kubectl config view
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: DATA+OMITTED
    server: https://192.168.56.11:6443
  name: cluster.local
- cluster:
    certificate-authority-data: DATA+OMITTED
    server: https://DAFD69250FB3334006DB2540B619B289.sk1.ap-northeast-2.eks.amazonaws.com
  name: myeks.ap-northeast-2.eksctl.io
contexts:
- context:
    cluster: cluster.local
    namespace: default
    user: kubernetes-admin
  name: kubernetes-admin@cluster.local
- context:
    cluster: myeks.ap-northeast-2.eksctl.io
    user: nrf0028@myeks.ap-northeast-2.eksctl.io
  name: nrf0028@myeks.ap-northeast-2.eksctl.io
current-context: nrf0028@myeks.ap-northeast-2.eksctl.io
kind: Config
preferences: {}
users:
- name: kubernetes-admin
  user:
    client-certificate-data: REDACTED
    client-key-data: REDACTED
- name: nrf0028@myeks.ap-northeast-2.eksctl.io
  user:
    exec:
      apiVersion: client.authentication.k8s.io/v1beta1
      args:
      - token
      - -i
      - myeks
      command: aws-iam-authenticator
      env:
      - name: AWS_STS_REGIONAL_ENDPOINTS
        value: regional
      - name: AWS_DEFAULT_REGION
        value: ap-northeast-2
      interactiveMode: IfAvailable
      provideClusterInfo: false
```

```
hajong@hajong-H87M-D3H:~$ kubectl config get-clusters
NAME
myeks.ap-northeast-2.eksctl.io
cluster.local
```

![](images/Pasted%20image%2020230227140453.png)

```
eksctl create cluster -f cluster.yaml
```

```
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: basic-cluster
  region: eu-north-1

nodeGroups:
  - name: ng-1
    instanceType: m5.large
    desiredCapacity: 10
  - name: ng-2
    instanceType: m5.xlarge
    desiredCapacity: 2
```

이런 식으로 yaml로 생성 조건을 지정해줄 수 도 있다. config 파일의 스키마도 확인할 수 있다. [링크](https://eksctl.io/usage/schema/)
config의 예시들도 있다. [링크](https://github.com/weaveworks/eksctl/tree/main/examples) 
eks 등 다른 솔루션들에서 제공하는 쿠버네티스 버전은 보통 1.24~1.25이다. 이 조건에서 학습하는 것이 좋겠으나. CKA 에서는 최신 버전으로 시험을 보기는 한다. 

```yaml
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: mycluster
  region: ap-northeast-2
  version: "1.24"

#AZ
availabilityZones: ["ap-northeast-2a", "ap-northeast-2b",  "ap-northeast-2c", "ap-northeast-2d"]

# IAM OIDC & Service Account
iam:
  withOIDC: true
  serviceAccounts:
    - metadata:
        name: aws-load-balancer-controller
        namespace: kube-system
      wellKnownPolicies:
        awsLoadBalancerController: true
    - metadata:
        name: ebs-csi-controller-sa
        namespace: kube-system
      wellKnownPolicies:
        ebsCSIController: true
    - metadata:
        name: cluster-autoscaler
        namespace: kube-system
      wellKnownPolicies:
        autoScaler: true

# Unmanaged Node Groups
nodeGroups:
  # On-Demand Instance / Public Network / SSH
  - name: ng-1
    instanceType: t3.medium
    desiredCapacity: 1
    availabilityZones: ["ap-northeast-2a", "ap-northeast-2b"]
    ssh:
      allow: true
      publicKeyPath: ./eks-key.pub

  # Spot Instances / Scaling / Private Network
  # IAM Policy: AutoScaler, ALB Ingress, CloudWatch, EBS
  - name: ng-spot-2
    minSize: 1
    desiredCapacity: 2
    maxSize: 3
    privateNetworking: true
    instancesDistribution:
      maxPrice: 0.01
      instanceTypes: ["t3.small", "t3.medium"]
      onDemandBaseCapacity: 0
      onDemandPercentageAboveBaseCapacity: 0
      spotInstancePools: 2
    availabilityZones: ["ap-northeast-2c", "ap-northeast-2d"]
    iam:
      withAddonPolicies:
        autoScaler: true
        albIngress: true
        cloudWatch: true
        ebs: true

  # Mixed(On-Demand/Spot) Instances
  - name: ng-mixed-3
    desiredCapacity: 2
    instancesDistribution:
      maxPrice: 0.01
      instanceTypes: ["t3.small", "t3.small"]
      onDemandBaseCapacity: 1
      onDemandPercentageAboveBaseCapacity: 50

# Managed Node Groups
managedNodeGroups:
  # On-Demand Instance
  - name: managed-ng-1
    instanceType: t3.small
    desiredCapacity: 2

  # Spot Instance
  - name: managed-ng-spot-2
    instanceTypes: ["t3.small", "t3.medium"]
    desiredCapacity: 1
    spot: true

# Fargate Profiles
fargateProfiles:
  - name: fg-1
    selectors:
    - namespace: dev
      labels:
        env: fargate

# CloudWatch Logging
cloudWatch:
  clusterLogging:
    enableTypes: ["api", "scheduler"]
```

OIDC는 Open ID connection의 약어이다. 일반 계정을 사용하는 경우 외부 인증이 필요하고 외부 인증과 연결하기 위한 표준이 OIDC이다. 이 값을 true로 해야 iam 인증이 쿠버네티스와 연동되게 된다. aws eks의 경우 aws의 기능들과 쿠버네티스를 연동시켜줘야 한다. 이를 위해 서비스 계정이 필요하고 해당 리소스를 제어하기 위한 역할이 필요하다. wellKnownPolicies로 해당 aws의 역할을 만들고 서비스 계정에 바인딩하는 것이다. 
나중에 필요할 때 만들 수도 있지만 그렇게 하는 경우 나중에 계정과 역할을 따로 만들어서 바인딩해야하는데 이 방법은 어렵다고한다.

EKS는 관리형 솔루션이기 때문에 컨트롤 플레인을 aws에서 관리하고 우리는 컨트롤 플레인, etcd 등을 확인할 수 없다.  

노드를 관리형으로 구성할지, 비관리형으로 구성할지 지정할 수 있다. 관리형 노드와 비 관리형 노드의 파라미터는 조금 다르다. 노드를 관리형으로 만드는 경우 노드를 관리까지 해준다. 
인스턴스의 형태를 온디맨드, 스팟 등 다른 방식으로 지정할 수도 있다. 스팟의 경우는 잡 같은 것을 실행할 때 사용한다고 함
api-server의 로그에 접근할 방법이 없기 때문에 cloud watch에 해당 로그들을 남기는 방식을 취한다. 

실제 구성 파일은 다음과 같다. ec2 인스턴스에도 해당 역할들을 부여해줘야 한다. 

```yaml
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: myeks
  region: ap-northeast-2
  version: "1.24"

# AZ
availabilityZones: ["ap-northeast-2a", "ap-northeast-2b",  "ap-northeast-2c"]

# IAM OIDC & Service Account
iam:
  withOIDC: true
  serviceAccounts:
    - metadata:
        name: aws-load-balancer-controller
        namespace: kube-system
      wellKnownPolicies:
        awsLoadBalancerController: true
    - metadata:
        name: ebs-csi-controller-sa
        namespace: kube-system
      wellKnownPolicies:
        ebsCSIController: true
    - metadata:
        name: cluster-autoscaler
        namespace: kube-system
      wellKnownPolicies:
        autoScaler: true

# Managed Node Groups
managedNodeGroups:
  # On-Demand Instance
  - name: mynodes-t3
    instanceType: t3.medium
    minSize: 1
    desiredCapacity: 2
    maxSize: 3
    privateNetworking: true
    #ssh:
      #allow: true
      #publicKeyPath: ./keypair/myeks.pub
    availabilityZones: ["ap-northeast-2a", "ap-northeast-2b", "ap-northeast-2c"]
    iam:
      withAddonPolicies:
        autoScaler: true
        albIngress: true
        cloudWatch: true
        ebs: true

# Fargate Profiles
fargateProfiles:
  - name: myfg
    selectors:
    - namespace: dev
      labels:
        env: dev
        
# CloudWatch Logging
cloudWatch:
  clusterLogging:
    enableTypes: ["*"]
```

위에서 생성해준 sa를 확인해보면 어노테이션이 달려있는 것을 확인할 수 있다. 
지금까지 배웠던 여러가지 기술들을 온프레미스 환경과 클라우드 환경에서 바뀌는 부분에 대해서 이해해야 한다. Service(NodePort, LoadBalancer, Ingress, SC, HPA 등이 잘 되는지 등을 확인해봐야한다. 

- 먼저 노드들을 확인해보면 노드들은 모두 prv에 존재하기 때문에 외부에서 접근할 수 없다. (privateNetworking: true로 되어있기 때문에 인스턴스들이 prv에 생성된다. )따라서 NodePort를 열어도 접근할 수 없다. 또한 public에 노드들이 있다하더라도 해당 포트가 보안그룹에 포함되어있지 않기 때문에 접근할 수 없다. 
```
$ kubectl get nodes -o wide
NAME                                                 STATUS   ROLES    AGE   VERSION                INTERNAL-IP       EXTERNAL-IP   OS-IMAGE         KERNEL-VERSION                  CONTAINER-RUNTIME
ip-192-168-127-135.ap-northeast-2.compute.internal   Ready    <none>   97m   v1.24.10-eks-48e63af   192.168.127.135   <none>        Amazon Linux 2   5.10.165-143.735.amzn2.x86_64   containerd://1.6.6
ip-192-168-138-157.ap-northeast-2.compute.internal   Ready    <none>   97m   v1.24.10-eks-48e63af   192.168.138.157   <none>        Amazon Linux 2   5.10.165-143.735.amzn2.x86_64   containerd://1.6.6
```
- LoadBalancer를 생성하게 되면 로컬에서는 MetalLB를 사용했었다. aws에서는 대신에 cloud controller manager가 aws elb를 생성하게 된다. 외부에 로드밸런서를 두고 내부망에 전달해주는 것이 안전하다는 것이다. 그런데 이때 생성되는 LB는 CLB이고 이는 기능적 제약이 있어서 좋지 않다. 그래서 애드온을 설치하여 NLB가 생성되게 해줘야한다. 
- Ingress의 경우도 마찬가지로 ALB를 생성해야 하는데 애드온이 없다면 리소스 도 생기지 않고 aws에 LoadBalancer도 생성되지 않는다.  [AWS Load Balancer Controller](https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.4/) 를 설치해야 한다. 위에서 관련 SA를 만든 이유가 바로 이것이다. 해당 리소스를 파드가 생성할 수 있어야 하기 때문이다. 
- 볼륨도 마찬가지로 PVC를 생성하려면 ebs를 제어할 수 있는 애드온이 필요하다. 클러스터가 생성되면서 스토리지 클래스를 생성하지만 프로비저너가 없기 때문에 PV를 만들 수 없다. 그래서 EBS CSI Driver Provisioner 를 설치해야 한다. 
- metrics server가 설치되어있지 않기 때문에 top을 사용할 수 없다. 또한 HPA도 사용할 수 없다.

쿠버네티스 서버는 당연히 모두 다 같은 서버이다. 그러나 로컬환경인지 클라우드환경인지에 따라서 클라우드 환경이라면 어떤 클라우드 솔루션인지에 따라서 구성 방법은 다 다를 것이다. 이에 따라 다른 구성을 해야한다는 것을 알고 있어야 한다.