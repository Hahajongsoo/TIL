{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53c1ff5b",
   "metadata": {},
   "source": [
    "## 분류(Classification) : <br>주어진 데이터(X)를 분류하고자 하는 값(y)에 할당하는 방법\n",
    "\n",
    "\n",
    "- 분류란 주어진 input data를 찾고자 하는 target value에 assign 하는 것을 말합니다.\n",
    "\n",
    "\n",
    "- input data는 일반적으로 벡터이며, (feature vector)<br> target value는 일반적으로 scalar입니다.(integer)\n",
    "\n",
    "\n",
    "- 비슷한 특징을 가지는 데이터가 같은 분류로 나뉘어 지는 것을 서로 상대적으로 가까운 feature vector들이 같은 tartget value를 부여받는 것 입니다.\n",
    "\n",
    "\n",
    "- 예를 들어, 고양이와 강아지 사진이 섞여있는 2000장의 이미지가 있다고 친다면<br> 고양이 사진을 머신러닝 모델이 판단하여 'Cat'이라고 label을 부여한다.\n",
    "\n",
    "\n",
    "- 여기서 라벨은 0 또는 1로 표기하는 것이 일반적\n",
    "\n",
    "\n",
    "- 주어진 이미지는 feature vector로 표현이 됩니다.\n",
    "\n",
    "\n",
    "- 머신러닝 분류 모델을 거치면, inference 결과로 0 또는 1이 나오도록 하는 문제를 **binary classification**이라고 합니다.\n",
    "\n",
    "\n",
    "- 위의 2000장은 모두 label이 있어야 하며 해당 label을 통해서 inference 결과가 맞았는지 아닌지를 확인할 수 있습니다.\n",
    "\n",
    "\n",
    "- 이 때 머신러닝 분류 모델에는 다음과 같은 모델들이 있습니다.\n",
    "\n",
    "1. Linear Classifier\n",
    "2. Logistic Regression\n",
    "3. Navie Bayes\n",
    "4. KNN\n",
    "5. SVM\n",
    "6. Random Forest\n",
    "7. Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d668627c",
   "metadata": {},
   "source": [
    "### Linear Classifier : <br> $y = Wx + b$로 표시되는 선형 함수로 데이터를 분류하는 모델\n",
    "\n",
    "- linear classifier는 하나의 선형식으로 데이터를 나누는 방법을 말합니다.\n",
    "\n",
    "\n",
    "- 38선으로 북한과 남한을 나누는 것을 생각해보면 직관적으로 이해하기 쉽습니다.\n",
    "\n",
    "\n",
    "- 하나의 선형식으로 위/아래로 공간이 나뉘기 때문에, 위는 1로 아래는 0으로 inference를 하게 되면 두 가지의 케이스로 데이터를  분류할 수 있습니다.\n",
    "\n",
    "\n",
    "\n",
    "**Go Detail**\n",
    "\n",
    "$y=Wx+b$의 식을 자세히 들여다 보면, 다음과 같이 표시할 수 있습니다.\n",
    "\n",
    "$\\rightarrow y=w_1*x_1+w_2*x_2 + \\dots + w_n*x_n + b$\n",
    "\n",
    "이렇게 일반화된 수식을 이해를 돕기 위해서 두개의 항을 가지고 생각해 보겠습니다.\n",
    "\n",
    "$\\rightarrow y=w_1*x_1+w_2*x_2 + b$\n",
    "\n",
    "$\\rightarrow \\hat{y}=3x_1-2*x_2 + 1, x = (1, 2) \\rightarrow \\hat{y}=3*1 -2*2+1=0\\,(output\\ score) \\rightarrow 1 \\, (predicted\\ value)$\n",
    "\n",
    "$if\\ target\\ value = 1, \\ correct$\n",
    "\n",
    "$\\rightarrow x=(3, -1) \\rightarrow \\hat{y} = 3*(-3)-1*1+1 = -9\\,(output\\ score) \\rightarrow 0 \\, (predicted\\ value)$\n",
    "\n",
    "$if\\ target\\ value = 1, \\ incorrect$\n",
    "\n",
    "위의 수식에서$(x_1,x_2)$는 2차원 feature vector이며, $y$는 output score입니다.<br> (output score란, predicted value를 계산하기 위해서 필요한 값입니다.)\n",
    "\n",
    "\n",
    "처음에 random으로 세팅된 파라미터 값을 가지고 하나의 데이터에 대해서 $y$값을 계산합니다 계산된 $y$값이 0보다 크다면 1을 아니라면 0을 predicted value로 출력합니다.\n",
    "\n",
    "\n",
    "이 때 0을 기준으로 하는 이유는, 선형식 위에 있는 점인 경우 계산 결과가 0이기 때문에 0보다 크냐 작냐에 따라서 선형식을 위냐 아래냐를 판단할 수 있기 때문입니다.\n",
    "\n",
    "**<U>그렇게 되면, $\\hat{y}$는 0 또는 1이 되며, $y$도 0 또는 1이므로 맞고 틀림을 체크할 수 있습니다.</U>**\n",
    "\n",
    "\n",
    "- 따라서, linear classifier로 학습을 했다는 건 데이터를 잘 나누는 적절한 파라미터 $W$ 와 $b$를 찾는 것이 됩니다. 여기서 $x$와 $y$는 데이터\n",
    "\n",
    "\n",
    "- 이 경계면을 **decision boundary**라고 하며, 이 경계면을 linear classifier라고 합니다. \n",
    "\n",
    "\n",
    "- 경계면을 가지고 있다면 데이터를 나누는 예측을 할 수 있다. 데이터를 통해서 데이터를 잘 나눠주는 정보를 파악한 것 이다. 그 정보는 $W$와 $b$이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9768e843",
   "metadata": {},
   "source": [
    "### Logistic Regression: <br> 주어진 데이터(X)를 통해서 사건의 발생 확률(y)를 예측하는 통계 모델\n",
    "\n",
    "\n",
    "- Logistic Regression은 대표적인 이진 분류 모델입니다.\n",
    "\n",
    "\n",
    "#### Linear Regression에 대한 간단한 이해\n",
    "\n",
    "- Linear Regression은 전체 데이터의 <U>경향성을 파악하는 선형식</U>을 구하는 방법을 말합니다.\n",
    "\n",
    "\n",
    "- Linear Regression은 기본적으로 특정값(target value)을 예측하는 것을 목표로 합니다.\n",
    "\n",
    "\n",
    "- 특정 값을 찾기 위해서는 주어진 데이터(training data)의 패턴을 찾아야 합니다.\n",
    "\n",
    "\n",
    "- 그 패턴을 선형식으로 파악하는 것이 Linear Regression이라고 합니다.\n",
    "\n",
    "\n",
    "- 경향성을 찾게 된다면 그 경향성은 test data를 넣었을 때 예측하는 값이 된다. 특정 value를 선형식으로 예측하는 것\n",
    "\n",
    "\n",
    "#### Logistic Regression\n",
    "\n",
    "\n",
    "- Logistic Regression은 Linear Regression을 분류 모델로써 확장한 모델입니다.\n",
    "\n",
    "\n",
    "- Linear Regression은 특정 수치값(numeric value, continuous)을 예측하는 것에는 좋지만, 특정 카테고리(categorical value, discrete)를 예측하는데는 적합하지 않았습니다.\n",
    "\n",
    "\n",
    "- Lineaer Regression 결과에 적당한 함수를 적용하여 output score를 0과 1사이의 값으로 변환하는 것으로 카테고리가 나올 확률을 예측하는 문제로 변환합니다.\n",
    "\n",
    "\n",
    "- 이 확률 값은 예측값이 1이 될 확률이며, 이 확률이 0.5를 넘기면 1로 예측하고 그렇지 않다면 0으로 예측하는 분류 모델로서 사용할 수 있습니다.\n",
    "\n",
    "\n",
    "**Go Detail**\n",
    "\n",
    "Logistic Regression 모델은 다음과 같은 수식으로 표현됩니다.\n",
    "\n",
    "아래 수식은 Linear Regression의 결과를 확장한 것 입니다.\n",
    "\n",
    "$P(Y=1 \\vert x) = {1 \\over{1 + e^{-(Wx+b)}}}$ (Equation A)\n",
    "\n",
    "Linear Regression의 계산 결과를 $z$라고 하면, Logit funcion(또는  Sigmmoid funcion)을 계산한 결과가 위의 수식입니다.\n",
    "\n",
    "Logit Function은 다음과 같습니다.\n",
    "\n",
    "$\\phi(z) = {1 \\over{1 + e^{-z}}}$\n",
    "\n",
    "Logit function은 무한대의 범위를 가지는 $z$를 [0,1] 사이의 값으로 바꿔줍니다.\n",
    "\n",
    "이때 [0, 1]사이의 값은 <U>예측값이 1이 될 확률로 해석</U>됩니다.\n",
    "\n",
    "즉 Equation A를 봤을 때 input x가 들어가고 최종적으로 [0,1] 사이의 확률이 나온다고 생각하시면 되며 그 확률은 최종적으로 1이라는 target value가 나올 확률입니다. 이 확률이 0.5를 넘으면 1이라고 예측 값을 출력합니다.\n",
    "\n",
    "\n",
    "여기서도 업데이트 되는 파라미터는 여전히 $W$와 $b$입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3e8736",
   "metadata": {},
   "source": [
    "### Decision Tree : <br> 조건에 따라서 데이터를 분류하는 모델\n",
    "\n",
    "\n",
    "- Decision Tree는 대표적인 **non-parametric** 모델입니다. (non-parametic모델이 좋다고 알려져있다. 그리고 이를 확장한 형태의 모델들이 있다.)\n",
    "\n",
    "\n",
    "- Decision Tree는 대표적인 **white-box** 모델입니다. (모델이 어떻게 작동하는지 명확히 알 수 있다. 설명가능하다. explainable, interpretable)\n",
    "\n",
    "\n",
    "- 우리가 지금부터 다루는 Decision Tree 모델은 CART(Classification And Regression Tree)입니다. (분류와 회귀 둘 다 가능하다. 모델의 기준만 회귀에 맞게, 분류에 맞게 바꿔주면 둘 다 가능하다.)\n",
    "\n",
    "\n",
    "#### Tree Structure\n",
    "\n",
    "- Node, Edge\n",
    "\n",
    "- Root, Leaf, Parent, Children(left, right), Siblings\n",
    "\n",
    "- Subtree\n",
    "\n",
    "- hight of tree, level, depth\n",
    "\n",
    "- Tree 중에서 children이 최대 2개인 tree를 Binary Tree라고 합니다.\n",
    "\n",
    "\n",
    "#### CART(Classification And Regression Tree)\n",
    "\n",
    "\n",
    "- CART는 대표적인 Decision Tree 모델중에 하나입니다.\n",
    "\n",
    "\n",
    "- CART는 binary tree입니다.\n",
    "\n",
    "\n",
    "- CART는 노드마다 fearture 하나를 골라서 최적의 기준으로 나눌 수 있게 기준을 정합니다. (노드 하나하나는 feature 하나를 대상으로 하고 특정 feature에서 기준을 주고 그것에 따라서 data를 나눈다)\n",
    "\n",
    "\n",
    "- 이 때 최적이 되는 기준은 \"Gini criterion\"을 사용합니다.\n",
    "\n",
    "\n",
    "- Gini Criterion은 불순도(impurity)를 의미합니다. 불순도란 불순한 정도 즉 섞여있는 정도를 의미합니다. 불순도가 제일 낮은 경우가 서로 제일 안섞여 있는 경우입니다.\n",
    "\n",
    "\n",
    "- 즉 Gini Criterion이 0이 될 때가 깔끔하게 나뉘어 있는 경우입니다. (Best case)\n",
    "\n",
    "\n",
    "- <U>CART 모델은 Gini Criterion이 가장 작아지는 포인트를 찾아서, 데이터를 나누게 됩니다.</U>\n",
    "\n",
    "\n",
    "- 슬프게도 Decision Tree에서 최적의 Tree를 찾는게 NP-complete 문제라서, (현실적으로 불가능함)<br> heuristic한 방식(column 별로 target이 구분되는 구간을 모두 찾고 해당 구간별로 gini criterion을 모두 구해 최소가 되는 구간을 찾고 나누고 이것을 반복한다.)으로 찾습니다. <br>( Numeric value 의 경우 구간이 무한대이기 때문에 최적의 Tree를 찾는 것이 매우 힘들다.)\n",
    "\n",
    "\n",
    "\n",
    "- 이렇게 찾은 best split point를 기준으로 데이터를 양분한 뒤, left subtree와 right subtree에 대해서 반복적으로 같은 작업을 수행하면서 데이터를 나눠줍니다.\n",
    "\n",
    "\n",
    "- (특별한 제약조건이 없다면) 모든 데이터가 나누어져서 leaf node가 모두 순수한 상태가 되면 멈춥니다. (=gini criterion이 0이 되는 케이스) 해당 상태에는 label이 하나이다.\n",
    "\n",
    "\n",
    "- train data로 tree를 build 하는 과정이 decision tree의 학습 과정이다.\n",
    "\n",
    "#### inference \n",
    "\n",
    "- 만들어진 DT에 test data를 넣어서, 트리를 탐색(retirieve)하게 되면 최종 결과가 나오게 됩니다.\n",
    "\n",
    "- 이 결과를 예측값으로 사용해서 분류를 하게 됩니다.\n",
    "\n",
    "#### Pros and Cons\n",
    "\n",
    "- Pros:\n",
    "    - white box model(학습 결과가 클리어하게 해석 가능하다.)\n",
    "    - easy to train(학습이 쉽다.)\n",
    "    \n",
    "- Cons:\n",
    "    - easy to overfit(쉽게 training data를 외우게 됩니다.)\n",
    "    - training data가 조금만 바뀌어도 학습 모델이 전혀 다르게 만들어 집니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5de1f88",
   "metadata": {},
   "source": [
    "### Random Forest: <br> Decision Tree가 모여서 더 좋은 결과를 내는 모델\n",
    "\n",
    "- Random Forest는 CART 모델이 가지는 단점을 극복하기 위해서 제시된 모델입니다.\n",
    "\n",
    "\n",
    "- Random Forest는 DT 하나가 training data에 너무 쉽게 overfit되고 training data의 변화에 민감하다면 DT를 여러개 사용해서 다수결을 하는 방식으로 보완하자는 아이디어를 제시합니다.\n",
    "\n",
    "\n",
    "- 별거 아닌 것 같았는데, practical하게 굉장히 좋은 성능을 보여줍니다.\n",
    "\n",
    "\n",
    "- 이렇게 단일 모델을 여러게 모아서 더 좋은 판단을 하는 방법론은 **Model Ensemble** 이라고 합니다\n",
    "\n",
    "\n",
    "#### Not Decision Forest but **Random** Forest\n",
    "\n",
    "\n",
    "- Decision Tree를 그냥 모으기만 하면 더 좋은 결과를 낼 수가 없습니다.\n",
    "    - 같은 데이터에 대해서 만들어진 DT는 같은 결과를 출력합니다.\n",
    "    - 왜냐면, 다 같은 best split point가 매번 뽑히기 때문에 그렇습니다.\n",
    "    - 조금 더 다양성이 필요합니다!\n",
    "    \n",
    "    \n",
    "- 다양성 확보를 위한 전략으로 2가지를 채택했습니다.\n",
    "    1. Bagging(Bootstrap Aggregating)\n",
    "    \n",
    "    $\\rightarrow$ data sampling(모집단 자체를 바꾼다.)\n",
    "    2. Random Subspace method\n",
    "    \n",
    "    $\\rightarrow$ feature sampling(DT가 뽑은 feature를 바꾼다.)\n",
    "    \n",
    "- Bagging\n",
    "\n",
    "    - Bootstrapping: 복원추출로 샘플링을 한다. 확률분포를 유지시킨 상태로 데이터를 뽑는 방법\n",
    "        - 원본 데이터와 비슷한 분포를 가지는 샘플을 가질 수 있다. 비슷하면서 다채로운 데이터\n",
    "    \n",
    "    - Aggregating: classifier의 결과에 대한 다수결\n",
    "    \n",
    "    \n",
    "- Random Subspace Method\n",
    "\n",
    "    only some randomly selected subset of features are available from which to select the best split\n",
    "    \n",
    "    \n",
    "- data sampling + feature sampling을 통해서 만들어진 각 DT에 다양성을 제공합니다.\n",
    "\n",
    "- 각 DT를 학습할 때 마다, Bootstrapping과 Random Subspace method를 적용합니다.\n",
    "\n",
    "- 몇 개의 DT를 모을지는 hyper-parameter입니다.\n",
    "\n",
    "- 이렇게 만들어진 DT는 결론을 **다수결로 평가**(분류이기 때문)하는 것으로 \"집단 지성\"을 구현할 수 있습니다. \n",
    "\n",
    "\n",
    "- Random Forest는 그냥 DT들을 모으는게 아닌, randomness를 적당히 포함하는 것으로 DT의 약점을 잘 보완한 모델입니다.\n",
    "\n",
    "- 정형 데이터를 머신러닝으로 수행할 때 굉장히 좋은 baseline model이 됩니다.( 만만하게 잘 되는 모델)\n",
    "\n",
    "- Random Forest는 DT들의 모임이기 때문에 어느정도 explainabliity를 가지고 있습니다.\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
