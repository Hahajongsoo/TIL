{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d16bb824",
   "metadata": {},
   "source": [
    "## Regression\n",
    "- 주어진 데이터(X)와 찾고자 하는 값(y) 사이의 관계를 찾는 방법( 관계를 찾는 것 , 값을 정해주는 것)\n",
    "- 종속변수와 하나 이상의 독립변수 사이의 관계를 추정하는 것이다.\n",
    "\n",
    "- 회귀란 주어진 input data 와 관심있는 target value 사이의 관계를 모델링 하는 것을 말합니다.\n",
    "    - input data는 일반적으로 벡터이며\n",
    "    - target value는 일반적으로 실수값입니다.\n",
    "- feature vector로 target value를 예측하는 것을 목표로 합니다. (결과 값 그대로가 실제 값이다. target value 그대로를 예측하려고 한다. 그것을 예측할 수 있는 모델을 구하는 것이다.)\n",
    "\n",
    "- feature vector로 target value를 찾기 위해서는 관계식이 필요한데, 이 관계식을 모델링 하는 것이 회귀 분석의 목표입니다.\n",
    "\n",
    "\n",
    "- 데이터의 경향성에 맞게 파라미터를 조절한다.\n",
    "\n",
    "- 머신러닝 모델이 관계식을 찾게 되면, 해당 관계식에 test data를 inference한 결과가 예측값 $\\hat{y}$이 됩니다.\n",
    "- 분류와 다르게 inference한 결과값 자체가 예측값이 됩니다.\n",
    "- supervised learning이기 때문에, target value를 찾는 방향으로 학습이 진행됩니다.\n",
    "\n",
    "- regression models\n",
    "    - linear regression\n",
    "    - lasso\n",
    "    - ridge\n",
    "    - polynomial regression\n",
    "    - random forest\n",
    "    - XGBoost\n",
    "    - LightGBM\n",
    "    - neural network\n",
    "- 회귀에 깊게 파고 드는것 보다는 모델 위주로, 통계 모델에 가깝기 때문에 통계적 지식이 많이 필요하다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5b3c52",
   "metadata": {},
   "source": [
    "### Linear Regression: <br>$y = Wx+b$로 표시되는 선형식으로 $x$와 $y$사이의 관계를 찾는 모델.\n",
    "\n",
    "- 분류의 경우 y도 feature 다.  회귀의 경우는 그렇지 않다. \n",
    "\n",
    "- linear regression은 하나의 선형식으로 $X$와 $y$사이의 관계를 찾아내는 방법입니다.\n",
    "- 분류와 다르게, 회귀 모델은 선형식의 계산 <U>결과 자체가 예측 값</U>입니다.\n",
    "\n",
    "- 3차원을 예로 들면, 3차원 공간상의 점들이 target value가 된다.\n",
    "- 성별과 나이에 따라서 해당 마트의 구매빈도가 얼마나 연관이 있는가를 보고싶다.\n",
    "- 수식이 target value를 모두 맞출 수 있다면 best이지만 모든 것을 맞추는 것은 불가능하다. 선형 회귀모델이 모든 해를 구하는 것이 목적이 아닐 뿐 더러, 정답을 맞추려고 하는 것이 목적이 아닐 뿐 더러 데이터 자체에 error가 있고 데이터들이 선형적인 관계를 가지지 않는 경우가 훨씬 많을 것이다.\n",
    "\n",
    "- 선형적인 식이 데이터들의 어떤 경향을 나타내는지 찾고 싶은 것이다. 그렇다면 해당 경향을 최대한 잘 나타내는 것을 찾고 싶은 것이다.\n",
    "\n",
    "- 해당 식은 모든 target value들 과의 차이의 합이 최소가 되는 식을 찾고 싶은 것이다.\n",
    "\n",
    "**Go Detail**\n",
    "\n",
    "$y = Wx+b$의 식을 자세히 들여다보면, 다음과 같이 표시할 수 있습니다.\n",
    "\n",
    "$\\rightarrow y=w_1*x_1+w_2*x_2 + \\dots + w_n*x_n + b$\n",
    "\n",
    "분류와 접근 방식이 동일하기 때문에, 겹치는 설명은 스킵\n",
    "\n",
    "linear classifier처럼 처음에는 랜덤 값을 가지는 $w_i$들을 가지고 예측을 수행합니다.\n",
    "\n",
    "임의의 값 $\\hat{y}_i$가 나왔습니다. 이 예측값은 보통 실제 값과 동떨어져 있을 것입니다.\n",
    "\n",
    "예측 값이 실제 값과 가까워지려면 파라미터$(w, b)$들을 업데이트 해야합니다. \n",
    "\n",
    "이 때 gradient descent algorithm이 사용되어 $w, b$를 업데이트 해줍니다.\n",
    "\n",
    "업데이트가 되는 방향은 주어진 loss funtion의 최소가 되는 지점으로 향하는 방향입니다.\n",
    "\n",
    "MSE는 convex 하므로 linear regression 함수는 주어진 조건에서 무조건 optimal solution을 찾을 수 있다.(전제조건이 있긴함)\n",
    "\n",
    "물론 이게 최고의 solution이라는 얘기는 아니고 linear regression이 할 수 있는 best case는 항상 찾을 수 있다는 얘기\n",
    "\n",
    "- 회귀에서 가장 많이 사용하는 loss function은 MSE(Mean Squared Error)\n",
    "\n",
    "$$MSE = {1\\over{}N} \\sum_{i=1}^N (y_i^2 - \\hat{y}_i^2)^2$$\n",
    "\n",
    "- 즉, 모델의 예측값 $\\hat{y}_i$이 실제값 $y_i$에 점점 가까워지게 학습이 됩니다. -> 전체적으로 loss의 평균이 작아지는 방향으로 학습이 진행됩니다.\n",
    "\n",
    "- MSE를 사용하면 차이가 큰 데이터가 있는 경우 loss가 더 크게 나오기 때문에 이상치가 있다면 제거하거나 보정해주는것이 필요합니다. (이상치가 있으면 해당 이상치에 따라가게 된다. 즉 전체 데이터 경향성 예측에 너무 많은 영향을 주게 된다.)\n",
    "\n",
    "- 따라서, linear regression도 역시 파라미터$W$와 $b$를 찾는 문제가 되며, 적절한 파라미터를 찾았을 때 데이터를 잘 파악하는 선형식을 찾을 수 있게 됩니다.\n",
    "\n",
    "- linear regression은 두 가지 큰 장점이 있습니다.\n",
    "    1. 통계적으로 설명 가능한 이론이 많습니다. ( 설명 도구가 많다.)\n",
    "    2. interpretability가 있다.(설명 가능하다.) \n",
    "    -> 수식 자체가 선형식이기 때문에, 직접 계산을 해서 예측값이 왜 나오는지 설명이 가능하다.\n",
    "    3. linear model 자체가 가지는 simplicity 때문에, general한 모델이 나오는 편입니다.\n",
    "    -> 오히려 복잡한 모델들 보다 예측력이 더 뛰어납니다! 회귀의 경우 값을 맞추는 것이기 때문에 (할 수 없다. 어떤 데이터가 들어올지 모르기 때문에)정확하게 예측하는 것이 아니라 simple하게 general하게 그리게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6c794a",
   "metadata": {},
   "source": [
    "### Lasso, Ridge: <br> Linear Regression 모델이 고차원 공간에서 overfitting이 쉽게 되는 문제를 해결한 기법.\n",
    "\n",
    "- feature가 많아지는 경우, 회귀 모델이 특정 feature에 끌려가거나 모든 변수를 설명하지 못하는 경우가 발생한다. training data에는 어찌저찌 맞추더라도 general한 문제를 풀지 못하게 된다. 모든 feature가 동일한 가중치가지는 것을 해결\n",
    "\n",
    "\n",
    "**Simple Linear Regresion**\n",
    "$$\\sum_{i=1}^M (y_i^2 - \\hat{y}_i^2)^2 = \\sum_{i=1}^M \\left( \\hat{y}_i - \\sum_{j=0}^p w_j \\times x_ij \\right)^2 $$\n",
    "\n",
    "**Lasso**\n",
    "$$\\sum_{i=1}^M (y_i^2 - \\hat{y}_i^2)^2 = \\sum_{i=1}^M \\left( \\hat{y}_i - \\sum_{j=0}^p w_j \\times x_ij \\right)^2 + \\lambda \\sum_{j}^p \\left\\vert w_j \\right\\vert$$\n",
    "- weight의 L1 term 을 Loss function에 더해줍니다. ($\\lambda$는 hyper-parameter)\n",
    "- Loss 가 무조건 증가하게 됩니다.\n",
    "- 추가한 항(L1 term)도 gredient descent algorithm의 최적화 대상에 속합니다.\n",
    "- L1 term을 제약조건(constraint)이라고 부르고 또는 Rergularization term (학습을 규제, 방해) 이라고 합니다.\n",
    "    \n",
    "    -> L1 regularization\n",
    "\n",
    "- 해당 점에서 $\\beta_1$이 0가 된다. -> 자동으로 feature가 선택된다.\n",
    "\n",
    "**Ridge**\n",
    "$$\\sum_{i=1}^M (y_i^2 - \\hat{y}_i^2)^2 = \\sum_{i=1}^M \\left( \\hat{y}_i - \\sum_{j=0}^p w_j \\times x_ij \\right)^2 + \\lambda \\sum_{j}^p  w_j^2 $$\n",
    "- weight의 L2 term 을 Loss function에 더해줍니다. ($\\lambda$는 hyper-parameter)\n",
    "- Loss 가 무조건 증가하게 됩니다.\n",
    "- 추가한 항(L1 term)도 gredient descent algorithm의 최적화 대상에 속합니다.\n",
    "- L1 term을 제약조건(constraint)이라고 부르고 또는 Rergularization term (학습을 규제, 방해) 이라고 합니다.\n",
    "\n",
    "    -> L2 regularization\n",
    "\n",
    "- 해당 점에서 $\\beta_1$이 0에 가까운 값이 된다. -> 해당 feature의 영향을 줄인다.\n",
    "    \n",
    "- 통계에서 coefficient는 $\\beta$ 이고, 이 그림에서 $\\hat{\\beta}$는 optimal value이다.\n",
    "\n",
    "- $\\beta$들의 조합으로 같은 $\\hat{\\beta}$을 가지는 집합을 표시할 수 있게 되고 그 집합이 regulation term을 만나야한다.\n",
    "\n",
    "- 해당 점에서 $\\beta_1$이 0가 된다. -> 자동으로 feature가 선택된다.\n",
    "\n",
    "- Lasso나 Ridge를 적용했을 때, 성능이 향상된다면 Linear Regression 모델에 사용되는 feature vector가 차원을 줄일 필요가 있다는 얘기가 됩니다.\n",
    "\n",
    "    -> feature selection이 성능 향상을 가져온다.\n",
    "    \n",
    "- Regularization을 할 때, weight를 사용하는 방식을 weight decay라고 합니다.\n",
    "\n",
    "- weight decay를 주게 되면, gradient descent algorithm이 loss space를 탐색할 때, 제약조건을 받게 되는 효과가 있습니다.\n",
    "\n",
    "- 제약 조건 때문에, 특정 weight 들이 사라지는 효과가 생기면서 (0에 가까워짐) feature subset selection을 하는 효과가 있습니다.\n",
    "\n",
    "- 일반적으로는 feature engnieering을 잘 수행해주고 regression을 해주는 것이 더 좋긴 하다. lasso, ridge는 차원이 줄어들 여지가 있는 것을 확인하는 보조적인 기법으로 보는 것이 맞다. 통계에서는 상당히 많은 이유를 가지므로 설명을 해야할 때는 많이 사용된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d8759c",
   "metadata": {},
   "source": [
    "### XGBoost: <br> 하드웨어 최적화를 시킨 Gradient Boosting Model\n",
    "- random forest, CART의 진화형 같은 느낌\n",
    "\n",
    "**Boosting**\n",
    "\n",
    "- Boosting model은 Bagging 방식이 만들어지는 우너리가 전체 성능을 향상하는데 직접적인 연관이 없는 것을 보완한 모델입니다.\n",
    "- Sequential model입니다.\n",
    "- 첫번째로 만든 DT가 잘못 분류한 것들을 <U>그 다음 DT가 보완하는 방식으로 순차적으로 Tree를 build</U>합니다.\n",
    "- 다음 DT는 이전 DT가 잘못 분류한 데이터들에 weight를 주는 것으로 DT가 뽑을 데이터의 sampling을 조절합니다. (더 잘 뽑히거나 잘 안뽑히게)\n",
    "- 이후 aggregating\n",
    "\n",
    "- 이것도 결국 직접적으로 성능을 향상시키는 것은 보장이 되지 않는다. \n",
    "    \n",
    "    -> Gradient descent algorithm을 boosting model에 도입해서, 다음 DT가 이전 DT와 합쳐져서 <U>더 적은 loss를 가지게 되는 방향</U>으로 DT를 만드는 방법을 Gradient Boosting model이라고 합니다.\n",
    "    \n",
    "- sequential model이므로 속도가 느리다는 단점이 존재한다.\n",
    "\n",
    "**XGBoost**\n",
    "\n",
    "- Gradient Boosting model(GBM) + System Optimization\n",
    "- tree의 best split point를 찾을 때, feature를 정렬하는 게 가장 큰 cost를 소모한다는 점을 확인하였습니다.\n",
    "- 정렬하는 비용을 block 단위로 잘라서 update하는 방식을 제안하여 GBM과 거의 유사한 성능을 내느 방식을 제안합니다.\n",
    "\n",
    "- 훨씬 더 빠르게 정렬한 내용들을 사용할 수 있게 시스템 최적화를 합니다.\n",
    "    - Cache awareness\n",
    "    - I/O performance\n",
    "    - GPU accleration\n",
    "    - C++ 로 구현\n",
    "    \n",
    "**요약**\n",
    "- XGBoost는 시스템 최적화를 통해서 practical한 좋은 솔루션을 제안합니다.\n",
    "\n",
    "    -> 컴퓨터가 얼마나 학습을 잘하게하느냐에 대한 개선이 해당 알고리즘이 널리 퍼지게 되고 dominant하게 되며 주요한 알고리즘으로 자리잡게 될 수 있는지를 보여준 알고리즘이다.\n",
    "\n",
    "- GPU를 사용할 수 있게 되어서, computing resource를 이전보다 더 많이 사용해서 더 좋은 성능을 낼 수 있는 방법을 제안하게 되었습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44096b32",
   "metadata": {},
   "source": [
    "### LightGBM: <br> 기존 GBM들 보다 훨씬 더 빠르게 학습이 되는 모델\n",
    "\n",
    "- XGBoost의 장점은 모두 계승하면서 알고리즘적인 개선을 통해 더 빠른 학습을 하게 된다.\n",
    "\n",
    "\n",
    "- DT와 RF의 단점은 tree의 depth가 커지면 overfitting 할 가능성이 커진다\n",
    "    \n",
    "    -> depth를 최대한 줄이기 위해서 leaf가 한번에 뻗어나가는 방식으로 만든다. (balanced tree) 또는 depth를 제한해야 한다. 그러려면 level-wise tree growth여야 학습이 general하게 될 수 있다.\n",
    "    \n",
    "- 기존의 GBM들은 Level-wise 방식으로 tree 를 build 했습니다.\n",
    "\n",
    "- Level-wise라는 건 DT가 학습을 할 때, 같은 level에 있는 노드들을 모두 split한 뒤에 다음 level로 넘어가는 방식을 얘기합니다.(Breadth-First Search)\n",
    "\n",
    "- 깊이가 너무 커지면 overfitting될 가능성이 높기 때문에, Level을 제한하여 최대한 모델ㅇ르 키우는 방식을 사용해왔습니다. (Model Generalizaion)\n",
    "\n",
    "\n",
    "- LightGBM이 제안하는 메인 아이디어는 Level-wise 방식이나 Leaf-wise방식 모두 optimal을 만들게 된다면 비슷한 DT를 만들게 된다는 것에서 시작합니다.\n",
    "\n",
    "- Leaf-wise를 사용한다면, 훨씬 더 빠르게 optimal을 찾을 수 있다는 것에 포인트입니다.\n",
    "\n",
    "- 전체 Loss가 줄어드는 방향으로 node를 선정해서 split합니다. 이 때, level을 유지하려는 경향을 포기합니다.\n",
    "\n",
    "- 필요한 노드들만 split하면 되기 때문에, 기존 GBM들과 비교했을 떄 훨씬 빠르게 학습이 가능하다는 장점이 있습니다.\n",
    "\n",
    "- 단 적은 데이터를 사용하게 되면 overfitting이 될 가능성이 높아집니다.(10,000row 이상일 떄만 사용 권장)\n",
    "\n",
    "- 다른 GBM들에 비해 hyper-parameter sensitive합니다. (특히 max_depth에 가장 민감함, 이외에도 hyper-parameter들이 많다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16f0558",
   "metadata": {},
   "source": [
    "- 분류의 경우는 대표적인 분류 모델들이 많기에 아래 두 모델을 설명하지 않았다. \n",
    "- 회귀의 경우는 보통 위의 경우만 많이 사용한다. 회귀모델 자체가 예측값을 모델링해야해서 generalize되고 simple하게 사용할 수 있는 모델이 많이 없다. LightGBM이 성능이 좋기 때문에 practical하게 많이 사용한다."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
