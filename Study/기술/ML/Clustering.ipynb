{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e824030",
   "metadata": {},
   "source": [
    "## Clustering: <br> 주어진 데이터 (X)사이의 유사한 데이터들을 묶어주는 방법\n",
    "- label이 없다.\n",
    "\n",
    "- 클러스터링이랑, 비슷한 데이터들 끼리 같은 그룹으로 그렇지 않은 데이터들과든 다른 그룹으로 묶어주는 방법입니다.\n",
    "\n",
    "\n",
    "- 클러스터링은 대표적인 unsupervised learning 방식입니다.\n",
    "\n",
    "\n",
    "- input data는 일반적으로 벡터이며,(feature vector)<br> target value는 없습니다.\n",
    "\n",
    "\n",
    "- target value가 없다보니, feature vector가 굉장히 중요합니다.\n",
    "    \n",
    "    -> feature engeenring의 영향을 많이 받습니다.\n",
    "    \n",
    "    \n",
    "- 클러스터링에서는 주어진 데이터를 가지고 판단하는 수 밖에 없기 때문에, 2가지 요소가 정말 중요합니다.\n",
    "    1. feature vector\n",
    "    2. similarity\n",
    "  \n",
    "  \n",
    "- 모델에 따라 결과가 천차만별로 바뀝니다.\n",
    "    - K-means\n",
    "    - Hierarchical Agglomerative Clustering\n",
    "    - DBSCAN\n",
    "    - HDBSCAN\n",
    "    - Spectral Clustering\n",
    "    - Mean Shift\n",
    "    - BIRCh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe46545",
   "metadata": {},
   "source": [
    "### K-means:<br> 주어진 데이터에서 K개의 중심점을 찾아서 데이터를 묶어주는 방법\n",
    "\n",
    "- K-means는 정말 대표적인 클러스터링 모델입니다.\n",
    "- 1960년대에 처음 제안되어 지금까지도 제일 많이 사용되는 모델입니다.\n",
    "- 모델이 직관적이고 간단하며 빠릅니다.\n",
    "\n",
    "**Algorithm**\n",
    "1. 주어진 데이터에서 랜덤으로 K개의 데이터를 첫 기준점(centroid)으로 잡습니다. 각 centroid에 0번부터 차례대로 번호(cluster label)를 부여합니다.\n",
    "2. 각 데이터는 K개의 centroid중에 가장 가까운 centroid를 찾아서 같은 번호를 부여받습니다.\n",
    "3. 같은 번호인 데이터들끼리 평균(mean)을 계산합니다.\n",
    "4. 각 평균을 새로운 centroid로 지정합니다.\n",
    "5. 2번부터 다시 반복합니다.\n",
    "6. 더 이상 cluster label이 바뀌는 데이터가 없다면 알고리즘이 종료됩니다.\n",
    "\n",
    "**Pros and Cons**\n",
    "\n",
    "- Pros\n",
    "    - 아주 빠릅니다. $O(nkd)$\n",
    "    - 모델의 수행원리가 간단해서 해석이 용이합니다. \n",
    "        \n",
    "        -> unsupervised learning 들은 <U>해석이 굉장히 중요</U>합니다! (label이 없기 때문에 맞았는지 틀렸는지 이야기하기 힘들다.)\n",
    "\n",
    "    - objective function이 convex라서 무조건 수렴합니다. (아름다운 결과는 아닐 수 있음)\n",
    "\n",
    "\n",
    "- Cons\n",
    "    - mean을 기준으로 하기 때문에 outlier에 굉장히 취약합니다.\n",
    "    - 데이터의 모양이 hyper-spherical이 아니라면 잘 묶이지 않습니다.\n",
    "    - initial centroid를 어떻게 고르느냐에 따라서 성능이 천차만별로 바뀝니다.\n",
    "    \n",
    "        -> K-means++(2007)가 이 문제를 어느정도 개선했습니다. (다음 centroid는 현재부터 멀리 있는 데이터를 뽑을 확률이 높게 끔 설정)\n",
    "    \n",
    "    - K가 hyper-parameter입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91172925",
   "metadata": {},
   "source": [
    "### Hierarchical Agglomerative Clustering:<br> 데이터를 유사한 순서대로 묶어서 계층 구조를 만드는 방식으로 데이터를 묶어주는 방법.\n",
    "\n",
    "- HAC는 상향식 계층 클러스터링이라고 부릅니다. 상향식이라는 이름이 붙은건 위의 그림에서 아래서부터 위로 점차 데이터를 묶어가기 때문입니다. \n",
    "- 위와 같은 그림을 Dendrogram이라고 부릅니다.\n",
    "- dendrogram에서 x축은 데이터 하나하나를 의미하고, y축은 유사도(대부분 euclidean distance)를 나타냅니다.\n",
    "- 모든 데이터간 유사도를 계산해야하기 때문에, 많이 느립니다.\n",
    "\n",
    "\n",
    "**Algorithm**\n",
    "1. 모든 데이터를 각자 독립적인 클러스터로 세팅합니다. (서로 다른 N개의 cluster label을 부여받음)\n",
    "2. 유사도(silimarity)와 묶는 방식(linkage)을 정합니다. (유사도는 euclidean distance로, 묶는 방식은 single로 가정)\n",
    "3. 가장 유사도가 높은 2개의 클러스터를 고릅니다.\n",
    "4. 정해진 방식으로 묶습니다. (single의 경우 가장 가까운 데이터의 pair가 포함된 두 개의 클러스터를 합칩니다.)\n",
    "5. 모든 데이터가 묶여서 하나의 클러스터가 될 때 까지, 3, 4번을 반복합니다. 모든 데이터가 묶이면 dendrogram을 그릴 수 있습니다.\n",
    "6. dendrogram에서 특정 threshold(distance)를 기준으로 세로로 잘랐을 때, 나뉘는 클러스터들을 최종 클러스터로 선정합니다.\n",
    "\n",
    "**Linkage criteria**\n",
    "1. single: 가장 가까운 데이터 Pair가 포함된 두 개의 클러스터를 합칩니다.\n",
    "2. Average : 클러스터 간의 평균 거리가 가장 가까운 두 개의 클러스터를 합칩니다.\n",
    "3. Complete : 임의의 두개의 클러스터 중에 가장 멀리있는 데이터간의 거리가 가장 가까운 두 개의 클러스터를 합칩니다. (minimax)\n",
    "4. Ward : 모든 클러스터들의 within cluster variance가 최소가 되는 클러스터들을 합칩니다.\n",
    "    \n",
    "    (within cluster vairance란, 클러스터 냅의 데이터간의 sum of square distance를 의미합니다.\n",
    "    \n",
    "    사실 ward criterion을 사용하면, K-means와 유사한 방식으로 묶어주게 됩니다.\n",
    "    \n",
    "    \n",
    "**Pros and Cons**\n",
    "\n",
    "- Pros\n",
    "    - 원하는 similarity와 linakge를 사용할 수 있어, 다양한 공간에서 다양한 형태의 클러스터를 찾을 수 있습니다. (K-means는 유사도로 유클리드 거리밖에 쓰지 못한다.)\n",
    "    - dendrogram을 이용하여 데이터에 따라 유연하게 최적의 클러스터 개수를 정할 수 있습니다. (일정한 기준 but 다른 데이터)\n",
    "    - 어떤 linkage 방법을 사용하더라도, 한번에 하나씩 클러스터가 줄어들기 때문에 원하는 클러스터 개수를 찾을 수도 있습니다.\n",
    "- Cons\n",
    "    - K-means에 비하면 매우 느립니다. 대용량 데이터에는 적합하지 않습니다. $O(N^3)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63acc599",
   "metadata": {},
   "source": [
    "### DBSCAN(Density-Based Spatial Clustering of Applications with Noise): <br> 정의한 밀도에 따라 인접한 데이터를 계속해서 묶어나가는 방법.\n",
    "\n",
    "- DBSCAN은 밀도라는 개념을 도입하여, 서로 가까이 있는 데이터들을 하나의 클러스터로 묶어줍니다.\n",
    "- DBSCAN은 이전 기법들과는 다르게 noise data를 outlier로 취급하여 분류합니다. (outlier detection)\n",
    "- 이러한 특징으로 Outlier를 찾는 문제에도 활용됩니다.\n",
    "    \n",
    "    (e.g. 불량품 검출, 사기 거래 감지, ... )\n",
    "    \n",
    "**Algorithm**\n",
    "1. 밀도를 정의하기 위한 파라미터 MinPts와 Eps를 정의합니다.\n",
    "    - Eps는 같은 묶음으로 판단하는 기준이 되는 거리값입니다. (euclidean distance 기준)\n",
    "    - MinPts는 같은 묶음으로 판단하기 위해서 Eps를 반지름으로 하는 원을 그렸을 때, 최소한으로 포함되어야 하는 데이터의 개수입니다. (range query 범위를 만족하는 데이터를 찾는 query)\n",
    "2. 각 데이터를 기준으로 Eps 크기를 가지는 원을 그려서 그에 해당하는 데이터를 찾습니다.(range query)\n",
    "    \n",
    "    Range query의 결과로 MinPts 이상의 데이터가 포함된다면, 그 데이터를 Core point라고 지정합니다.\n",
    "    \n",
    "3. Core Point와 연결된 모든 Core Point들은 하나의 클러스터로 묶입니다.\n",
    "4. 만약, 어떤 포인트가 range query를 했을 때 Minpts를 만족하지 못하지만, Range query의 결과에 Core Point가 포함되어 있는 경우에 해당 포인트는 Border Point가 됩니다. Border Point까지는 하나의 클러스터로 묶입니다.\n",
    "5. Core Point, Border Point를 모두 만족하지 못하는 데이터는 Noise Point(Outlier)로 판단이 되며, 이 때 -1의 cluster lael을 부여받습니다.\n",
    "\n",
    "**Pros and Cons**\n",
    "\n",
    "- Pros\n",
    "    - 다양한 형태의 데이터에 대해서 클러스터를 굉장히 잘 파악합니다. (=성능이 좋다.)\n",
    "    - 어떻게든 다른 클러스터에 모든 데이터를 포함시키는 다른 방법들과는 달리, outlier를 정의하고 있기 때문에 만들어진 클러스터의 품질이 좋습니다. ( 특징이 뚜렷하다, 밀도가 높다, 해석력이 뛰어나다.)\n",
    "- Cons\n",
    "    - MinPts와 Eps가 hyper-parameter입니다.\n",
    "    - 모든 포인트에 Range Query를 계산해야 해서 꽤 느린 편입니다. $O(N^2)$\n",
    "        \n",
    "        계속 똑같은 데이터에 적용하는 경우 index를 bulid 해놓으면 nlog(n)까지 떨어진다. e.g. 지도\n",
    "        \n",
    "    - 고차원 공간에서 성능이 떨어지는 단점이 있습니다. (Curse of dimensinality)\n",
    "    \n",
    "        유클리드 거리를 계산하는 것 때문에 그렇다. K-means는 성능이 차원에 엄청 영향을 받지 않는다. range query는 차원의 영향을 받는다. index가 build 되어 있어도 필터링을 하지 못한다.\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c512b7f7",
   "metadata": {},
   "source": [
    "### Spectral Clustering: <br> KNN Graph를 생성하여 데이터의 특징을 잘 파악한뒤 성능이 좋은 클러스터를 생성해주는 방법\n",
    "\n",
    "**Graph**\n",
    "\n",
    "- 데이터 간의 관계를 모델링할 떄 사용되는 자료구조를 Graph라고 합니다.\n",
    "- Node와 Edge로 구성되어 있습니다.\n",
    "- Social Network나 Molecular Structure가 대표적인 Graph를 활용해서 해석할 수 있는 도메인입니다.\n",
    "\n",
    "**Spectral Clustering**\n",
    "\n",
    "- 대표적인 graph clustering 기법입니다.( 묶이는 대상이 subgraph)\n",
    "- 주어진 데이터를 그래프 모델로 해석하여, subgraph로 데이터를 나누는 것으로 묶어주는 방식입니다.\n",
    "- 이렇게 데이터를 나누는 과정이 Graph Laplacian Matrix의 Eigenvector를 찾는 것과 같은 과정이 되기 때문에, spectral이라는 말이 붙었습니다.\n",
    "- 각 데이터를 노드로 두고, 각 데이터로부터 $\\epsilon$보다 가까운 거리에 있는 노드들을 연결한 KNN Graph(K-Nearest Neighbor)를 만듭니다. (k는 여기서 거리를 기준으로)\n",
    "\n",
    "\n",
    "- 만든 그래프가 가장 잘 나눠지는 곳을 찾아서 그래프를 2분할 합니다.(find min-cut) (그래프에서 가장 연결이 약한 곳을 찾는다)\n",
    "- 원하는 개수의 클러스터(subgraph)가 생길 때 까지 그래프를 분할합니다. \n",
    "    \n",
    "    (만약,  찾고자 하는 클러스터가 5개라면 분할을 4번 진행하면 됩니다.)\n",
    "    \n",
    "**Pros and Cons**\n",
    "- Pros\n",
    "    - 기존 공간에 구애받지 않고 데이터를 그래프 구조로 파악하는 것으로 성능이 꽤 좋게 나오는 편입니다.\n",
    "- Cons\n",
    "    - KNN graph를 만들고 min-cut을 찾는게 시간이 오래걸립니다. $O(N^3)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63368eb",
   "metadata": {},
   "source": [
    " 비지도 학습은 답이 없기 때문에 잘 나왔다는 것을 판단하기 어렵다. 정량평가가 어렵다\n",
    " \n",
    "**<U>보통은 해석의 영역으로 넘어간다.</U>**\n",
    " \n",
    " -> clear하게 만드는데 집중한다. 앞서 말한 두 가지 요소를 강력하게 만든다.\n",
    "     \n",
    "   1. feature extraction (embedding)\n",
    "   2. 특정 데이터에 따라 silimarity가 있다. (1을 하면 2는 해결된다.)\n",
    "   \n",
    "**임베딩을 잘 하자는 것이 현재 경향**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2079fa13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
