 # Chapter 1
## Introduction to Airflow
- 데이터 엔지니어링이란?
	- 데이터와 관련된 모든 행동과 데이터를 신뢰할 수 있고, 반복 가능하며 유지관리할 수 있는 프로세스로 만드는 것을 의미한다.
- 워크플로란?
	- 데이터 엔지니어링 작업을 수행하기 위한 일련의 단계이다. 예를 들면 데이터를 다운로드하고 데이터를 복사하고 정보를 필터링하고 데이터베이스에 적재하는 것 등을 의미한다.
	- 복잡성의 단계가 다양하다. 수 단계에서 수 백 단계의 워크플로까지 다양하고 이러한 복잡도는 사용자에따라 다르다
	- 일반적인 데이터 엔지니어링에 대한 워크플로의 의미이다.
- Airflow란?
	- airflow는 일반적인 워크플로우를 프로그래밍하는 플랫폼이다.
		- 워크플로우의 생성, 스케쥴링, 모니터링을 포함한다.
	- 다양한 언어로 구현할 수 있으나 워크플로우는 파이썬으로 작성되었다.
	- 워크플로우는 DAG(Directed Acyclic Graphs)로 작성한다.
	- 코드, CLI, 웹 UI 등으로 접근할 수 있다.
- DAG의 간략한 설명
	- 비순환 방향성 그래프
	- airflow에서는 워크플로우를 구성하는 일련의 작업을 나타낸다. 
	- 작업과 작업들간의 종속성으로 구성된다.
	- DAG에 대한 다양한 세부 정보로 생성되며 이는 이름, 생성일자, 소유자, 이메일 알람 옵션 등을 포함한다.

### Running a workflow in Airflow
Airflow task를 실행하는 가장 쉬운 방법은 shell command를 이용하는 것이다.
```shell
airflow run <dag-id> <task-id> <start-date>
```

## Airflow DAGs
### DAG란?
- 방향성
	- 방향성이 있다는 것은 구성 요소간의 종속성을 나타내는 고유한 흐름이 있다는 것을 의미한다. 이러한 종속성은 구성 요소의 순서를 정하는 문맥을 제공한다.
-  비순환
	- 루프, 순환, 반복을 하지 않는다. 이는 워크플로우를 다시 실행할 수 없다는 의미가 아니라 구성 요소가 실행 당 한 번만 실행된다는 것을 의미한다.
- 그래프
	- 그래프는 구성요소와 구성요소간의 관계 또는 종속성을 나타낸다.
### Airflow 에서 DAG
- Python 으로 작성 되지만 다른 언어로 작성된 컴포넌트도 사용할 수 있다. 즉 DAG를 Python을 사용해 정의하지만 배쉬스크립트나 다른 실행 가능한 것들 예를들어 Spark job 등을 포함할 수 있다.
- DAG는 실행 가능한 컴포넌트로 구성된다. 이러한 컴포넌트들은 operator, sensor 등이 있다. airflow에서는 이것을 task 라고 한다.
- 명시적으로나 암시적으로 정의된 종속성을 포함하고 있다. 이러한 종속성은 실행 순서를 정의하기 때문에 airflow는 워크플로우에서 어떤 지점에 어떤 컴포넌트가 실행되어야하는지 알고 있다. 
	- 예를 들어, 파일을 데이터베이스로 가져오기 전에 서버에 파일을 복사할 수 있다.
### Define a DAG
```python
from airflow.models import DAG

from datetime import datetime
default_arguments = {
	 'owner': 'hajong',
	 'email': 'gkwhdtn95051@gmail.com',
	 'start_date': datetime(2022, 11, 8)
}

etl_dag = DAG('etl_workflow', default_args=default_arguments)
```
- 기본 인수 dict를 만든다. 이 dict는 DAG의 컴포넌트에 적용될 attribute로 구성되어있다.
	- attibute는 필수는 아니지만 Airflow 런타임 동작을 정의하는데 많은 권한을 제공한다.

### DAGs on the command line
- `airflow` cli 프로그램은 많은 subcomman를 포함하고 있다.
- 많은 명령어들이 DAG와 관련되어 있다.
- `airflow list_dags`로 DAG들을 확인할 수 있다.

### Command line vs Python
- cli tool
	- airflow를 시작
	- 수동으로 dag와 task를 실행
	- airflow에서 로그를 수집

- Python
	- DAG 생성
	- DAG의 특성 수정

## Airflow web interface
### DAGs
![](images/Pasted%20image%2020221108100358.png)
- 대부분의 시간을 보낼 페이지이다. 사용 가능한 DAG와 워크플로우의 수의 상태를 빠르게 확인할 수 있다.
- cron 포맷으로 스케쥴을 보여준다.
- 소유자 확인할 수 있다.
- 최근 실행된 task들, 최근 실행 그리고 DAG 실행을 확인할 수 있다.
- DAG 이름을 클릭하면 DAG 상세 페이지로 이동한다.

### DAG detail view
![](images/Pasted%20image%2020221108100829.png)
- DAG 자체의 정보에 대한 접근을 할 수 있다.
- 그래프, 트리, 코드등 다양한 관점에서 코드에서의 task와 종속성을 나타내는 정보를 확인할 수 있다.
- task 기간, task 시도 횟수, 타이밍, Gantt 차트 보기 등 DAG에 대한 특정 세부 정보에 접근할 수 있다.
- DAG를 트리거하고, 새로고침하고 삭제할 수 있다.

# Chapter 2
## Airflow operators
- Airflow operator는 workflow의 단일 task를 의미한다. 이는 명령줄 실행, 이메일 전송, Python 스크립트 실행 등 어떤 유형의 작업이든 수행할 수 있다.
- 독립적으로 실행된다. 즉, 작업을 완료하기 위한 모든 리소스들이 opterator 안에 포함되어 있다. 
- 일반적으로 opterator는 정보를 공유하지 않는다. 이는 workflow를 단순하게하고 airflow가 task를 좀 더 효율적인 방법으로 실행하게한다. 물론 operator 간 정보를 공유하는 방법은 존재한다.
- airflow는 다양한 task를 수행하기 위한 다양한 operator를 가지고 있다.

### BashOperator
- BashOperator는 주어진 배쉬 명령어나 배쉬스크립트를 실행한다. 해당 명령은 workflow에서 의미가 있는 Bash가 할 수 있는 거의 모든 것이다.
- `task_id`, `bash_command`, `dag`  의 세개의 인수가 필요하다. 
- BashOperator는 나중에 자동으로 정리되는 임시 디렉토리에서 명령을 실행한다.
- 명령에 대한 환경 변수를 지정할 수 있다.

#### BashOperator examples
```python
from airflow.operators.bash_operator import BashOperator


 example_task = BashOperator(task_id='bash_ex'
							 bash_command='echo 1'
							 dag=dag  )

bash_task = BashOperator(task_id='clean_address'
					bash_command='cat addresses.txt | awk "NF==10" > cleand.txt'
					dag=dag)
```

### Operator gotchas
- operator를 사용할 때 몇 가지 일반적인 문제가 있다.
- 개별 operator가 동일한 위치 혹은 환경에서 실행된다는 보장이 없다. 
	- 한 operator가 특정 디렉토리에서 어떠한 설정을 가지고 실행되었다고 해서 다음 operator가 같은 정보게 접근할 수 있는 것은 아니다.
- 특히 BashOperator에 대해서는 환경변수를 설정할 필요가 있다.
	- bash에서는 홈 디렉토리를 나타내기 위해서 `~` 을 사용하는 것이 일반적이다. 하지만 airflow에는 이것이 정의되어 있지 않다. 또한 스크립트 실행을 위한 환경변수(HOME 등), AWS credential, DB 접근 등을 위한 환경변수도 설정해줘야한다.
- 상승된 권한으로 task를 실행하는 것이 까다롭다. 리소스에 대한 접근은 작업을 실행하는 특정 사용자에 대해서 설정되어야 한다.

## Airflow tasks
### Tasks
- operator의 인스턴스이다. 워크플로우의 operator를 가리킨다.
- 보통 Python 코드에서 변수로 할당된다.
- Airflow 툴에서는 해당 변수가 아니라 task_id로 접근해야 한다.

### Task depedencies
- task 완료의 순서를 정의한다.
- 필수는 아니지만 보통 task dependency가 있다. 정의되지 않은 경우 작업 실행은 순서 보장 없이 Airflow에서 자체 처리된다.
- task depedency를 upstream task 또는 downstream task라고 한다. upstream task는 다른 downstream task보다 먼저 완료되어야 함을 의미한다.
- Airflow 1.8부터 task dependency는 비트시프트 연산자를 사용하여 정의된다.
	- `>>` : upstream operator
	- `<<` : downstream operator
- 간단하게 upstream은 **이전**을 의미하고 downstream은 **이후**를 의미한다. 

### Simple task dependency
```python
task1 = BashOperator(task_id='first_task'
					 bash_command='echo 1'
					 dag=example_dag)

task2 = BashOperator(task_id='second_task'
					 bash_command='echo 2'
					 dah=example)

task1 >> task2 # or task2 << task1
```

### Task dependencies in the Airflow UI
- 비트시프트 연산자를 사용하지 않은 경우 : task간 종속성이 없음을 확인할 수 있다.
![](images/Pasted%20image%2020221109103933.png)
- 비트시프트 연산자를 사용한 경우 : task간 종속성이 있음을 확인할 수 있다.
![](images/Pasted%20image%2020221109104109.png)

### Multiple dependencies
- 종속성은 필요에 따라 워크플로우를 정의하는데 필요한 만큼 복잡할 수 있다.
- chained dependencies
```python
task1 >> task2 >> task3 >> task4
```

- Mixed dependencies
	- task1 과 task3이 수행되어야 task2가 수행되는 DAG가 생성된다.
	- task1 과 task3은 어떤 것이든 먼저 실행될 수 있다.
```python
task1 >> task2 << task3
```
or
```python
task1 >> task2
task3 >> task2
```

## Additional operators
### PythonOperator
- Python 함수나 메소드등 호출가능한 것을 실행한다.
- bahsoperator 와 비슷하지만 다른 옵션들이 있다. `python_collable` 에는 실행할 것의 이름이 들어간다.
- 함수에 전달할 인수를 지정할 수 있다.
```python
from airflow.operators.python_operator import PythonOperator
def printme():
	print("this goes in the logs!")
python_task = PythonOperator(
		task_id = 'simple_print',
		python_callable=printme,
		dag=example_dag
)
```

#### Arguments
- task에 인수를 전달하는 것을 지원한다. Positional, keyword 둘 다 지원한다.
- `op_kwargs` dict를 이용할 것임
```python
def sleep(length_of_time):
	time.sleep(length_of_time):

python_task = PythonOperator(
		task_id = 'sleep',
		python_callable=sleep,
		op_kwargs={'length_of_time': 5}
		dag=example_dag
)
```

### EmailOperator
- `airflow.operator` 안에 있다.
- 이메일을 전송한다.
- 일반적인 구성요소가 들어갈 수있다.
	- HTML 콘텐츠
	- 첨부파일
 - 이메일을 성공적으로 전송하기 위해서는 이메일 서버 세부정보로 Airflow 시스템을 구성해야한다.
```python
from airflow.operator.email_operator import EmailOperator

email_task = EmailOperator(
		task_id='email_sales_report',
		to='sales_manager@exeample.com',
		subject='Automated Sales Report',
		html_content='Attached is the latest sales report',
		files='leatest_sales.txt',
		dag=example_dag
)
```


## Airflow scheduling
### DAG Runs
- 어떤 시점에서의 워크플로우의 특정 인스턴스이다.
- DAG는 수동으로 실행될 수 있고 `scheule_interval` 을 통해 실행될 수 있다.
- 각 DAG 실행은 자체 및 내부 작업에 대한 상태를 유지한다.
	- `running`
	- `failed`
	- `success`
 ![](images/Pasted%20image%2020221109111731.png)
### Schedule details
DAG를 예약할 때 요구사항에 따라 고려해야할 여러 attribute가 있다.
- `start_date` : DAG를 처음으로 예약할 수 있는 시간 지정 일반적으로 `datetime` 객체로 정의된다.
- `end_date` : DAG를 예약할 수 있는 마지막 시간  
- `max_tries` : DAG 실행이 완전히 실패하기 전에 재시도할 횟수
- `schedule_interval` : DAG가 실행되는 빈도

### Schedule interval
- DAG를 예약하는 빈도를 의미한다.
- 스케쥴링은 `start_date`  와 `end_date` 사이에서 일어난다.  DAG가 완전히 실행된다는 것 보다는, 해당 시간 사이에 스케쥴링 될 수 있다는 것을 의미한다.
- `cron` 포맷으로 정의되거나 빌트인 설정으로 정의된다.

### Airflow schduler presets
Airflow에는 프리셋 혹은 자주 사용되는 시간 간격을 나타내는 바로가기 옵션이 있다.
 | preset  | cron equivalent |
 | ------- | --------------- |
 | @hourly | `0 * * * *`     |
 | @dauly  | `0 0 * * *`     |
 | @weekly | `0 0 * * 0`                |
#### special presets
- `None` : 스케쥴링 하지 않음을 의미하며 수동으로 트리거되는 DAG에 사용된다.
- `@once` : 한 번만 예약한다.

### schedule_interval issues
DAG 스케쥴링에는 고려해야할 몇 가지 뉘앙스가 있다.
- `start_date`를 가능한 가장 빠른 값으로 사용하려고 한다.
- 하지만 스케쥴링은 `start_date` + `schedule_interval` 에 수행된다. 이를 주의해야한다.