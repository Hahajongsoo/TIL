 # Chapter 1
## Introduction to Airflow
- 데이터 엔지니어링이란?
	- 데이터와 관련된 모든 행동과 데이터를 신뢰할 수 있고, 반복 가능하며 유지관리할 수 있는 프로세스로 만드는 것을 의미한다.
- 워크플로란?
	- 데이터 엔지니어링 작업을 수행하기 위한 일련의 단계이다. 예를 들면 데이터를 다운로드하고 데이터를 복사하고 정보를 필터링하고 데이터베이스에 적재하는 것 등을 의미한다.
	- 복잡성의 단계가 다양하다. 수 단계에서 수 백 단계의 워크플로까지 다양하고 이러한 복잡도는 사용자에따라 다르다
	- 일반적인 데이터 엔지니어링에 대한 워크플로의 의미이다.
- Airflow란?
	- airflow는 일반적인 워크플로우를 프로그래밍하는 플랫폼이다.
		- 워크플로우의 생성, 스케쥴링, 모니터링을 포함한다.
	- 다양한 언어로 구현할 수 있으나 워크플로우는 파이썬으로 작성되었다.
	- 워크플로우는 DAG(Directed Acyclic Graphs)로 작성한다.
	- 코드, CLI, 웹 UI 등으로 접근할 수 있다.
- DAG의 간략한 설명
	- 비순환 방향성 그래프
	- airflow에서는 워크플로우를 구성하는 일련의 작업을 나타낸다. 
	- 작업과 작업들간의 종속성으로 구성된다.
	- DAG에 대한 다양한 세부 정보로 생성되며 이는 이름, 생성일자, 소유자, 이메일 알람 옵션 등을 포함한다.

### Running a workflow in Airflow
Airflow task를 실행하는 가장 쉬운 방법은 shell command를 이용하는 것이다.
```shell
airflow run <dag-id> <task-id> <start-date>
```

## Airflow DAGs
### DAG란?
- 방향성
	- 방향성이 있다는 것은 구성 요소간의 종속성을 나타내는 고유한 흐름이 있다는 것을 의미한다. 이러한 종속성은 구성 요소의 순서를 정하는 문맥을 제공한다.
-  비순환
	- 루프, 순환, 반복을 하지 않는다. 이는 워크플로우를 다시 실행할 수 없다는 의미가 아니라 구성 요소가 실행 당 한 번만 실행된다는 것을 의미한다.
- 그래프
	- 그래프는 구성요소와 구성요소간의 관계 또는 종속성을 나타낸다.
### Airflow 에서 DAG
- Python 으로 작성 되지만 다른 언어로 작성된 컴포넌트도 사용할 수 있다. 즉 DAG를 Python을 사용해 정의하지만 배쉬스크립트나 다른 실행 가능한 것들 예를들어 Spark job 등을 포함할 수 있다.
- DAG는 실행 가능한 컴포넌트로 구성된다. 이러한 컴포넌트들은 operator, sensor 등이 있다. airflow에서는 이것을 task 라고 한다.
- 명시적으로나 암시적으로 정의된 종속성을 포함하고 있다. 이러한 종속성은 실행 순서를 정의하기 때문에 airflow는 워크플로우에서 어떤 지점에 어떤 컴포넌트가 실행되어야하는지 알고 있다. 
	- 예를 들어, 파일을 데이터베이스로 가져오기 전에 서버에 파일을 복사할 수 있다.
### Define a DAG
```python
from airflow.models import DAG

from datetime import datetime
default_arguments = {
	 'owner': 'hajong',
	 'email': 'gkwhdtn95051@gmail.com',
	 'start_date': datetime(2022, 11, 8)
}

etl_dag = DAG('etl_workflow', default_args=default_arguments)
```
- 기본 인수 dict를 만든다. 이 dict는 DAG의 컴포넌트에 적용될 attribute로 구성되어있다.
	- attibute는 필수는 아니지만 Airflow 런타임 동작을 정의하는데 많은 권한을 제공한다.

### DAGs on the command line
- `airflow` cli 프로그램은 많은 subcomman를 포함하고 있다.
- 많은 명령어들이 DAG와 관련되어 있다.
- `airflow list_dags`로 DAG들을 확인할 수 있다.

### Command line vs Python
- cli tool
	- airflow를 시작
	- 수동으로 dag와 task를 실행
	- airflow에서 로그를 수집

- Python
	- DAG 생성
	- DAG의 특성 수정

## Airflow web interface
### DAGs
![](images/Pasted%20image%2020221108100358.png)
- 대부분의 시간을 보낼 페이지이다. 사용 가능한 DAG와 워크플로우의 수의 상태를 빠르게 확인할 수 있다.
- cron 포맷으로 스케쥴을 보여준다.
- 소유자 확인할 수 있다.
- 최근 실행된 task들, 최근 실행 그리고 DAG 실행을 확인할 수 있다.
- DAG 이름을 클릭하면 DAG 상세 페이지로 이동한다.

### DAG detail view
![](images/Pasted%20image%2020221108100829.png)
- DAG 자체의 정보에 대한 접근을 할 수 있다.
- 그래프, 트리, 코드등 다양한 관점에서 코드에서의 task와 종속성을 나타내는 정보를 확인할 수 있다.
- task 기간, task 시도 횟수, 타이밍, Gantt 차트 보기 등 DAG에 대한 특정 세부 정보에 접근할 수 있다.
- DAG를 트리거하고, 새로고침하고 삭제할 수 있다.

# Chapter 2
## Airflow operators
- Airflow operator는 workflow의 단일 task를 의미한다. 이는 명령줄 실행, 이메일 전송, Python 스크립트 실행 등 어떤 유형의 작업이든 수행할 수 있다.
- 독립적으로 실행된다. 즉, 작업을 완료하기 위한 모든 리소스들이 opterator 안에 포함되어 있다. 
- 일반적으로 opterator는 정보를 공유하지 않는다. 이는 workflow를 단순하게하고 airflow가 task를 좀 더 효율적인 방법으로 실행하게한다. 물론 operator 간 정보를 공유하는 방법은 존재한다.
- airflow는 다양한 task를 수행하기 위한 다양한 operator를 가지고 있다.

### BashOperator
- BashOperator는 주어진 배쉬 명령어나 배쉬스크립트를 실행한다. 해당 명령은 workflow에서 의미가 있는 Bash가 할 수 있는 거의 모든 것이다.
- `task_id`, `bash_command`, `dag`  의 세개의 인수가 필요하다. 
- BashOperator는 나중에 자동으로 정리되는 임시 디렉토리에서 명령을 실행한다.
- 명령에 대한 환경 변수를 지정할 수 있다.

#### BashOperator examples
```python
from airflow.operators.bash_operator import BashOperator


 example_task = BashOperator(task_id='bash_ex'
							 bash_command='echo 1'
							 dag=dag  )

bash_task = BashOperator(task_id='clean_address'
					bash_command='cat addresses.txt | awk "NF==10" > cleand.txt'
					dag=dag)
```

### Operator gotchas
- operator를 사용할 때 몇 가지 일반적인 문제가 있다.
- 개별 operator가 동일한 위치 혹은 환경에서 실행된다는 보장이 없다. 
	- 한 operator가 특정 디렉토리에서 어떠한 설정을 가지고 실행되었다고 해서 다음 operator가 같은 정보게 접근할 수 있는 것은 아니다.
- 특히 BashOperator에 대해서는 환경변수를 설정할 필요가 있다.
	- bash에서는 홈 디렉토리를 나타내기 위해서 `~` 을 사용하는 것이 일반적이다. 하지만 airflow에는 이것이 정의되어 있지 않다. 또한 스크립트 실행을 위한 환경변수(HOME 등), AWS credential, DB 접근 등을 위한 환경변수도 설정해줘야한다.
- 상승된 권한으로 task를 실행하는 것이 까다롭다. 리소스에 대한 접근은 작업을 실행하는 특정 사용자에 대해서 설정되어야 한다.

## Airflow tasks
### Tasks
- operator의 인스턴스이다. 워크플로우의 operator를 가리킨다.
- 보통 Python 코드에서 변수로 할당된다.
- Airflow 툴에서는 해당 변수가 아니라 task_id로 접근해야 한다.

### Task depedencies
- task 완료의 순서를 정의한다.
- 필수는 아니지만 보통 task dependency가 있다. 정의되지 않은 경우 작업 실행은 순서 보장 없이 Airflow에서 자체 처리된다.
- task depedency를 upstream task 또는 downstream task라고 한다. upstream task는 다른 downstream task보다 먼저 완료되어야 함을 의미한다.
- Airflow 1.8부터 task dependency는 비트시프트 연산자를 사용하여 정의된다.
	- `>>` : upstream operator
	- `<<` : downstream operator
- 간단하게 upstream은 **이전**을 의미하고 downstream은 **이후**를 의미한다. 

### Simple task dependency
```python
task1 = BashOperator(task_id='first_task'
					 bash_command='echo 1'
					 dag=example_dag)

task2 = BashOperator(task_id='second_task'
					 bash_command='echo 2'
					 dah=example)

task1 >> task2 # or task2 << task1
```

### Task dependencies in the Airflow UI
- 비트시프트 연산자를 사용하지 않은 경우 : task간 종속성이 없음을 확인할 수 있다.
![](images/Pasted%20image%2020221109103933.png)
- 비트시프트 연산자를 사용한 경우 : task간 종속성이 있음을 확인할 수 있다.
![](images/Pasted%20image%2020221109104109.png)

### Multiple dependencies
- 종속성은 필요에 따라 워크플로우를 정의하는데 필요한 만큼 복잡할 수 있다.
- chained dependencies
```python
task1 >> task2 >> task3 >> task4
```

- Mixed dependencies
	- task1 과 task3이 수행되어야 task2가 수행되는 DAG가 생성된다.
	- task1 과 task3은 어떤 것이든 먼저 실행될 수 있다.
```python
task1 >> task2 << task3
```
or
```python
task1 >> task2
task3 >> task2
```

## Additional operators
### PythonOperator
- Python 함수나 메소드등 호출가능한 것을 실행한다.
- bahsoperator 와 비슷하지만 다른 옵션들이 있다. `python_collable` 에는 실행할 것의 이름이 들어간다.
- 함수에 전달할 인수를 지정할 수 있다.
```python
from airflow.operators.python_operator import PythonOperator
def printme():
	print("this goes in the logs!")
python_task = PythonOperator(
		task_id = 'simple_print',
		python_callable=printme,
		dag=example_dag
)
```

#### Arguments
- task에 인수를 전달하는 것을 지원한다. Positional, keyword 둘 다 지원한다.
- `op_kwargs` dict를 이용할 것임
```python
def sleep(length_of_time):
	time.sleep(length_of_time):

python_task = PythonOperator(
		task_id = 'sleep',
		python_callable=sleep,
		op_kwargs={'length_of_time': 5}
		dag=example_dag
)
```

### EmailOperator
- `airflow.operator` 안에 있다.
- 이메일을 전송한다.
- 일반적인 구성요소가 들어갈 수있다.
	- HTML 콘텐츠
	- 첨부파일
 - 이메일을 성공적으로 전송하기 위해서는 이메일 서버 세부정보로 Airflow 시스템을 구성해야한다.
```python
from airflow.operator.email_operator import EmailOperator

email_task = EmailOperator(
		task_id='email_sales_report',
		to='sales_manager@exeample.com',
		subject='Automated Sales Report',
		html_content='Attached is the latest sales report',
		files='leatest_sales.txt',
		dag=example_dag
)
```


## Airflow scheduling
### DAG Runs
- 어떤 시점에서의 워크플로우의 특정 인스턴스이다.
- DAG는 수동으로 실행될 수 있고 `scheule_interval` 을 통해 실행될 수 있다.
- 각 DAG 실행은 자체 및 내부 작업에 대한 상태를 유지한다.
	- `running`
	- `failed`
	- `success`
 ![](images/Pasted%20image%2020221109111731.png)
### Schedule details
DAG를 예약할 때 요구사항에 따라 고려해야할 여러 attribute가 있다.
- `start_date` : DAG를 처음으로 예약할 수 있는 시간 지정 일반적으로 `datetime` 객체로 정의된다.
- `end_date` : DAG를 예약할 수 있는 마지막 시간  
- `max_tries` : DAG 실행이 완전히 실패하기 전에 재시도할 횟수
- `schedule_interval` : DAG가 실행되는 빈도

### Schedule interval
- DAG를 예약하는 빈도를 의미한다.
- 스케쥴링은 `start_date`  와 `end_date` 사이에서 일어난다.  DAG가 완전히 실행된다는 것 보다는, 해당 시간 사이에 스케쥴링 될 수 있다는 것을 의미한다.
- `cron` 포맷으로 정의되거나 빌트인 설정으로 정의된다.

### Airflow schduler presets
Airflow에는 프리셋 혹은 자주 사용되는 시간 간격을 나타내는 바로가기 옵션이 있다.
 | preset  | cron equivalent |
 | ------- | --------------- |
 | @hourly | `0 * * * *`     |
 | @dauly  | `0 0 * * *`     |
 | @weekly | `0 0 * * 0`                |
#### special presets
- `None` : 스케쥴링 하지 않음을 의미하며 수동으로 트리거되는 DAG에 사용된다.
- `@once` : 한 번만 예약한다.

### schedule_interval issues
DAG 스케쥴링에는 고려해야할 몇 가지 뉘앙스가 있다.
- `start_date`를 가능한 가장 빠른 값으로 사용하려고 한다.
- 하지만 스케쥴링은 `start_date` + `schedule_interval` 에 수행된다. 이를 주의해야한다.

# Chapter 3
## Airflow sensors 
- 특정 조건이 참이 될 때 까지 기다리는 operator. 예를 들면 다음의 조건들이 있다.
	- 파일 생성
	- 데이터베이스 레코드의 업로드
	- 웹 요청에 대한 특정 반응
- 조건이 참인지 확인하는 빈도를 정의할 수 있다.
- 센서도 operator이므로 다른 operator와 마찬가지로 task에 할당된다.
	- 비트시프트를 사용할 수 있다.

### sensor details
- `airflow.sensors.base_sensor_operator`에서 가져올 수 있다.
- sensor arguments:
	- `mode` : 조건을 체크하는 방법
		- `mode='poke'` : 기본 값으로 완료될 때 까지 계속 확인한다.
		- `mode='reschedule'` : task 슬롯을 포기하고 다른 슬롯을 사용할 수 있을 때 까지 기다린다.
	- `poke_interval` : 체크하는 빈도
	- `timeout` : task를 실패로 표시하기 전에 대기하는 시간(초)
		- 다른 일반적인 operator attribute도 포함한다. (`task_id`, `dag`)

### File sensor
- `airflow.contrib.sensors` 라이브러리의 일부분이다. 
- 파일 시스템의 특정 위치에 파일이 있는지 확인한다.
- 디렉토리 내의 모든 파일을 확인할 수 있다.
![[Pasted image 20221112130602.png]]

### Other sensors
- `ExternalTaskSensors` : 다른 DAG에 있는 작업이 완료될 때 까지 기다린다.
	- 하나의 워크플로우를 복잡하게 만들지 않고도 다른 워크플로우 작업에 연결할 수 있다.
-  `HttpSensor` : 웹 URL을 요청하고 내용을 확인한다.
- `SqlSensor` : SQL쿼리를 실행하여 내용을 확인한다.
- 다른 센서들은 `airflow.sensors` 또는 `airflow.contrib.sensors`에서 확인할 수 있다.

### why sensors?
- 조건이 언제 참이 되는지가 불확실 할 때 사용할 수 있다.
	- 오늘 어떤 작업이 끝난다는 것은 알지만 해당 시간이 정확하지 않을 때, 주기적으로 체크하게 할 수 있다.
- 해당 조건이 참이 아니더라도 DAG가 즉시 종료되지 않게 할 수 있다.
- Loop 없이 반복적인 작업을 추가할 수 있다.

## Airflow executors
- 워크플로우 내에 정의된 작업을 실제로 실행하는 구성요소이다.
- 각 executor는 tesk set을 실행하기 위한 다른 기능과 동작을 각각 가지고 있다.
- executor의 예 `SequentialExecutor`, `LocalExecutor`, `CeleryExecutor`

### SequentialExecutor
- Airflow 의 기본 실행 엔진이다.
- 한 번에 하나의 작업만 실행한다.
	- 동일한 시간대에 여러 워크플로우를 예약하면 예상보다 시간이 오래 걸릴 수 있다.
-  워크플로우를 팔로우하는 것이 간단하기 때문에 디버깅에 유용하다.
- functional 하지만 학습 및 테스트, 작업 리소스의 제한으로 실제 프로덕션에서는 추천되지 않는다. 

### LocalExecutor
- 단일 시스템에서 실행된다.
- 각 작업을 로컬 시스템의 프로세스로 취급한다.  
- parallelism 은 사용자에 의해 정의된다. 
- 해당 호스트 시스템의 모든 리소스를 활용할 수 있다.

## CeleryExecutor
- Celery는 여러 시스템이 클러스터로 통신할 수 있도록 Python 으로 작성된 일반 대기열 시스템이다.
- Celery backend를 작업 관리자로 사용한다.
- 여러 Airflow 시스템이 주어진 워크플로우나 작업의 워커로 구성될 수 있다.
- 셋업과 구성을 설정하는 것이 더 어렵다.
- 많은 수의 DAG로 작업하거나 처리할 요구 사항이 증가할 것으로 예상되는 조직에 아주 효과적인 방법이다.

### Determin your executor
- `airflow.cfg` 파일에서 확인할 수 있다.
	- `cat airflow/airflow.cfg | grep "executor = "`
- cli에서 `airlfow list_dags`에서 INFO로도 확인할 수 있다.

## Debugging and troubleshooting in Airflow
### Typical issues
- 일정에 따라 실행되지 않는 DAG
- 단순히 시스템에 로드되지 않는 DAG
- 문법 에러

### DAG won't run on schedule
- 일반적인 이유는 스케쥴러가 실행되고 있지 않아서이다. 실행되고 있는지 체크하자.
	- 보통 웹 UI에 바로 알람이 온다.
	- cli 에서 `airflow scheduler`로 해결 가능
- `schedule_interval`이 경과되지 않은 경우 
- 해당 executro에 작업을 실행하기 위한 여유 슬롯이 없는 경우
	- executor type을 바꾼다.
	- 시스템 리소스 추가
	- 시스템 추가
	- DAG일정 변경 

 ### DAG wont't load
 - DAG가 웹 UI에서 보이지 않고 `airflow list_dags`에도 뜨지 않는 경우
	 -  DAG 파일이 올바른 디렉토리에 있는지 확인
		 - `airflow.cfg` 파일을 통해 DAG 디렉토리 설정 가능
		 - 해당 경로는 절대경로여야 한다.

### Syntex errors
- DAG가 나타나지 않는 가장 일반적인 이유이다.
- `airflow list_dags` 혹은 `python <dagfile.py>` 로 문법 에러가 있는지 확인해보는 것이 빠르다. 

## SLAs and reporting in Airflow

## SLAs
- Service Leval Agreement 를 의미한다.
	- 비즈니스에서 이는 가동시간, 가용성 보장을 의미한다. 
	- Airflow에서는 작업 또는 DAG를 실행하는데 필요한 시간을 의미한다.
- SLA Miss 는 작업 또는 DAG가 SLA의 예상 타이밍을 충족하지 못하는 모든 상황을 의미한다.
- SLA Miss인 상황에는 시스템 구성에 따라 이메일 경고가 전송되고 로그에 정보가 저장된다.
- SLA miss는 웹 UI에서 확인할 수 있다.

### SLA Misses
![[Pasted image 20221112135509.png]]
- SLA를 누락한 작업과  실패한 시기에 대한 정보를 제공한다.
- 실패시 이메일이 전송되었는지 여부도 나타낸다.

### Defining SLAs
- task에서 `sla` 인수를 적용하는 방법
![[Pasted image 20221112135725.png]]
- `defalut_args` dict에 sla 키를 정의하는 방법
![[Pasted image 20221112135837.png]]

### General reporting
- 성공, 실패, 에러 시 메세지를 보내기 위한 기본 제공 옵션이 있다.
- dag 생성시 전달하는 `default_args` 의 키로 전달된다. 필수 요소는 email 키이다.
![[Pasted image 20221112140055.png]]

# chapter 4

## Working with templates
- DAG 실행중에 정보를 대체할 수 있다.
- 작업을 정의할 때 추가적인 유연성을 제공한다. 
- `Jinja` 템플릿 언어를 사용해서 생성한다.
![[Pasted image 20221112144138.png]]

## More templates
### Advanced template
![[Pasted image 20221112144914.png]]

### Variables
- 템플릿 시스템의 일부로 Airflow는 내장 런타임 변수 세트를 제공한다.
- DAG실행, 개별 작업 및 시스템 구성에 대한 다양한 정보를 제공한다.
![[Pasted image 20221112145149.png]]

### Macros
- 매크로 변수인 `{{ macros }}` 도 존재한다.
- Airflow 템플릿에 대한 다양한 유용한 객체, 메소드에 대한 참조를 제공한다. 
- `{{ macros.dateime }}` : `datetime.datetime` 객체
- `{{ macros.timedelta }}` : `timedelta` 객체
- `{{ macros.uuid }}` : `uuid` 객체
- `{{ macros.ds_add('2020-04-15', 5) }}` : 템플릿 내에서 날짜 계산을 수행하는 쉬운 방법을 제공한다.

## Branching
- Airflow 내에서 조건부 논리에 대한 기능을 제공한다. operator 결과에 따라 작업을 선택적으로 실행하거나 건너뛸 수 있음을 의미한다.
- 기본적으로 `BranchPythonOperator` 를 사용한다.
	- `from aiflow.operators.python_operator import BranchPythonOperator`
	- `python_callable` 을 인수로 가지고 실행할 task_id 를 반환한다.
![[Pasted image 20221112150606.png]]

## creating a production pipeline

### DAG, task 실행 리마인더
- 특정 작업 실행하기
	- `airflow run <dag_id> <task_id> <date>`
- DAG 실행하기
	- `airflow trigger_dag -e <date> <dag_id>`

### Operator 리마인더
- `BashOperator` - `bash_command` 를 인수로 받는다.
- `PythonOperator` - `python_callable` 을 인수로 받는다.
- `BranchPythonOperator` - `python_callable` 과 `provide_context=True` 를 인수로 받고, `python_callable`은 `**kwargs`를 인수로 받아야만한다.
- `FileSensor` - `filepath` 가 필요하고 `mode`나 `poke_interval` 이 필요하다. 
 
### Template 리마인더
- Airflow의 다양한 객체가 template을 사용할 수 있다.
- 특정 필드는 템플릿 문자열을 허용하지만 그렇지 않은 필드도 있다.
	- 빌트인 도큐멘테이션을 확인하는 것이 필드를 체크하는 방법중 하나다.  
![[Pasted image 20221112151942.png]]
