# What is spark
스파크는 클러스터 컴퓨팅을 위한 플랫폼이다. 스파크는 데이터와 계산을 다수의 노드로 구성된 클러스터에 나누게 한다. 각 노드는 작은 양의 데이터에만 작동하기 때문에 데이터를 나누는 것은 아주 큰 용량의 데이터셋을 다루는데 도움이 된다.
각각의 노드는 전체 데이터의 하위 집합으로만 작업하고 필요한 전체 계산의 일부도 수행하므로 데이터 처리와 계산이 클러스터 내부 노드들에서 병렬적으로 이루어져야 한다. 병렬 계산이 특정 유형의 프로그래밍 작업을 더 빠르게 하는 것은 사실이다.
그러나 컴퓨팅 파워가 좋아디면서 복잡성도 커졌다.
문제에 대해서 spark가 최선의 선택인지 아닌지 정하는 것은 경험이 필요하지만 다음의 질문을 고려해 볼 수 있다.
- 단일 컴퓨터에서 다루기에는 너무 큰 데이터인가?
- 계산이 쉽게 병렬적으로 이루어질 수 있는가?

## Using Spark in Python
스파크를 사용하는 첫 단계는 클러스터에 연결하는 것이다.
실제로 클러스터는 모든 다른 노드에 연결된 원격 시스템에서 호스팅된다. 데이터와 계산을 분할하는 마스터라고 불리는 한 대의 컴퓨터가 있다. 마스터는 워커라고 불리는 클러스터의 나머지 컴퓨터들과 연결되어 있다. 마스터는 워커에 데이터와 실행할 계산을 보내고 워커는 그 결과를 다시 마스터에게 보낸다.
스파크를 막 시작할 때는 클러스터를 로컬에 실행하는 것이 간단하다. 그러므로 이 과정에서는 다른 컴퓨터에 연결하는 대신 모든 계산은 시뮬레이션 된 클러스터의 데이터 캠프 서버에서 실행된다.
연결을 만드는 것은 `SparkContext`의 인스턴스를 만들기만 하면 된다. 클래스 생성자는 연결하려는 클러스터의 특성을 특정하는 약간의 옵셔널 아큐먼트를 받는다.
이러한 모든 특성을 가지고 있는 객체는 `SparkConf()` 생성자와 생성된다. 

## Using DataFrames
스파크의 핵심 데이터 구조는 Resilient Distributes Dataset(RDD)이다. RDD는 클러스터의 여러 노드에 걸켜 데이터를 분할하여 spark가 작업할 수 있도록 하는 저수준 객체이다. 그러나 RDD는 직접 사용하는 것이 어렵기 때문에 이 과정에서는 RDD위에 구축된 Spark DataFrame 추상화를 사용하게 된다.
스파크 데이터프레임은 SQL 테이블 처럼 동작하도록 설계되었다. 이해하기 쉬울뿐만 아니라 데이터프레임은 복잡한 작업에 RDD보다 더 최적화 되어 있다.
데이터의 열과 행을 수정하고 결합할 때, 같은 결과를 얻을 수 있는 여러가지 방법이 있지만, 어떤 방법은 다른 방법들 보다 훨씬 오래 걸리는 경우도 있다. RDD를 사용할 때 쿼리를 최적화 하는 올바른 방법을 찾는 것은 데이터 사이언티스트의 몫이지만, 데이터 프레임 구현에는 이러한 최적화 기능이 많이 내장되어 있다.
스파크 데이터프레임으로 작업하려면 먼저 `SparkContext`에서  `SparkSession` 객체를 생성해야 한다. `SparkContext`를 클러스터에 대한 연결로 생각하고 `SparkSession`을 해당 연결에 대한 인터페이스로 생각하면 된다.

## PySpark 사용
- `SparkSession.builder.getOrCreate()`을 사용하면 기존 SparkSession을 반환하거나 새로운 SparkSession을 생성한다.
```python
# Import SparkSession from pyspark.sql
from pyspark.sql import SparkSession

# Create my_spark
my_spark = SparkSession.builder.getOrCreate()
```
- `spark.catalog.listTables()` 을 사용하면 테이블의 목록을 확인할 수 있다.
```python
# Print the tables in the catalog
print(spark.catalog.listTables())
```
- `SparkSession` 즉 데이터프레임 인터페이스의 장점은 스파크 클러스터의 테이블에서 SQL 쿼리를 실행할 수 있다는 것이다. 
```python
# Don't change this query
query = "FROM flights SELECT * LIMIT 10"

# Get the first 10 rows of flights
flights10 = spark.sql(query)

# Show the results
flights10.show()
```
- 때로는 Pandas를 사용해서 로컬에서 데이터를 다루는 것이 알맞을 때가 있다. 그럴때에는 쿼리로 만든 새로운 테이블을 pandas DataFrame으로 만들어주면 된다.
```python
# Don't change this query
query = "SELECT origin, dest, COUNT(*) as N FROM flights GROUP BY origin, dest"

# Run the query
flight_counts = spark.sql(query)

# Convert the results to a pandas DataFrame
pd_counts = flight_counts.toPandas()

# Print the head of pd_counts
print(pd_counts.head())
```
- 위와는 반대로 pandas DataFrame을 스파크 클러스터에 넣는 방법도 있다. `.createDataFrame()`은 pandas DataFrame을 가져와서 spark DataFrame으로 만든다. 하지만 이는 로컬에 저장될 뿐 쿼리를 실행하려면 catalog에 저장해야 한다. 
- `.createTempView()`는 등록할 테이블 명을 인수로 받아 spark DataFrame을 catalog에 저장한다. 하지만 이는 임시 테이블로 특정 SparkSession만 접근할 수 있다. 혹은 `.createOrReplaceTempView()`를 사용할 수도 있다. 이전에 생성된 것이 없다면 그냥 생성하고 이미 생성된게 있다면 해당 테이블을 업데이트 한다.
![](images/Pasted%20image%2020221117200320.png)
```python
# Create pd_temp
pd_temp = pd.DataFrame(np.random.random(10))

# Create spark_temp from pd_temp
spark_temp = spark.createDataFrame(pd_temp)

# Examine the tables in the catalog
print(spark.catalog.listTables())

# Add spark_temp to the catalog
spark_temp.createOrReplaceTempView('temp')

# Examine the tables in the catalog again
print(spark.catalog.listTables())
```
- 파일에서 데이터를 바로 읽어올 수도 있다.
```python
# Don't change this file path
file_path = "/usr/local/share/datasets/airports.csv"

# Read in the airports data
airports = spark.read.csv(file_path, header=True)

# Show the data
airports.show()
```
