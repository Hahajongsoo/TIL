# 클러스터 아키텍처
쿠버네티스의 목적은 애플리케이션을 컨테이너의 형태로 자동화된 방식으로 배포하는 것이다. 그래야 요구에 따라 애플리케이션의 많은 인스턴스를 쉽게 배포할 수 있고 애플리케이션끼리의 통신이 쉽게 가능하다. 
쿠버네티스 클러스터는 노드의 집합으로 이뤄져있다. 호스트 애플리케이션이 컨테이너 형태로 있는 물리 PC일 수도 있고 가상 머신일 수도 있고 온프레미스일 수도 있고 클라우드에 있을 수도 있다. 
worker node는 컨테이너를 실행하는 노드이다. master node는 어떻게 컨테이너를 실행할 지 계획하고, 어떤 노드에 실행할지 결정하고, 노드와 컨테이너에 대한 정보를 저장하고, 컨테이너들을 모니터링하는 등의 모든 컨테이너 실행 프로세스를 관리한다.  
- etcd: 쿠버네티스의 모든 정보는 고가용성의 키-값 저장소인 etcd에 저장된다. 
- kube-scheduler: 컨테이너에 할당된 리소스와 노드에서 사용 가능한 리소스를 확인하고 컨테이너를 가능한 노드에 실행한다거나 컨테이너가 실행되어야 하는 노드에만 실행한다거나 하는 일을 하는 것이 스케쥴러이다. 
- controller-manager
	- node-controller: 클러스터에 새로운 노드를 띄우는 역할을 한다. 노드를 사용할 수 없는 상황을 핸들링한다.
	- Replication-controller: replication 그룹에서 원하는 수의 컨테이너가 실행되도록 한다. 
- kube-apiserver: 쿠버네티스에서 가장 중요한 관리 구성요소이다. 클러스터 내에서 모든 작업을 오케스트레이션한다. 
-  애플리케이션이 컨테이너의 형태로 실행된다. 마스터 노드에 클러스터를 관리하는 여러 구성요소들이 컨테이너의 형태로 실행될 수 있고 클러스터의 DNS 시스템이 컨테이너로 실행될 수 있다. 따라서 컨테이너 런타임이 필요하다 그 중 하나가 도커가 될 수 있다. 
- kubelet: 클러스터의 각 노드에서 실행되는 에이전트이다. kube apiserver의 지시를 듣고 필요한대로 노드에서 컨테이너를 배포하거나 삭제한다. kube apiserver는 주기적으로 kubelet으로 부터 노드와 컨테이너의 상태를 모니터링하기 위해서 상태 보고서를 가져온다. 
- kube-proxy: 클러스터의 여러 노드들에서 실행되는 컨테이너들이 서로 통신할 수 있게 한다. 
# Docker vs ContainerD
쿠버네티스 관련 글을 보다보면 예전에는 docker, 요즘에는 containerD가 같이 언급되는 것을 볼 수 있다. 또한 `ctr`, `nerdctl`, `crictl` 등의 cli 툴도 볼 수 있었을 것이다.
## docker
컨테이너 툴 중에서 처음에는 docker만 있었다. rkt 같은 다른 도구도 있었지만 docker의 ux가 컨테이너를 사용하는 것을 매우 간단하게 만들었고 그 결과 docker가 가장 주요한 컨테이너 툴이 됐다. 그리고 나서 docker를 오케스트레이션하려고 k8s가 등장했다. k8s는 처음에는 docker를 오케스트레이션 하려고 만들어졌기 때문에 서로 강하게 연결되어 있었고 k8s는 docker로만 작동하고 다른 컨테이너 툴은 지원하지 않았다. 
이후 k8s가 컨테이너 오케스트레이터로 인기를 얻으면서 다른 컨테이너 런타임도 참여하고자 했다. 그래서 k8s는 CRI(Container Runtime Interface)를 도입했다. CRI를 통해 어떤 공급자도 OCI(Open Container Intiative)를 준수함다면 k8s의 컨테이너 런타임으로 작동할 수 있게 됐다. OCI는 이미지를 빌드하는 방법에 대한 명세인 `imagespec` 과 컨테이너 런타임 개발에 대한 표준인 `runtimespec`를 정의한다. 이러한 표준을 기반으로 누구든지 k8s와 함께 이용할 수 있는 컨테이너 런타임을 개발할 수 있었다. 
그러나 docker는 CRI 표준을 지원하기 위해 만들어지지 않았다. 그럼에도 docker는 대부분의 사용자가 사용하는 툴이었기 때문에 k8s는 CRI 외부에서 docker를 지원하기 위한 임시적인 방법인 `dockershim`을 도입했다. 다른 컨테이너 툴들은 CRI를 통해 작동하는 반면 docker는 CRI 없이 작동했다. docker는 컨테이너 런타임만 있는 것이 아니다. docker CLI, API, BUILD, VOLUMES, AUTH, SECURITY 및 runC라 불리는 컨테이너 런타임 그리고  runC를 관리하는 데몬인 Containerd가 포함되어 있다. 그런데 Containerd는 CRI 호환성이 있어서 다른 런타임 처럼 직접 작동할 수 있기 때문에 docker와 별개로 런타임으로 사용될 수 있다. 이후 dockershim을 유지하는 것은 불필요한 노력과 복잡성을 추가했기 때문에 k8s 1.24 버전부터는 dockershim을 제거하기로 했고 docker 지원도 제거되었다. 그러나 docke로 빌드된 이미지들은 컨테이너로 동작하기 위해서  OCI에 의한 표준인 `imagespec`을 따르기 때문에 k8s가 컨테이너 런타임으로 docker를 지원하지 않더라도 containerd와는 계속 동작한다. 
## ContainerD
containerd는 docker의 일부인 동시에 자체 프로젝트이며, CNCF의 gradutaed 상태이다. 따라서 containerd 단독으로 설치해서 사용할 수 있다. 
### CLI
#### ctr
containerd 를 설치하면 ctr이라는 명령 줄 도구가 함께 제공되며, 주로 containerd를 디버깅하는 데 사용된다. 기능이 제한적이고 사용자 친화적이지 않기 때문에 운영 환경에서 컨테이너를 실행하거나 관리하기 위해서 사용하지 않는다.
#### nerdctl
더 나은 대안은 nerdctl이다. containerd를 위한 docker 같은 CLI를 제공한다. 또한 암호화된 컨테이너 이미지 사용 같은  docker CLI에서 미래에 정식으로 구현될 새로운 기능에 대한 액세스를 제공한다. 또한 이미지의 lazy pulling, p2p image distribution, image 서명 및 검증, 도커에서는 사용할 수 없는 namespace 등을 제공한다. docker cli에서 실행하는 거의 모든 명령을 docker 대신 nerdctl을 사용하는 것으로 사용할 수 있다.

## crictl
crictl은 CRI 호환 컨테이너 런타임과 상호 작용하는데 사용되는 CLI이다.이는 k8s 관점에서 볼 때 일종의 상호작용이다. k8s 커뮤니티에서 개발 및 유지관리 하며 다양한 컨테이너 런타임에서 작동한다. 주로 디버깅 목적으로 사용된다. crictl로 컨테이너를 생성할 수 있지만 쉽지 않다. 컨테이너를 실행하는 것과는 거리가 멀다. 이러한 작업은 kubelet과 이뤄지는 것을 기억해야한다. kubelet은 특정한 수의 pod가 노드에 있는지 확인하는 역할을 담당한다. 따라서 crictl 도구를 사용하여 컨테이너를 생성하려고 하면 kubelet은 관련 정보를 모르기 때문에 해당 컨테이너를 삭제할 것이다. 위의 cli 툴들과 가장 주요한 차이는 파드를 알고 있다는 것이다. 따라서 파드 리스트를 확인할 수 있다. 
docker와 명령이 유사하다. 
k8s 1.24 이전의 버전에서는 dockershim이 사용되었기 때문에 기본 컨테이너 런타임 엔드포인트에 dockershim 이 포함되어 있었으나 이후 버전 부터는 그렇지 않기 때문에 해당 엔드포인트가 cri-codkerd.sock으로 대체되었다. crictl을 사용하는 경우 사용자가 수동으로 엔드포인트를 수정해야한다. 
# ETCD
## ETCD for beginners
etcd는 분산 신뢰성 있는 키-값 스토어로 간단하고 안전하며 빠르다. 
## key-value store
전통적인 데이터베이스는 표 형식으로 구성되어있다. RDB는 row와 column 형식으로 데이터를 저장한다. 새로운 정보를 추가할 때마다 전체 테이블에 영향을 주고 많은 빈 셀이 생긴다. 새로운 정보들은 열에 들어가고 어떤 행들은 해당 정보를 가지지 않을 수 있기 때문이다. 예를 들어 사람에 대한 정보를 추가한다고 했을 때, 연봉이 포함될 수 있지만 일하지 않는 경우 연봉이 포함될 수 없다. 또한 학생의 경우 성적이 포함될 수 있지만 성인의 경우 현재 성적이 포함될 수 없다.
키-값 스토어는 문서나 페이지 형식으로 정보를 저장한다. 각 개인은 문서를 갖고 해당 개인에 대한 모든 정보가 해당 파일에 저장된다. 이러한 파일은 어떤 형식이나 구조이든 상관 없으며 하나의 파일을 변경하더라도 다른 파일에는 영향을 주지 않는다. 이러한 문서에 추가 세부 정보를 추가하려면 다른 문서를 업데이트 하지 않고도 정보를 추가할 수 있다. 
### ETCD
etcd 는 실행시 기본적으로 2379 포트를 사용한다. 이후 etcd 서비스에 클라이언트를 연결하여 정보를 저장하고 검색할 수 있다. etcd 와 함께 제공되는 기본 클라이언트는 etcdctl 클라이언트이다. etcdctl 클라이언트는 etcd의 명령 줄 클라이언트로 키-값 쌍을 저장하고 검색하는 데 사용할 수 있다. 
정보를 저장하기 위해서 다음의 명령을 사용할 수 있다.
```bash
etcdctl set key1 value1
```
정보를 검색하기 위해서는 다음의 명령을 사용할 수 있다.
```bash
etcdctl get key1
value1
```
### version
첫 번째 버전인 0.1 버전은 2013년 8월에 출시됐다. 공식적인 stable 버전인 2.0 버전은 2015년 2월에 출시됐으며 이때 RAFT 합의 알고리즘이 재설계됐고 초당 10000회 이상의 쓰기를 지원했다. 이후 2017년 1월에는 성능 향상 및 최적화가 더 많이 이루어진 버전 3을 출시했다. 2018년 11월에는 etcd 프로젝트가 CNCF에서 인큐베이션을 받았다. 
여기서 중요한 것은 2 버전에서 3 버전으로 넘어갈 때 많은 변경 사항이 있었다는 것이다. API 버전이 변경되며 etclctl 명령어도 변경되었다. 
위에서 살펴본 set과 get 명령어는 2 버전의 명령이지만 etcdctl 명령은 버전 2와 버전 3을 동시에 사용하도록 구성된다. API 버전이 2인 경우 `etcdctl --version` 명령을 사용하면 etcdctl 버전과 API 버전을 확인할 수 있다. API 버전을 변경하려면  `export ETCDCTL_API=3` 같이 환경 변수에 값을 설정해주면 된다. API 버전이 3인 경우 `etcdctl version` 으로 버전을 확인할 수 있고 값을 설정하는 명령도 set이 아닌 put이다.
## ETCD in kubernetes
etcd는 노드, 파드, 컨피그맵, 시크릿, 계정, 롤, 롤 바인딩 등과 같은 클러스터에 관한 정보를 저장한다. `kubectl get` 명령으로 볼 수 있는 모든 정보는 etcd 서버에서 가져온 것이다. 노드를 추가하거나 파드를 배포하는 등 클러스터에 대한 모든 변경 사항은 etcd 서버에 업데이트된다. etcd 서버에 업데이트 된 경우에만 변경 사항이 완료된 것으로 간주된다. 
클러스터를 어떻게 설정했느냐에 따라 etcd가 다르게 배포된다. 
### by kubernetes install
#### manual
클러스터를 스크래치에서 설정한 경우 자체로 etcd 바이너리를 다운로드하고 이를 설치한 다음 마스터 노드에서 etcd를 서비스로 구성하여 배포한다. 서비스로 전달되는 옵션은 인증서, 클러스터 설정 관련 등 여러 옵션이 있다. 여기서 알아야할 부분은 `--advertise-client-urls` 이다. 이것은 etcd가 수신 대기하는 주소로 기본적으로 서버의 IP 및 기본 포트인 2379이다. kube-apiserver가 etcd 서버에 도달하려고 할 때 구성해야 하는 url이다. 
#### kubeadm
클러스터를 kubeadm으로 설치하는 겨우 kubeadm는 kube-system 네임스페이스의 파드로 etcd 서버를 배포한다. 해당 파드에서 etcd 데이터베이스를 탐색할 수 있는 etcdctl 유틸리티를 사용할 수 있다. kubernetes에 저장된 모든 키를 리스트 하려면 다음과 같은 명령을 사용하면 된다.
```bash
vagrant@kube-control1:~$ kubectl exec -it -n kube-system etcd-kube-control1 -- etcdctl --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/server.crt --key /etc/kubernetes/pki/etcd/server.key get / --prefix --keys-only | head
/registry/apiextensions.k8s.io/customresourcedefinitions/apiservers.operator.tigera.io

/registry/apiextensions.k8s.io/customresourcedefinitions/bgpconfigurations.crd.projectcalico.org

/registry/apiextensions.k8s.io/customresourcedefinitions/bgppeers.crd.projectcalico.org

/registry/apiextensions.k8s.io/customresourcedefinitions/blockaffinities.crd.projectcalico.org

/registry/apiextensions.k8s.io/customresourcedefinitions/caliconodestatuses.crd.projectcalico.org
```
kubernetes에서 저장하는 데이터는 특정 디렉토리 구조에 저장된다. 루트 디렉토리는 레지스트리이며 그 하위에 노드, 파드, 레플리카셋, 디플로이먼트 등과 같은 k8s 구성 요소가 있다.
### HA environment
고가용성 환경에서 클러스터에는 여러 마스터 노드가 있고, 마스터 노드에 분산된 여러 etcd 인스턴스가 있을 것이다. 이 경우 etcd 인스턴스가 올바르게 서로를 알 수 있도록 etcd 서비스 구성에서 올바른 파라미터를 설정하는 것이 중요하다. `--initial-cluster` 옵션에 etcd 서비스의 다른 인스턴스를 지정해야 한다.  
# kube-API server
kube-apiserver는 k8s의 주요 관리 구성 요소이다. `kubectl` 명령을 실행하면 실제로는 kube-apiserver에 도달한다는 것을 기억해야한다. kube-apiserver는 먼저 요청을 인증하고 유효성을 검사한다. 그런 다음 etcd 클러스터에서 데이터를 검색하고 요청된 정보를 응답한다. 실제로 kubectl 명령을 사용하지 않아도 된다. 대신 POST 요청을 보내서 API를 직접 호출할 수도 있다. 
파드를 생성하는 예를 살펴보자. 이 작업을 수행할 때도 요청이 인증되고 검증된다. 이 경우 
- apiserver는 노드에 할당하지 않은 채로 파드 객체를 생성한다. 
- etcd 서버의 정보를 업데이트하고 파드가 생성되었다는 정보를 사용자에게 업데이트 한다. 
- 스케쥴러는 apiserver를 지속적으로 모니터링하고 노드가 할당되지않은 새 파드가 있다는 것을 인식한다. 이후 스케줄러는 새 파드를 배치할 적절한 노드를 식별하고 이를 kube-apiserver에 통보한다. 
- apiserver는 그 정보를 etcd 클러스터에 업데이트한다.
- 이후 apiserver는 해당 정보를 적절한 워커 노드의 kubelet에 전달한다.
- kubelet은 해당 노드에서 파드를 생성하고 컨테이너 런타임 엔진에 애플리케이션 이미지를 배포하도록 지시한다. 작업이 완료되면 kubelet은 상태를 다시 apiserver에 업데이트하고 apiserver는 다시 etcd 클러스터에 데이터를 업데이트한다.
변경이 요청될 때 마다 이와 유사한 패턴이 반복된다. kube-apiserver는 클러스터에서 변경을 수행해야 하는 다양한 작업의 중심에 있다. 
kube-apiserver는 사용자를 인증하고 요청을 검증한다. 그리고 etcd 에서 데이터를 검색하고 업데이트 한다. kube-apiserver는 etcd와 직접 상호 작용하는 유일한 구성 요소이다. kube-scheduler, kube-controller-manager, kubelet 등의 다른 구성 요소는 업데이트를 수행하기 위해서 kube-apiserver를 사용해야한다. 
## install
kubeadm 으로 클러스터를 부트스트랩 했다면 알 필요 없지만, 하드웨어 위에 설정하는 경우 kube-apiserver는 kubernetes 릴리스 페이지의 바이너리로 사용할 수 있다. 다운로드하고 마스터 노드에서 서비스로 실행하도록 구성한다. kube-apiserver는 많은 파라미터와 함께 실행된다.
k8s 아키텍처는 서로 다른 다양한 구성 요소가 많은 다른 방식으로 상호 작용하고 서로 통신해야 하므로 모든 구성 요소가 다른 구성 요소의 위치를 알아야 한다. 인증, 권한 부여, 암호화 및 보안에 다양한 모드가 있다. 그래서 여러 옵션이 있다.
그 중 많은 옵션은 다른 구성 요소 간의 연결을 보호하는 데 사용되는 인증서이다. `--etcd-server` 옵션은 etcd 서버의 위치를 지정하는 옵션이다. 이를 통해 kube-apiserver가 etcd 서버와 연결한다. 
### options
kubeadm 으로 설치한 경우 kubeadm은 마스터 노드의 kube-system 네임스페이스에 kube-apiserver를 파드로 배포한다. 파드 manifest 파일에서 옵션을 볼 수 있으며 해당 파일은 `/etc/kubernetes/manifest` 디렉토리에 있다.
kube-apiserver를 그냥 설치한 경우 kube-apiserver 서비스를 검사하여 옵션을 확인할 수 있다. 해당 파일은 `/etc/systemd/system/kube-apiserver.service` 이다.
또한 마스터 노드에서 ps 명령을 이용하여 실행 중인 프로세스와 옵션을 볼 수 있다.
# kube-controller-manager
kube-controller-manager는 쿠버네티스의 다양한 컨트롤러를 관리한다. 
## controller
쿠버네티스에서 컨트롤러는 시스템 내의 다양한 구성 요소의 상태를 계속 모니터링하고 전체 시스템을 원하는 작동 상태로 가져오도록 하는 프로세스이다.
예를 들어 노드 컨트롤러는 노드의 상태를 모니터링하고 응용 프로그램을 계속 실행시키기 위해서 적절한 조치를 취하는 역할을 한다. 이는 kube-apiserver를 통해 수행된다. 노드 컨트롤러는 노드의 상태를 매 5초마다 테스트한다. 이렇게 하면 노드 컨트롤러가 노드의 상태를 모니터링할 수 있다. 노드로부터 heartbeat를 받지 못하면 해당 노드는 unreachable로 표시된다. unreachable로 바뀌기 전 까지 40 초를 대기한다. unreachable 로 표시도니 후에는 5분 동안 다시 올라올 기회를 준다. 그렇지 않으면 해당 노드에 할당된 파드를 제거하고 해당 파드가 레플리카셋의 일부인 경우 동작하는 노드에 할당한다.  
레플리케이션 컨트롤러는 레플리카셋의 상태를 모니터링하고 레플리카셋 내에서 언제나 원하는 수의 파드를 사용할 수 있도록 보장한다. 만약 파드가 죽으면 다른 파드를 생성한다. 
쿠버네티스에는 이외에도 다양한 컨트롤러가 있다. 지금까지 쿠버네티스에서 본 개념 즉, 디플로이먼트, 서비스 , 네임스페이스 PV 와 같은 모든 것들은 이러한 다양한 컨트롤러를 통해 구현된다. 쿠버네티스의 많은 기능의 뒤에 있는 두뇌 역할을 한다. 
이 모든 컨트롤러들은 kube-controller-manager라고 하는 단일 프로세스에 패키지화된다. kube-controller-manager를 설치하면 다른 컨트롤러들도 같이 설치된다. 
## install
쿠버네티스 릴리스 페이지에서 kube-controller-manager를 다운로드하고 압축 해제 하고 서비스로 실행한다. 실행하면 볼 수 있는 여러 옵션들이 있다. 이 중에는 노드 컨트롤러에서 이야기 했던 노드 모니터링 주기, unreachable로 변경되는 유예 기간 및 pod-eviction 과 같은 설정이 있다. controllers라는 추가 옵션이 있으며 이를 사용하여 활성화할 컨트롤러를 지정할 수 있다.
## options
kubeadm 으로 설치한 경우 마스터 노드의 kube-system 네임스페이스에 kube-controller-manage를 파드로 배포한다. 파드의 manifest 파일 내의 옵션을 볼 수 있으며 해당 파일은 `/etc/kubernetes/manifest` 디렉토리에 있다.
kubeadm 이 아닌 경우 서비스 디렉토리에 위치한 kube-controller-manager 서비스를 검사하여 옵션을 볼 수 있다.
도한 마스터 노드에서 프로세스 목록을 나열하고 kube-controller-manager를 검색하여 실행 중인 프로세스와 유효한 옵션을 볼 수 있다.
# kube-scheduler
kube-scheduler는 노드에 파드를 스케줄링하는 역할을 담당한다. 스케줄러는 어떤 파드가 어떤 노드에 배치되어야 하는지 결정하는 역할만 한다. 실제로 파드를 노드에 배치하는 것은 kubelet의 역할이다.
쿠버네티스에서 스케줄러는 일정한 기준에 따라 파드를 배치할 노드를 결정한다. 각 파드에는 다른 리소스 요구사항이 있을 수 있다. 클러스터에는 특정 응용 프로그램에 대한 전용 노드가 있을 수 있다.
예를 들어 파드에는 CPU 및 메모리 요구 사항이 있다. 스케줄러는 파드에 대한 가장 적합한 노드를 식별하기 위해 두 가지 단계를 거친다. 
1. 첫 번째 단계에서 스케줄러는 이 파드에 맞지 않는 노드를 필터링하려고 노력한다. 예를들어 파드에서 요청한 CPU 및 메모리 리소스를 충족시키지 못하는 노드를 필터링한다.
2. 우선순위 함수를 사용하여 노드에 0부터 10까지의 점수를 할당한다. 예를 들어 스케줄러는 파드를 배치한 수 남는 리소스의 양을 계산한다. 더 많은 리소스가 남는 노드에 더 높은 점수를 부여한다. 
물론 이것은 사용자 정의할 수 있으며 자신만의 스케줄러를 작성할 수 있다. resource request and limit, taints and tolerations, node selector/affinity 등 다양한 기준이 있다. 
## install
쿠버네티스 릴리스 페이지에서 다운로드하고 서비스로 실행한다. 서비스로 실행할 때 스케줄러 manifest 파일을 지정한다.
## options
kubeadm 으로 설치한 경우 마스터 노드의 kube-system 네임스페이스에 kube-scheduler를 파드로 배포한다. 파드 manifest 파일에서 옵션을 확인할 수 있고 해당 파일은 `/etc/kubernetes/manifest` 디렉토리에 있다.
kubeadm 설정이 아닌 경우 마스터 노드에서 프로세스 목록을 나열하여 옵션을 확인할 수 있다.
# kubelet
워커 노드에 있는 kubelet은 쿠버네티스 클러스터에 노드를 등록한다. 노드에 컨테이너나 파드를 로드하도록 지시받으면 해당 노드에서 필요한 이미지를 가져오고 인스턴스를 실행하도록 컨테이너 런타임 엔진에 요청한다. 그런다음 kubelet은 파드와 해당하는 컨테이너의 상태를 지속적으로 모니터링하고 정기적으로 kube-apiserver에 보고한다. 
## install
kubeadm으로 구성하더라도 kubelet은 자동으로 배포되지 않는다. 워커 노드에 kubelet을 수동으로 설치해야 한다. 설치 파일을 다운로드하고 서비스로 실행한다.
워커 노드에서 실행중인 kubelet 프로세스와 유효한 옵션을 확인하려면 워커노드에서 프로세스 목록을 나열하고 kubelet을 검색하면 된다. 
# kube-proxy
쿠버네티스 클러스터 내에서 모든 파드는 다른 모든 파드에 도달할 수 있다. 이것은 클러스터에 파드 네트워킹 솔루션을 배포하여 달성된다. POD network 는 모든 노드에 걸쳐 있는 내부 가상 네트워크로 모든 파드가 연결된다. 이러한 네트워크를 배포하기 위한 많은 솔루션이 있다.
예를들어 첫 번째 노드에 웹 애플리케이션을 배포하고 두 번째 노드에 DB 애플리케이션을 배포했다고 하자. 웹 앱은 단순히 파드의 IP를 사용하여 DB에 액세스 할 수 있지만 DP 파드의 IP가 항상 동일하다는 보장은 없다. 웹 앱이 DB에 액세스하는 더 좋은 방법은 서비스를 사용하는 것이다. 따라서 클러스터 전체에서 DB 애플리케이션을 노출하기 위해 서비스를 만든다. 웹 앱은 이제 서비스의 이름으로 DB에 액세스 할 수 있다. 서비스도 IP 주소가 할당된다. 파드가 해당 이름 또는 IP를 사용하여 서비스에 액세스 하려고 할 때마다 트래픽을 백엔드 파드로 전달한다.
서비스는 파드 네트워크에 들어갈 수 없다. 서비스는 실제로 존재하지 않는 것이며 파드와 같은 컨테이너가 아니기 때문이다. 따라서 인터페이스나 listening 프로세스가 없다. 서비스는 쿠버네티스 메모리에만 존재하는 가상 구성 요소이다. 그러나 클러스터의 모든 노드에서 서비스에 액세스 할 수 있어야 한다. 이것이 kube-proxy의 역할이다. 

kube-proxy는 쿠버네티스 클러스터의 각 노드에서 실행되는 프로세스다. 새로운 서비스를 찾고 새로운 서비스가 생성될 때마다 각 노드에서 해당 서비스로 트래픽을 전달하기 위한 규칙을 생성한다. 이 작업은 iptables 규칙을 사용하여 수행된다.
위의 경우 각 노드에서 서비스 IP로 향하는 트래픽을 실제 파드 IP로 전달하기 위한 iptables 규칙을 생성한다. 이렇게 하여 kube-proxy가 서비스를 구성한다. 
## install
쿠버네티스 릴리스 페이지에서 다운로드하고 서비스로 실행한다. 
kubeadm은 kube-proxy를 각 노드에 파드로 배포한다. 데몬셋을 사용하기 때문에 각 노드에 단일 파드가 배포된다. 