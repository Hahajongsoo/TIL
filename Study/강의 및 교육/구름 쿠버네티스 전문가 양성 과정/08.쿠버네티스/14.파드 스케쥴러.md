# 노드에 파드 할당하기
특정 노드 집합에서만 동작하거나 특정 노드 집합에서 동작하는 것을 선호하도록 파드를 제한할 수 있다. 이를 수행하는 방법에는 여러 가지가 있으며 권장되는 접근 방식은 모두 레이블 셀렉터를 사용하여 선택을 용이하게 한다. 보통은 스케줄러가 자동으로 합리적인 배치를 수행하기에 이러한 제약 조건을 설정할 필요는 없으나 SSD 를 사용한다거나 GPU를 사용한다거나 등의 노드 그룹을 지정할 때 사용할 수 있다. 
## nodeName
스케쥴러에 의해 배치되는 것이 아니라 스케쥴러를 우회해서 강제로 배치하는 것이기 때문에 뒤에서 설정하는 어피니티, 테인트 등에 영향을 받지 않게된다.
- 고가용성을 유지할 수 없기 때문에 잘 사용하지 않는다.
```yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: myapp-rs-nn
spec:
  replicas: 2
  selector:
    matchLabels:
      app: myapp-rs-nn
  template:
    metadata:
      labels:
        app: myapp-rs-nn
    spec:
      nodeName: kube-node1
      containers:
      - name: myapp
        image: ghcr.io/c1t1d0s7/go-myweb:alpine
```

## nodeSelector
노드의 레이블을 설정하고 파드에 레이블셀렉터를 설정하여 파드가 할당될 노드를 지정할 수 있다. 예를 들어 특정 노드가 GPU에 특화되어 있다면 GPU가 필요한 파드를 해당 노드에 할당되도록 지정하는 것이다.
```yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: myapp-rs-ns
spec:
  replicas: 2
  selector:
    matchLabels:
      app: myapp-rs-ns
  template:
    metadata:
      labels:
        app: myapp-rs-ns
    spec:
      nodeSelector:
        gpu: highend
      containers:
      - name: myapp
        image: ghcr.io/c1t1d0s7/go-myweb:alpine
```

```
vagrant@kube-control1:~$ kubectl get nodes -L gpu
NAME            STATUS   ROLES           AGE   VERSION   GPU
kube-control1   Ready    control-plane   9d    v1.24.6   
kube-node1      Ready    <none>          9d    v1.24.6   highend
kube-node2      Ready    <none>          9d    v1.24.6   lowend
kube-node3      Ready    <none>          9d    v1.24.6   lowend
```
모두 노드1에만 생성됨을 확인할 수 있다.
```
vagrant@kube-control1:~$ kubectl get po -o wide
NAME                                      READY   STATUS    RESTARTS         AGE     IP              NODE         NOMINATED NODE   READINESS GATES
myapp-rs-ns-9hrtn                         1/1     Running   0                14s     10.233.73.121   kube-node1   <none>           <none>
myapp-rs-ns-vr76p                         1/1     Running   0                14s     10.233.73.120   kube-node1   <none>           <none>
```

## 어피니티
노드와 컨테이너간의 친밀성, 연관성을 어피니티라고 하고 노드 셀렉터와는 좀 다른 의미이다. 어피니티는 표현할 수 있는 제약 종류를 크게 확장한다.
```
vagrant@kube-control1:~$ kubectl explain pod.spec.affinity
KIND:     Pod
VERSION:  v1

RESOURCE: affinity <Object>

DESCRIPTION:
     If specified, the pod's scheduling constraints

     Affinity is a group of affinity scheduling rules.

FIELDS:
   nodeAffinity <Object>
     Describes node affinity scheduling rules for the pod.

   podAffinity  <Object>
     Describes pod affinity scheduling rules (e.g. co-locate this pod in the
     same node, zone, etc. as some other pod(s)).

   podAntiAffinity      <Object>
     Describes pod anti-affinity scheduling rules (e.g. avoid putting this pod
     in the same node, zone, etc. as some other pod(s)).
```
- nodeAffinity 는 노드와 파드간의 친화성을 의미한다.
- podAffinity/podAntiAffinity는 파드와 파드 사이의 친화성/비친화성을 의미한다.

다음의 필드는 다른 어피니티들에서도 동일하게 등장한다.
```
vagrant@kube-control1:~$ kubectl explain pod.spec.affinity.nodeAffinity
KIND:     Pod
VERSION:  v1

RESOURCE: nodeAffinity <Object>

DESCRIPTION:
     Describes node affinity scheduling rules for the pod.

     Node affinity is a group of node affinity scheduling rules.

FIELDS:
   preferredDuringSchedulingIgnoredDuringExecution      <[]Object>
     The scheduler will prefer to schedule pods to nodes that satisfy the
     affinity expressions specified by this field, but it may choose a node that
     violates one or more of the expressions. The node that is most preferred is
     the one with the greatest sum of weights, i.e. for each node that meets all
     of the scheduling requirements (resource request, requiredDuringScheduling
     affinity expressions, etc.), compute a sum by iterating through the
     elements of this field and adding "weight" to the sum if the node matches
     the corresponding matchExpressions; the node(s) with the highest sum are
     the most preferred.

   requiredDuringSchedulingIgnoredDuringExecution       <Object>
     If the affinity requirements specified by this field are not met at
     scheduling time, the pod will not be scheduled onto the node. If the
     affinity requirements specified by this field cease to be met at some point
     during pod execution (e.g. due to an update), the system may or may not try
     to eventually evict the pod from its node.
```
- preferred 는 선호(soft)를 의미하고 required는 요청(hard)을 의미한다. 강제성이 있느냐 없느냐로 분류할 수 있다. 
- DuringScheduling 은 스케쥴링이 될 때를 의미한다. 스케쥴링이 될 때는 파드, 리소스가 만들어질 때를 말한다.
- IgnoredDuringExecution 은 실행 중에는 무시됨을 의미한다. 파드가 이미 특정 노드에서 실행중일 때는 정책을 바꿔도 영향을 받지 않는다.

`preferredDuringSchedulingIgnoredDuringExecution`의 경우에는 선호이기 때문에 가중치(`weight`)가 존재한다. `preference` 에는 노드의 레이블을 선택하거나 필드를 선택할 값을 지정한다.
`requiredDuringSchedulingIgnoredDuringExecution`의 경우에는 반드시 지켜져야 하므로 가중치는 존재하지 않는다. 

### nodeAffinity
```yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: myapp-rs-nodeaff
spec:
  replicas: 2
  selector:
    matchLabels:
      app: myapp-rs-nodeaff
  template:
    metadata:
      labels:
        app: myapp-rs-nodeaff
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: gpu-model
                operator: In
                values:
                - '3080'
                - '2080'
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 10
            preference:
              matchExpressions:
              - key: gpu-model
                operator: In
                values:
                - titan
      containers:
      - name: myapp
        image: ghcr.io/c1t1d0s7/go-myweb:alpine
```

```
vagrant@kube-control1:~$ kubectl get nodes -L gpu-model
NAME            STATUS   ROLES           AGE   VERSION   GPU-MODEL
kube-control1   Ready    control-plane   9d    v1.24.6   
kube-node1      Ready    <none>          9d    v1.24.6   3080
kube-node2      Ready    <none>          9d    v1.24.6   2080
kube-node3      Ready    <none>          9d    v1.24.6   1080
```

```
vagrant@kube-control1:~$ kubectl get po -o wide
NAME                                      READY   STATUS    RESTARTS         AGE     IP              NODE         NOMINATED NODE   READINESS GATES
myapp-rs-nodeaff-4chxc                    1/1     Running   0                41s     10.233.73.122   kube-node1   <none>           <none>
myapp-rs-nodeaff-dxlkk                    1/1     Running   0                41s     10.233.74.17    kube-node2   <none>           <none>
```

### podAffinity, AntiAffinity
파드가 같은 노드에 생성되는지 생성되지 않는지를 나타낸다. 여기서 `topologyKey`는 분리시키는 기준을 의미한다. 노드의 레이블을 지정하게 된다. 예를 들어 AWS에서 가용영역이 다른 노드들에 파드를 배치시키고 싶다면 `topologyKey`에 가용영역에 관련된 레이블을 넣어주면 된다.

- template의 레이블이 tier-cache이고 podAntiAffinity 에서 cache를 지정했기 때문에 tier가 cache인 파드 끼리는 같은 노드에 생성되지 않을 것이다. 
```yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: myapp-rs-aff-cache
spec:
  replicas: 2
  selector:
    matchLabels:
      app: myapp-rs-aff-cache
      tier: cache
  template:
    metadata:
      labels:
        app: myapp-rs-aff-cache
        tier: cache
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: tier
                operator: In
                values:
                - cache
            topologyKey: "kubernetes.io/hostname"
      containers:
      - name: myapp
        image: ghcr.io/c1t1d0s7/go-myweb:alpine
```
- template의 레이블이 tier=frontend 이고 podAntiAffinity는 frontend 이고 podAffinity는 cache 이므로 파드들은 각각 따로 레이블이 tier=cache인 파드가 있는 노드에 생성될 것이다.
```yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: myapp-rs-aff-front
spec:
  replicas: 2
  selector:
    matchLabels:
      app: myapp-rs-aff-front
      tier: frontend
  template:
    metadata:
      labels:
        app: myapp-rs-aff-front
        tier: frontend
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: tier
                operator: In
                values:
                - frontend
            topologyKey: "kubernetes.io/hostname"
        podAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: tier
                operator: In
                values:
                - cache 
            topologyKey: "kubernetes.io/hostname"
      containers:
      - name: myapp
        image: ghcr.io/c1t1d0s7/go-myweb:alpine
```

- 결론적으로 cache가 먼저 다른 노드에 생성되고 이후에 frontend가 cache가 있는 노드에 생성될 것이다. 
```
vagrant@kube-control1:~$ kubectl get po -o wide
NAME                                      READY   STATUS    RESTARTS         AGE     IP              NODE         NOMINATED NODE   READINESS GATES
myapp-rs-aff-cache-7cvqc                  1/1     Running   0                8s      10.233.74.18    kube-node2   <none>           <none>
myapp-rs-aff-cache-q7tvh                  1/1     Running   0                8s      10.233.73.123   kube-node1   <none>           <none>

vagrant@kube-control1:~$ kubectl get po -o wide
NAME                                      READY   STATUS    RESTARTS         AGE     IP              NODE         NOMINATED NODE   READINESS GATES
myapp-rs-aff-cache-7cvqc                  1/1     Running   0                25s     10.233.74.18    kube-node2   <none>           <none>
myapp-rs-aff-cache-q7tvh                  1/1     Running   0                25s     10.233.73.123   kube-node1   <none>           <none>
myapp-rs-aff-front-5vb28                  1/1     Running   0                3s      10.233.73.124   kube-node1   <none>           <none>
myapp-rs-aff-front-lhsbv                  1/1     Running   0                3s      10.233.74.19    kube-node2   <none>           <none>
```

# 테인트(Taints)와 톨러레이션(Tolerations)
노드 어피니티는 노드 셋을 끌어들이는 파드의 속성이다. 테인트는 그 반대로 노드가 파드 셋을 제외 시킬 수 있다. 
톨러레이션은 파드에 적용된다. 톨러레이션을 통해 스케줄러는 그와 일치하는 테인트가 있는 노드에 파드를 스케줄할 수 있다. 
테인트와 톨러레이션은 함께 작동하여 파드가 부적절한 노드에 스케줄되지 않게 한다. 하나 이상의 테인트가 노드에 적용되는데, 이것은 노드가 테인트를 용인하지 않는 파드를 수용해서는 안된다는 것을 의미한다. 

노드를 국가, 파드를 사람으로 본다면 테인트는 국가의 정책, 톨러레이션은 비자로 볼 수 있다. 노드는 테인트를 가지고 있고 테인트는 key와 effect를 가지고 있다. 이때 파드가 해당 key로 톨러레이션을 가지지 않는다면 effect가 발동되는 것이다. 

effect에는 다음의 값들이 있다.
```
vagrant@kube-control1:~$ kubectl explain nodes.spec.taints.effect
KIND:     Node
VERSION:  v1

FIELD:    effect <string>

DESCRIPTION:
     Required. The effect of the taint on pods that do not tolerate the taint.
     Valid effects are NoSchedule, PreferNoSchedule and NoExecute.

     Possible enum values:
     - `"NoExecute"` Evict any already-running pods that do not tolerate the
     taint. Currently enforced by NodeController.
     - `"NoSchedule"` Do not allow new pods to schedule onto the node unless
     they tolerate the taint, but allow all pods submitted to Kubelet without
     going through the scheduler to start, and allow all already-running pods to
     continue running. Enforced by the scheduler.
     - `"PreferNoSchedule"` Like TaintEffectNoSchedule, but the scheduler tries
     not to schedule new pods onto the node, rather than prohibiting new pods
     from scheduling onto the node entirely. Enforced by the scheduler.
```
파드의 톨러레이션에 kube-control1에 대한 테인트를 넣어주게 되면 컨트롤 플레인에도 파드가 생성될 수 있다. 
```
vagrant@kube-control1:~$ kubectl get nodes kube-control1 -o jsonpath='{.spec.taints}'
[{"effect":"NoSchedule","key":"node-role.kubernetes.io/master"}] 
vagrant@kube-control1:~$ kubectl get nodes kube-node1 -o jsonpath='{.spec.taints}'
vagrant@kube-control1:~$ kubectl get nodes kube-node2 -o jsonpath='{.spec.taints}'
vagrant@kube-control1:~$ kubectl get nodes kube-node3 -o jsonpath='{.spec.taints}'
```

## 예시
```
vagrant@kube-control1:~$ kubectl get po -o wide
NAME                                      READY   STATUS    RESTARTS         AGE     IP              NODE         NOMINATED NODE   READINESS GATES
myapp-rs-aff-cache-45pvg                  1/1     Running   0                2m15s   10.233.74.24    kube-node2   <none>           <none>
myapp-rs-aff-cache-8hdwt                  1/1     Running   0                2m15s   10.233.73.127   kube-node1   <none>           <none>
myapp-rs-aff-front-66bj8                  1/1     Running   0                2m9s    10.233.73.67    kube-node1   <none>           <none>
myapp-rs-aff-front-9n7zl                  1/1     Running   0                2m9s    10.233.74.23    kube-node2   <none>           <none>
```

```
vagrant@kube-control1:~$ kubectl taint node kube-node3 env=production:NoSchedule
node/kube-node3 tainted
```

```yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: myapp-rs-notol
spec:
  replicas: 1
  selector:
    matchLabels:
      app: myapp-rs-notol
      tier: backend
  template:
    metadata:
      labels:
        app: myapp-rs-notol
        tier: backend
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: tier
                operator: In
                values:
                - cache
            topologyKey: "kubernetes.io/hostname"
      containers:
      - name: myapp
        image: ghcr.io/c1t1d0s7/go-myweb:alpine
```
- 새로 생성하는 파드는 cache가 있는 노드에 생성될 수 없고 cache 가 없는 노드에는 테인트가 설정되어있기 때문에 톨러레이션이 없는 이 파드는 어디에도 생성될 수 없다.
```
vagrant@kube-control1:~$ kubectl create -f myapp-rs-notol.yaml 
replicaset.apps/myapp-rs-notol created

vagrant@kube-control1:~$ kubectl get po myapp-rs-notol-9p8zj
NAME                                      READY   STATUS    RESTARTS         AGE
myapp-rs-notol-9p8zj                      0/1     Pending   0                22s

vagrant@kube-control1:~$ kubectl describe po myapp-rs-notol-9p8zj 
Name:           myapp-rs-notol-9p8zj
Namespace:      default
...
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  65s   default-scheduler  0/4 nodes are available: 1 node(s) had untolerated taint {env: production}, 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 node(s) didn't match pod anti-affinity rules. preemption: 0/4 nodes are available: 2 No preemption victims found for incoming pod, 2 Preemption is not helpful for scheduling.
```

```yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: myapp-rs-tol
spec:
  replicas: 1
  selector:
    matchLabels:
      app: myapp-rs-tol
      tier: backend
  template:
    metadata:
      labels:
        app: myapp-rs-tol
        tier: backend
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: tier
                operator: In
                values:
                - cache
            topologyKey: "kubernetes.io/hostname"
      tolerations:
      - key: env
        operator: Equal
        value: production
        effect: NoSchedule
      containers:
      - name: myapp
        image: ghcr.io/c1t1d0s7/go-myweb:alpine
```
톨러레이션이 있는 경우 파드가 생성됨을 확인할 수 있다.
```
vagrant@kube-control1:~$ kubectl create -f myapp-rs-tol.yaml 
replicaset.apps/myapp-rs-tol created

vagrant@kube-control1:~$ kubectl get po myapp-rs-tol-66bdd 
NAME                 READY   STATUS    RESTARTS   AGE
myapp-rs-tol-66bdd   1/1     Running   0          37s
```

# 커든(cordon)과 드레인(drain)
만약 시스템 유지보수를 한다고 했을 때 cordon으로 막고 drain으로 노드 내부 파드들을 모두 축출할 수 있다. drain을 하는 경우 cordon은 자동으로 진행된다. 유지보수가 완료된 후에는 uncordon으로 cordon을 풀어주면 된다. 

## cordon
cordon 명령은 노드에 스케쥴링 되지 않도록 마킹을 하는 명령이다. 
```
vagrant@kube-control1:~$ kubectl cordon --help
Mark node as unschedulable.

Examples:
  # Mark node "foo" as unschedulable
  kubectl cordon foo

Options:
    --dry-run='none':
        Must be "none", "server", or "client". If client strategy, only print the object that would be sent, without
        sending it. If server strategy, submit server-side request without persisting the resource.

    -l, --selector='':
        Selector (label query) to filter on, supports '=', '==', and '!='.(e.g. -l key1=value1,key2=value2). Matching
        objects must satisfy all of the specified label constraints.

Usage:
  kubectl cordon NODE [options]
```

### 예시
```yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: myapp-rs-cordon
spec:
  replicas: 4
  selector:
    matchLabels:
      app: myapp-rs-cordon
  template:
    metadata:
      labels:
        app: myapp-rs-cordon
    spec:
      containers:
      - name: myapp
        image: ghcr.io/c1t1d0s7/go-myweb:alpine
```
- cordon을 풀고자 하는 경우 `kubectl uncordon` 을 사용하면 된다.

## drain
해당 노드의 파드들을 삭제하는 명령이다. 
```
vagrant@kube-control1:~$ kubectl drain --help
Drain node in preparation for maintenance.

 The given node will be marked unschedulable to prevent new pods from arriving. 'drain' evicts the pods if the API
server supports https://kubernetes.io/docs/concepts/workloads/pods/disruptions/ . Otherwise, it will use normal DELETE
to delete the pods. The 'drain' evicts or deletes all pods except mirror pods (which cannot be deleted through the API
server).  If there are daemon set-managed pods, drain will not proceed without --ignore-daemonsets, and regardless it
will not delete any daemon set-managed pods, because those pods would be immediately replaced by the daemon set
controller, which ignores unschedulable markings.  If there are any pods that are neither mirror pods nor managed by a
replication controller, replica set, daemon set, stateful set, or job, then drain will not delete any pods unless you
use --force.  --force will also allow deletion to proceed if the managing resource of one or more pods is missing.

 'drain' waits for graceful termination. You should not operate on the machine until the command completes.
 ...
```
- 복제본 컨트롤러는 복제본만큼 다른 노드에 생성되기 때문에 상관 없지만 데몬셋의 경우 해당 노드가 아니면 다른 노드에 생성되지 않기 때문에 이를 삭제하기 위해서는 `--ignore-daemonsets=true`를 설정해줘야 한다.