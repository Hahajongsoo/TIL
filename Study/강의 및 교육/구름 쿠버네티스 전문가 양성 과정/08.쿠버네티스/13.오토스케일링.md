#kubernetes #goorm 

# 리소스컨트롤

## 파드 및 컨테이너 리소스 관리
파드를 지정할 때, 컨테이너에 필요한 리소스의 양을 선택적으로 지정할 수 있다. 지정할 가장 일반적인 리소스는 CPU와 메모리 등이 있다.
파드에서 컨테이너에 대한 리소스 request를 지정하면, kube-scheduler는 이 정보를 사용하여 파드가 배치될 노드를 결정한다. 리소스 limit을 지정하면, kubelet은 실행 중인 컨테이너가 설정한 제한보다 많은 리소스를 사용할 수 없도록 제한을 지정한다. 

### request 및 limit
`Requests describes the minimum amount of compute resources required`와 `Limits describes the maximum amount of compute resources allowed`라고 설명되어있다. requests 의 경우는 컨테이너가 그 만큼의 리소스를 쓰든 쓰지 않든 항상 확보해야한다. 리소스가 부족한 경우 해당 파드가 생성되지 않는다.  
limit을 지정하지 않으면 하나의 파드가 모든 리소스를 사용할 수도 있기 때문에 지정해놓는 것이 좋다. 또한 컨테이너의 프로세스가 허용된 양보다 많은 메모리를 사용하려고 하면 시스템 커널은 메모리부족 오류와 함께 시도한 프로세스를 종료한다. 
limit만 선언하는 경우 request의 값이 limit와 동일하게 설정된다.

### 파드와 컨테이너의 resouces.request 및 limit
-   `spec.containers[].resources.limits.cpu`
-   `spec.containers[].resources.limits.memory`
-   `spec.containers[].resources.limits.hugepages-<size>`
-   `spec.containers[].resources.requests.cpu`
-   `spec.containers[].resources.requests.memory`
-   `spec.containers[].resources.requests.hugepages-<size>`

## 리소스 단위
가장 큰 단위는 socket이 되고 그 안에 CPU가 있고 그 안에 thread가 있다. 
### CPU 리소스 단위
CPU 리소스에 대한 request 및 limit은 cpu 단위로 측정된다. 1 물리 CPU 코어 또는 1 가상 코어에 해당한다. 1 코어에 비교하여 소수점형태로 표현할 수 있으며, 100m의 형태로 밀리코어로도 표현할 수도 있다.
### 메모리 리소스 단위
메모리에 대한 리소스는 바이트 단위로 측정된다. si 접두어를 사용할 수도 있고 (G, M 등) Ki, Mi 같은 2의 거듭 제곱을 사용할 수도 있다. 다음은 대략 동일한 값이다.
```
128974848, 129e6, 129M, 128974848000m, 123Mi
```

## 예시
- request가 현재 가용 리소스보다 큰 경우
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod-huge-req
spec:
  containers:
  - name: myapp
    image: ghcr.io/c1t1d0s7/go-myweb:alpine
    resources:
      requests:
        cpu: 4000m
        memory: 4Gi
```
파드의 상태는 pending에 걸려있는 것을 확인할 수 있고,
```
NAME                                      READY   STATUS    RESTARTS         AGE
myapp-pod-huge-req                        0/1     Pending   0                6s
```
describe에서 리소스가 부족하여 kube-scheduler가 스케줄링할 수 없음을 확인할 수 있다.
```
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  20s   default-scheduler  0/4 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 4 Insufficient cpu, 4 Insufficient memory. preemption: 0/4 nodes are available: 1 Preemption is not helpful for scheduling, 3 No preemption victims found for incoming pod.
```

- limit만 지정한 경우
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod-lim
spec:
  containers:
  - name: myapp
    image: ghcr.io/c1t1d0s7/go-myweb:alpine
    resources:
      limits:
        cpu: 0.5
        memory: 20Mi
```
requests도 자동으로 적용된 것을 확인할 수 있다.
```
vagrant@kube-control1:~$ kubectl describe pod myapp-pod-lim
...
    Restart Count:  0
    Limits:
      cpu:     500m
      memory:  20Mi
    Requests:
      cpu:        500m
      memory:     20Mi
...
```
kubectl top pods로 각 파드의 리소스 사용량을 확인할 수 있고 sha256sum 명령어로 cpu에 부하를 줄 수 있다.
```
vagrant@kube-control1:~$ kubectl top pods
NAME                                      CPU(cores)   MEMORY(bytes)   
myapp-pod-lim                             500m         6Mi 
```

```
vagrant@kube-control1:~/$ kubectl exec myapp-pod-lim -- sha256sum /dev/zero
```

# 리소스 쿼터(resource quota)
`ResourceQuota` 오브젝트로 정의된 리소스 쿼터는 네임스페이스별 총 리소스 사용을 제한하는 제약 조건을 제공한다. 네임스페이스에서 만들 수 있는 오브젝트 수와 해당 네임스페이스의 리소스가 사용할 수 있는 총 컴퓨트 리소스의 양을 제한할 수 있다.

리소스 쿼터는 다음과 같이 작동한다.
-   다른 팀은 다른 네임스페이스에서 작업한다. 이것은 [RBAC](https://kubernetes.io/docs/reference/access-authn-authz/rbac/)으로 설정할 수 있다.
-   관리자는 각 네임스페이스에 대해 하나의 리소스쿼터를 생성한다.
-   사용자는 네임스페이스에서 리소스(파드, 서비스 등)를 생성하고 쿼터 시스템은 사용량을 추적하여 리소스쿼터에 정의된 하드(hard) 리소스 제한을 초과하지 않도록 한다.
-   리소스를 생성하거나 업데이트할 때 쿼터 제약 조건을 위반하면 위반된 제약 조건을 설명하는 메시지와 함께 HTTP 상태 코드 `403 FORBIDDEN`으로 요청이 실패한다.
-   `cpu`, `memory`와 같은 컴퓨트 리소스에 대해 네임스페이스에서 쿼터가 활성화된 경우 사용자는 해당값에 대한 요청 또는 제한을 지정해야 한다. 그렇지 않으면 쿼터 시스템이 파드 생성을 거부할 수 있다. 힌트: 컴퓨트 리소스 요구 사항이 없는 파드를 기본값으로 설정하려면 `LimitRanger` 어드미션 컨트롤러를 사용하자.

컴퓨팅 리소스, 스토리지, 오브젝트 수 모두 제한할 수 있다.
```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: myapp-quota-cpumem
spec:
  hard:
    requests.cpu: 500m
    requests.memory: 200Mi
    limits.cpu: 1000m
    limits.memory: 1Gi

apiVersion: v1
kind: ResourceQuota
metadata:
  name: myapp-quota-object
spec:
  hard:
    pods: 10
    replicationcontrollers: 2
    secrets: 10
    configmaps: 10
    persistentvolumeclaims: 5
    services: 5
    services.loadbalancers: 1
    services.nodeports: 2
    nfs-client.storageclass.storage.k8s.io/persistentvolumeclaims: 2

apiVersion: v1
kind: ResourceQuota
metadata:
  name: myapp-quota-storage
spec:
  hard:
    requests.storage: 10Gi
    nfs-client.storageclass.storage.k8s.io/requests.storage: 2Gi
```

# 리밋 레인지(Limit Range)
기본적으로 컨테이너는 쿠버네티스 클러스터에서 무제한 컴퓨팅 리소스로 실행된다. 리밋레인지는 네임스페이스에서 리소스 할당을 제한하는 정책이다. 
-   네임스페이스에서 파드 또는 컨테이너별 최소 및 최대 컴퓨팅 리소스 사용량을 지정한다.
-   네임스페이스에서 스토리지클래스별 최소 및 최대 스토리지 요청을 지정한다.
-   네임스페이스에서 리소스에 대한 요청과 제한 사이의 비율을 지정한다.
-   네임스페이스에서 컴퓨팅 리소스에 대한 기본 요청/제한을 설정하고 런타임에 있는 컨테이너에 자동으로 설정한다.

## 리밋 레인지 개요
-   관리자는 하나의 네임스페이스에 하나의 리밋레인지를 만든다.
-   사용자는 네임스페이스에서 파드, 컨테이너 및 퍼시스턴트볼륨클레임과 같은 리소스를 생성한다.
-   `LimitRanger` 어드미션 컨트롤러는 컴퓨팅 리소스 요청 사항을 설정하지 않은 모든 파드와 컨테이너에 대한 기본값과 제한을 지정하고 네임스페이스의 리밋레인지에 정의된 리소스의 최소, 최대 및 비율을 초과하지 않도록 사용량을 추적한다.
-   리밋레인지 제약 조건을 위반하는 리소스(파드, 컨테이너, 퍼시스턴트볼륨클레임)를 생성하거나 업데이트하는 경우 HTTP 상태 코드 `403 FORBIDDEN` 및 위반된 제약 조건을 설명하는 메시지와 함께 API 서버에 대한 요청이 실패한다.
-   `cpu`, `memory`와 같은 컴퓨팅 리소스의 네임스페이스에서 리밋레인지가 활성화된 경우 사용자는 해당 값에 대한 요청 또는 제한을 지정해야 한다. 그렇지 않으면 시스템에서 파드 생성이 거부될 수 있다.
-   리밋레인지 유효성 검사는 파드 실행 단계가 아닌 파드 어드미션 단계에서만 발생한다.

```yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: myapp-limitrange
spec:
  limits:
  - type: Pod
    min:
      cpu: 50m
      memory: 5Mi
    max:
      cpu: 1
      memory: 1Gi
  - type: Container
    defaultRequest:
      cpu: 100m
      memory: 10Mi
    default:
      cpu: 200m
      memory: 100Mi
    min:
      cpu: 50m
      memory: 5Mi
    max:
      cpu: 1
      memory: 1Gi
    maxLimitRequestRatio:
      cpu: 4
      memory: 10
  - type: PersistentVolumeClaim
    min:
      storage: 10Mi
    max:
      storage: 1Gi
```

## 예시
```
vagrant@kube-control1:~$ kubectl describe limitranges myapp-limitrange 
Name:                  myapp-limitrange
Namespace:             default
Type                   Resource  Min   Max  Default Request  Default Limit  Max Limit/Request Ratio
----                   --------  ---   ---  ---------------  -------------  -----------------------
Pod                    cpu       50m   1    -                -              -
Pod                    memory    5Mi   1Gi  -                -              -
Container              memory    5Mi   1Gi  10Mi             100Mi          10
Container              cpu       50m   1    100m             200m           4
PersistentVolumeClaim  storage   10Mi  1Gi  -                -              -
```
- 스케쥴러가 해당 리소스를 스케쥴링을 하지 못하는 것이 아니라 그 이전에 생성부터 할 수가 없다.
```
vagrant@kube-control1:~$ kubectl create -f myapp-pod-huge-req.yaml 
The Pod "myapp-pod-huge-req" is invalid: 
* spec.containers[0].resources.requests: Invalid value: "4": must be less than or equal to cpu limit
* spec.containers[0].resources.requests: Invalid value: "4Gi": must be less than or equal to memory limit
```

# Horizontal Pod Autocaling
```
horizontalpodautoscalers    hpa    autoscaling/v2      true      HorizontalPodAutoscaler
```

쿠버네티스에서 HPA는 워크로드 리소스(컨트롤러)를 자동으로 업데이트하며, 워크로드의 크기를 수요에 맞게 자동으로 스케일링하는 것을 목표로 한다. 
HPA는 스케일 아웃을 하는 것으로 파드의 수를 조절한다. 즉 컨트롤러의 repliacs의 값을 자동으로 조정한다.
HorizontalPodAutoscaler는 쿠버네티스 API 자원 및 컨트롤러 형태로 구현되어 있다. HorizontalPodAutoscaler API 자원은 컨트롤러의 행동을 결정한다. 쿠버네티스 컨트롤 플레인 내에서 실행되는 HPA 컨트롤러는 평균 CPU 사용률, 평균 메모리 사용률, 또는 다른 커스텀 메트릭 등의 관측된 메트릭을 목표에 맞추기 위해 목표물(예: 디플로이먼트)의 적정 크기를 주기적으로 조정한다.

### 알고리즘 세부 정보
가장 기본적인 관점에서 HPA 컨트롤러는 원하는(desired) 메트릭 값과 현재(current) 메트릭 값 사이의 비율로 작동한다.
```
원하는 레플리카 수 = ceil[현재 레플리카 수 * ( 현재 메트릭 값 / 원하는 메트릭 값 )]
```
여러 메트릭이 지정된 경우, 이 계산은 각 메트릭에 대해 수행된 다음 원하는 레플리카 수 중 가장 큰 값이 선택된다. 

원하는 상태가 되거나 원하는 상태에서 벗어나자마자 스케일링을 진행하게 되면 그 경계에서 머무는 경우 리소스의 과다한 사용이 일어날 수 있다. 그렇기 때문에 `.spec.behavior.scaleUp` 이나 `.spec.behavior.scaleDown`에 있는 `stabilizationWindowSeconds` 으로 경계를 넘었을 때 몇초 후에 스케일링을 진행할지 값을 지정할 수 있다. 기본 값은 300초이다.  

## 리소스 메트릭 지원
모든 HPA 대상은 스케일링 대상에서 파드의 리소스 사용량을 기준으로 스케일링할 수 있다. ==파드의 명세를 정의할 때는 cpu 및 memory와 같은 **리소스 request**을 지정해야한다.== 
```yaml
type: Resource
resource:
  name: cpu
  target:
    type: Utilization
    averageUtilization: 60
```
이 메트릭을 사용하면 HPA 컨트롤러는 스케일링 대상에서 파드의 평균 사용률을 60%로 유지한다. ==사용률은 파드의 요청된 리소스에 대한 현재 리소스 사용량 간의 비율==이다.

## 예시
```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: myapp-hpa-cpu
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: myapp-deploy-hpa
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
```

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-deploy-hpa
spec:
  replicas: 3
  selector:
    matchLabels:
      app: myapp-deploy-hpa
  template:
    metadata:
      labels:
        app: myapp-deploy-hpa
    spec:
      containers:
      - name: myapp
        image: ghcr.io/c1t1d0s7/go-myweb:alpine
        resources:
          requests:
            cpu: 50m
            memory: 5Mi
          limits:
            cpu: 100m
            memory: 20Mi
        ports:
        - containerPort: 8080
```

기준은 requests의 값으로 현재 replicas가 3이므로 50 * 3 으로 150m 코어이다. 따라서 다음처럼 cpu에 부하를 주더라도 limit 가 100m 코어이므로 hpa에서 지정한 cpu 사용률 70%를 넘을 수 없다.

```
vagrant@kube-control1:~$ kubectl top pods
NAME                                      CPU(cores)   MEMORY(bytes)   
myapp-deploy-hpa-57f5f475d-6dgj7          0m           1Mi             
myapp-deploy-hpa-57f5f475d-9gh4j          101m         6Mi             
myapp-deploy-hpa-57f5f475d-mtcg6          0m           6Mi

vagrant@kube-control1:~$ kubectl get hpa
NAME            REFERENCE                     TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
myapp-hpa-cpu   Deployment/myapp-deploy-hpa   66%/70%   2         10        3          3m6s
```

다른 부하를 추가적으로 주게 되면 replicas가 증가하는 것을 확인할 수 있다. 이때 replicas를 계산해본다면 `ceil[3 * 134/70] = 6` 이 된다.
```
vagrant@kube-control1:~$ kubectl get hpa
NAME            REFERENCE                     TARGETS    MINPODS   MAXPODS   REPLICAS   AGE
myapp-hpa-cpu   Deployment/myapp-deploy-hpa   134%/70%   2         10        6          5m5s

vagrant@kube-control1:~$ kubectl top pods
NAME                                      CPU(cores)   MEMORY(bytes)   
myapp-deploy-hpa-57f5f475d-6dgj7          101m         2Mi             
myapp-deploy-hpa-57f5f475d-9gh4j          101m         6Mi             
myapp-deploy-hpa-57f5f475d-hm7zp          0m           1Mi             
myapp-deploy-hpa-57f5f475d-kzrcj          0m           1Mi             
myapp-deploy-hpa-57f5f475d-mtcg6          1m           6Mi             
myapp-deploy-hpa-57f5f475d-vw4bm          0m           1Mi 
```

