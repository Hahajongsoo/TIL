# What is spark
스파크는 클러스터 컴퓨팅을 위한 플랫폼이다. 스파크는 데이터와 계산을 다수의 노드로 구성된 클러스터에 나누게 한다. 각 노드는 작은 양의 데이터에만 작동하기 때문에 데이터를 나누는 것은 아주 큰 용량의 데이터셋을 다루는데 도움이 된다.
각각의 노드는 전체 데이터의 하위 집합으로만 작업하고 필요한 전체 계산의 일부도 수행하므로 데이터 처리와 계산이 클러스터 내부 노드들에서 병렬적으로 이루어져야 한다. 병렬 계산이 특정 유형의 프로그래밍 작업을 더 빠르게 하는 것은 사실이다.
그러나 컴퓨팅 파워가 좋아디면서 복잡성도 커졌다.
문제에 대해서 spark가 최선의 선택인지 아닌지 정하는 것은 경험이 필요하지만 다음의 질문을 고려해 볼 수 있다.
- 단일 컴퓨터에서 다루기에는 너무 큰 데이터인가?
- 계산이 쉽게 병렬적으로 이루어질 수 있는가?

## Using Spark in Python
스파크를 사용하는 첫 단계는 클러스터에 연결하는 것이다.
실제로 클러스터는 모든 다른 노드에 연결된 원격 시스템에서 호스팅된다. 데이터와 계산을 분할하는 마스터라고 불리는 한 대의 컴퓨터가 있다. 마스터는 워커라고 불리는 클러스터의 나머지 컴퓨터들과 연결되어 있다. 마스터는 워커에 데이터와 실행할 계산을 보내고 워커는 그 결과를 다시 마스터에게 보낸다.
스파크를 막 시작할 때는 클러스터를 로컬에 실행하는 것이 간단하다. 그러므로 이 과정에서는 다른 컴퓨터에 연결하는 대신 모든 계산은 시뮬레이션 된 클러스터의 데이터 캠프 서버에서 실행된다.
연결을 만드는 것은 `SparkContext`의 인스턴스를 만들기만 하면 된다. 클래스 생성자는 연결하려는 클러스터의 특성을 특정하는 약간의 옵셔널 아큐먼트를 받는다.
이러한 모든 특성을 가지고 있는 객체는 `SparkConf()` 생성자와 생성된다. 

## Using DataFrames
스파크의 핵심 데이터 구조는 Resilient Distributes Dataset(RDD)이다. RDD는 클러스터의 여러 노드에 걸켜 데이터를 분할하여 spark가 작업할 수 있도록 하는 저수준 객체이다. 그러나 RDD는 직접 사용하는 것이 어렵기 때문에 이 과정에서는 RDD위에 구축된 Spark DataFrame 추상화를 사용하게 된다.
스파크 데이터프레임은 SQL 테이블 처럼 동작하도록 설계되었다. 이해하기 쉬울뿐만 아니라 데이터프레임은 복잡한 작업에 RDD보다 더 최적화 되어 있다.
데이터의 열과 행을 수정하고 결합할 때, 같은 결과를 얻을 수 있는 여러가지 방법이 있지만, 어떤 방법은 다른 방법들 보다 훨씬 오래 걸리는 경우도 있다. RDD를 사용할 때 쿼리를 최적화 하는 올바른 방법을 찾는 것은 데이터 사이언티스트의 몫이지만, 데이터 프레임 구현에는 이러한 최적화 기능이 많이 내장되어 있다.
스파크 데이터프레임으로 작업하려면 먼저 `SparkContext`에서  `SparkSession` 객체를 생성해야 한다. `SparkContext`를 클러스터에 대한 연결로 생각하고 `SparkSession`을 해당 연결에 대한 인터페이스로 생각하면 된다.

## PySpark 사용
- `SparkSession.builder.getOrCreate()`을 사용하면 기존 SparkSession을 반환하거나 새로운 SparkSession을 생성한다.
```python
# Import SparkSession from pyspark.sql
from pyspark.sql import SparkSession

# Create my_spark
my_spark = SparkSession.builder.getOrCreate()
```
- `spark.catalog.listTables()` 을 사용하면 테이블의 목록을 확인할 수 있다.
```python
# Print the tables in the catalog
print(spark.catalog.listTables())
```
- `SparkSession` 즉 데이터프레임 인터페이스의 장점은 스파크 클러스터의 테이블에서 SQL 쿼리를 실행할 수 있다는 것이다. 
```python
# Don't change this query
query = "FROM flights SELECT * LIMIT 10"

# Get the first 10 rows of flights
flights10 = spark.sql(query)

# Show the results
flights10.show()
```
- 때로는 Pandas를 사용해서 로컬에서 데이터를 다루는 것이 알맞을 때가 있다. 그럴때에는 쿼리로 만든 새로운 테이블을 pandas DataFrame으로 만들어주면 된다.
```python
# Don't change this query
query = "SELECT origin, dest, COUNT(*) as N FROM flights GROUP BY origin, dest"

# Run the query
flight_counts = spark.sql(query)

# Convert the results to a pandas DataFrame
pd_counts = flight_counts.toPandas()

# Print the head of pd_counts
print(pd_counts.head())
```
- 위와는 반대로 pandas DataFrame을 스파크 클러스터에 넣는 방법도 있다. `.createDataFrame()`은 pandas DataFrame을 가져와서 spark DataFrame으로 만든다. 하지만 이는 로컬에 저장될 뿐 쿼리를 실행하려면 catalog에 저장해야 한다. 
- `.createTempView()`는 등록할 테이블 명을 인수로 받아 spark DataFrame을 catalog에 저장한다. 하지만 이는 임시 테이블로 특정 SparkSession만 접근할 수 있다. 혹은 `.createOrReplaceTempView()`를 사용할 수도 있다. 이전에 생성된 것이 없다면 그냥 생성하고 이미 생성된게 있다면 해당 테이블을 업데이트 한다.
![](images/Pasted%20image%2020221117200320.png)
```python
# Create pd_temp
pd_temp = pd.DataFrame(np.random.random(10))

# Create spark_temp from pd_temp
spark_temp = spark.createDataFrame(pd_temp)

# Examine the tables in the catalog
print(spark.catalog.listTables())

# Add spark_temp to the catalog
spark_temp.createOrReplaceTempView('temp')

# Examine the tables in the catalog again
print(spark.catalog.listTables())
```
- 파일에서 데이터를 바로 읽어올 수도 있다.
```python
# Don't change this file path
file_path = "/usr/local/share/datasets/airports.csv"

# Read in the airports data
airports = spark.read.csv(file_path, header=True)

# Show the data
airports.show()
```

## Common data operations
- `.withColumn()` 으로 열 단위 작업을 수행할 수 있다. 이 메소드는 새로운 열의 이름과 열 자체를 인수로 받는다. 새로운 열은 `Column` 객체여야한다. 이 객체는 `df.colName`으로 생성할 수 있다. 스파크 데이터프레임은 immutable 하기 때문에 pandas에서 작업하는 것과는 좀 다르다. 따라서 이런 모든 방법은 새로운 데이터프레임을 반환한다. 원래의 데이터프레임에 덮어쓰기 위해서는 원래 변수에 반환된 데이터프레임을 다시 할당해야한다. 
```python
# Create the DataFrame flights
flights = spark.table("flights")

# Show the head
flights.show()

# Add duration_hrs
flights = flights.withColumn("duration_hrs" , flights.air_time / 60)
```
- `.filter()` 메소드는 SQL에서 `WHERE` 절과 같다. `WHERE` 절에 올 조건(SQL 문법)을 메소드의 인수로 넣어주면 된다. 문자열 그대로 넣어줘도 되고 `WHERE` 절에 사용할 객체들을 만들어서 boolean을 반환하는 해당 조건을 넣어줘도 된다.
```python
# Filter flights by passing a string
long_flights1 = flights.filter("distance > 1000")

# Filter flights by passing a column of boolean values
long_flights2 = flights.filter(flights.distance > 1000)

# Print the data to check they're equal
long_flights1.show()
long_flights2.show()
```
- SQL의 `SELECT`의 변형이 `.select()` 메소드이다. 이 메소드는 선택할 열들을 인수로 받는다. 문자열로 받을 수도 있고 열 객체로 받을 수도 있다. 열 객체로 넣는 경우 `.withColumn()` 에서 했던 것 처럼 연산을 수행하여 넣을 수도 있다. `.select()`와 `.withColumn()`의 차이는 `.withColumn`은 모든 열을 반환하는 반면 `.select()`는 선택한 열만 반환한다. 
```python
# Select the first set of columns
selected1 = flights.select("tailnum", "origin", "dest")

# Select the second set of columns
temp = flights.select(flights.tailnum, flights.origin, flights.dest)

# Define first filter
filterA = flights.origin == "SEA"

# Define second filter
filterB = flights.dest == "PDX"

# Filter the data, first by filterA then by filterB
selected2 = temp.filter(filterA).filter(filterB)
```
- `SELECT`에서 `AS`를 사용하는 것 처럼 `.alias()`로 `.select()`에서 반환할 열의 이름을 바꿀 수 있다. 이와 동등한 스파크 데이터프레임 메소드에는 문자열을 인수로 받는 `.selectExpr()`이 있다.
```python
# Define avg_speed
avg_speed = (flights.distance/(flights.air_time/60)).alias("avg_speed")

# Select the correct columns
speed1 = flights.select("origin", "dest", "tailnum", avg_speed)

# Create the same table using a SQL expression
speed2 = flights.selectExpr("origin", "dest", "tailnum", "distance/(air_time/60) as avg_speed")
```
- `.groupBy()` 메소드로 `GroupedData` 객체를 만들 수 있고 해당 객체에는 `.min(), .max(), .count()`등의 집계 메소드들이 있다. 
```python
# Find the shortest flight from PDX in terms of distance
flights.filter(flights.origin == "PDX").groupBy().min("distance").show()

# Find the longest flight from SEA in terms of air time
flights.filter(flights.origin == "SEA").groupBy().max("air_time").show()

# Average duration of Delta flights
flights.filter(flights.carrier == "DL").filter(flights.origin == "SEA").groupBy().avg("air_time").show()

# Total hours in the air
flights.withColumn("duration_hrs",flights.air_time/60).groupBy().sum("duration_hrs").show()
```

```python
# Group by tailnum
by_plane = flights.groupBy("tailnum")

# Number of flights each plane made
by_plane.count().show()

# Group by origin
by_origin = flights.groupBy("origin")

# Average duration of flights from PDX and SEA
by_origin.avg("air_time").show()
```

- `.agg()` 를 사용하면 더 다양한 집계함수들을 사용할 수 있고 해당 함수들은 `pyspark.sql.functions`에서 가져올 수 있다.
```python
# Import pyspark.sql.functions as F
import pyspark.sql.functions as F

# Group by month and dest
by_month_dest = flights.groupBy("month", "dest")

# Average departure delay by month and destination
by_month_dest.avg('dep_delay').show()

# Standard deviation of departure delay
by_month_dest.agg(F.stddev('dep_delay')).show()
```

- join도 할 수 있다. 조인할 테이블과 on, how를 인수로 받는다.
```python
# Examine the data
print(airports.show())


# Rename the faa column
airports = airports.withColumnRenamed("faa", "dest")

# Join the DataFrames
flights_with_airports = flights.join(airports, on="dest", how="leftouter")

# Examine the new DataFrame
print(flights_with_airports.show())
```

# Machine Learning Pipeline
- `pyspark.ml` 모듈의 핵심에는 `Transformer` 와 `Estimator` 클래스가 있다. 모듈의 거의 모든 다른 클래스들은 이 두 기본 클래스들 처럼 동작한다. 
- `Transformer` 클래스는 데이터프레임을 받아서 새로운 데이터프레임을 리턴하는 `.transform()` 메소드가 있다. 보통 기존 데이터프레임에 새로운 열이 추가된 것이다. 예를 들어 `Bucketizer` 클래스를 사용하여 연속형 피쳐에 빈을 생성하거나 `PCA` 클래스를 사용하여 데이터셋의 차원을 줄일 수 있다. 
- `Estimator` 클래스는 모두 `.fit()` 메소드를 포함한다. 이 메소드는 또한 데이터프레임을 받지만 다른 데이터프레임을 반환하는 대신 모델 객체를 반환한다. 

## 데이터타입
모델링을 하기전에 스파크는 숫자형 데이터만 다룬다는 것을 알아야한다. 즉 데이터프레임의 모든 열은 정수형이거나 소수여야한다. 데이터를 가져올 때 열에 어떤 정보가 있는지 추측하도록 했다. 하지만 스파크가 항상 정확하게 추측하는 것은 아니고 데이터프레임의 일부 열은 실제 숫자가 아닌 숫자를 포함하는 문자열임을 알 수 있다.
이러한 부분을 고치기 위해서는 `.withColumn()`와 함께 `.cast()` 메소드를 사용해야한다. `.withColumn()`은 데이터프레임에, `.cast()`는 열에 동작한다.
`.cast()` 메소드에 전달해야 하는 유일한 인수는 데이터 타입이다. 정수형을 원하면 `integer`를 실수형을 원하면 `double`을 넘겨주면 된다. 
```python
# Cast the columns to integers
model_data = model_data.withColumn("arr_delay", model_data.arr_delay.cast("integer"))

model_data = model_data.withColumn("air_time", model_data.air_time.cast("integer"))

model_data = model_data.withColumn("month", model_data.month.cast("integer"))

model_data = model_data.withColumn("plane_year", model_data.plane_year.cast("integer"))
```

## 문자열
스파크는 모델링을 하기 위해서 숫자 데이터가 필요하다.  boolean도 정수형으로 바뀔 수 있는 것을 확인했다. 하지만 범주형 데이터의 경우 어떻게 숫자 데이터로 바꿀 수 있을까?
PySpark 는 `pyspark.ml.features` 의 하위 모듈에 내장된 함수들로 이를 처리할 수 있다. 원핫인코더를 사용해서 해당 데이터를 원핫벡터로 만들 수 있다. 
먼저 `StringIndexer`를 만든다. 이 클래스의 구성요소는 문자열로 된 열이 있는 데이터프레임을 받고 각각의 고유값을 숫자로 매핑하는 `Estimator`다. 그런 다음 `Estimator`는  데이터프레임을 받아 매핑한 것을 메타데이터로 추가하고 문자열 컬럼에 해당하는 숫자형 컬럼이 있는 데이터프레임을 반환하는 `Transformer`를 반환한다.
이후에 이 숫자형 컬럼을 `OneHotEncoder`를 사용해서 원핫벡터로 인코딩하는 것이다. 
```python 
# Create a StringIndexer
carr_indexer = StringIndexer(inputCol="carrier", outputCol="carrier_index")

# Create a OneHotEncoder
carr_encoder = OneHotEncoder(inputCol="carrier_index", outputCol="carrier_fact")
```

## 벡터 합치기
마지막 단계는 피쳐를 포함하는 모든 컬럼을 단일 컬럼으로 합치는 것이다. 모든 스파크 모델링 루틴에서는 데이터가 이 형식일 것이라 예상하기 때문에 모델링을 하기 전에 이 작업을 수행해야한다. 컬럼의 각 값을 벡터에 엔트리로 저장한다. 모델의 관점에서는 모든 관측은 해당 관측치에 대한 모든 정보와 해당 관측값에 대한 레이블을 포함하는 벡터이다.
이것을 `VectorAssembler`를 사용해서 해결할 수 있따. 이 `Transformer`는 새로운 열로 결함할 모든 열들을 인수로 받는다.
```python
# Make a VectorAssembler
vec_assembler = VectorAssembler(inputCols=["month", "air_time", "carrier_fact", "dest_fact", "plane_age"], outputCol="features")
```

## 파이프라인
`Pipeline`은 이전에 생성한 모든 `Estimators`와 `Transformes`를 결함하는 클래스이다. 이것은 같은 모델링 프로세스를 계속 재사용가능하게 한다. 
```python
# Import Pipeline
from pyspark.ml import Pipeline

# Make the pipeline
flights_pipe = Pipeline(stages=[dest_indexer, dest_encoder, carr_indexer, carr_encoder, vec_assembler])
```

### 파이프라인에서 데이터 변환
```python
# Fit and transform the data
piped_data = flights_pipe.fit(model_data).transform(model_data)

# Split the data into training and test sets
training, test = piped_data.randomSplit([.6, .4])
```

### Pyspark에서 모델 학습
- `pyspark.ml`의 서브 모듈들중 모델들을 임포트하면 된다.
```python
# Import LogisticRegression
from pyspark.ml.classification import LogisticRegression

# Create a LogisticRegression Estimator
lr = LogisticRegression()
```

- cross validation을 하기 위한 툴들도 마련되어있다. `pyspark.ml.evaluation`에 각기 다른 종류의 모델의 검증을 위한 클래스들이 있다. 예를 들면 logistic regression 은 binary classification이므로 `BinaryClassificationEvaluator` 클래스를 사용하면 된다. evaluator 는 ROC 를 계산한다. 
```python
# Import the evaluation submodule
import pyspark.ml.evaluation as evals

# Create a BinaryClassificationEvaluator
evaluator = evals.BinaryClassificationEvaluator(metricName="areaUnderROC")
```
- gridsearch 를 위한 grid를 추가하는 클래스도 있다. `pyspark.ml.tuning`에는 튜닝을 위한 클래스들이 있다. 
```python
# Import the tuning submodule
import pyspark.ml.tuning as tune

# Create the parameter grid
grid = tune.ParamGridBuilder()

# Add the hyperparameter
grid = grid.addGrid(lr.regParam, np.arange(0, .1, .01))
grid = grid.addGrid(lr.elasticNetParam, [0, 1])

# Build the grid
grid = grid.build()
```
- `pyspark.ml.tuning` 의 `CrossValidator`로 CV를 이용하여 hpo를 수행한다.
```python
# Create the CrossValidator
cv = tune.CrossValidator(estimator=lr,
               estimatorParamMaps=grid,
               evaluator=evaluator
               )
```
- 이후 `.fit(data)` 메소드를 사용하여 hpo를 수행하면 된다. 
```python
# Call lr.fit()
# cv.fit(training)을 수행하면 hpo를 한다. 오래걸리므로 datacamp의 예제는 그냥 모델 학습
best_lr = lr.fit(training)

# Print best_lr
print(best_lr)
```
- 모델 검증
```python
# Use the model to predict the test set
test_results = best_lr.transform(test)
  
# Evaluate the predictions
print(evaluator.evaluate(test_results))
```